{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=word_tokenize(\"Very well explained take on Carter/ Russia/ FISA/ Trump's sitch.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "#         elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "#             temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "#             if(temp1):\n",
    "#                 temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "#             temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/venezuela.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "#tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/roevwade.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/wnut17test.csv\",sep =',',keep_default_na=False)\n",
    "tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/covid_2K.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "# print(len(tweets_unpartitoned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998\n"
     ]
    }
   ],
   "source": [
    "# tweets_1=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_2=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_3=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned= pd.concat([tweets_1,tweets_2,tweets_3])\n",
    "print(len(tweets_unpartitoned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with TurboParser NP Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holder=[]\n",
    "batch_number=0\n",
    "# tweetList=[]\n",
    "sentenceList=[]\n",
    "sentID=0\n",
    "sentID_to_tweet_ID={}\n",
    "mentionList=[]\n",
    "\n",
    "for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "    index=row.Index\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    hashtags=str(row.HashTags)\n",
    "\n",
    "    user=str(row.User)\n",
    "    tweetText=str(row.TweetText)\n",
    "    annot_raw=\"\"\n",
    "    stanford_candidates=\"\"\n",
    "    ritter_candidates = \"\"\n",
    "    calai_candidates=\"\"\n",
    "\n",
    "    ne_List_final=[]\n",
    "    userMention_List_final=[]\n",
    "    tweetSentenceList=splitSentence(tweetText)\n",
    "    sentenceList.extend(tweetSentenceList)\n",
    "    \n",
    "    for sentence in tweetSentenceList:\n",
    "        sentID_to_tweet_ID[sentID]=int(index)\n",
    "        sentID+=1\n",
    "    \n",
    "    mentions=[]\n",
    "    \n",
    "#     if(len(tweetSentenceList)!=len(str(row.mentions_other_BIO).split(';'))):\n",
    "#         print('index: ',index)\n",
    "    \n",
    "    for sentence_level in str(row.mentions_other).split(';'):\n",
    "        if(sentence_level):\n",
    "            for mention in sentence_level.split(','):\n",
    "                if(mention):\n",
    "                    mentions.append(mention.strip())\n",
    "    \n",
    "#     if(len(tweetSentenceList)!= len(mentions)):\n",
    "#         print('tally: ',len(tweetSentenceList), len(mentions))\n",
    "#         print(tweetSentenceList)\n",
    "#         print(row.mentions_other)\n",
    "#     print(mentions)\n",
    "\n",
    "    mentionList.append(mentions)\n",
    "#     tweetList.append(tweetText)\n",
    "    \n",
    "    for sen_index in range(len(tweetSentenceList)):\n",
    "        sentence=tweetSentenceList[sen_index]\n",
    "        annotation=[]\n",
    "        tweetWordList=getWords(sentence)\n",
    "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
    "#         phase1Candidates\n",
    "        dict1 = {'tweetID':str(index), 'sentID':str(sen_index), 'hashtags':hashtags, 'user':user, 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList, 'start_time':now,'entry_batch':batch_number,'annotation':annotation,'stanford_candidates':stanford_candidates,'ritter_candidates':ritter_candidates,'calai_candidates':calai_candidates}\n",
    "        df_holder.append(dict1)\n",
    "\n",
    "#     for candidate in ne_List_final:\n",
    "#         #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
    "#         candidateText=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
    "#         candidateText=(candidateText.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "#         candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#         # if(index==9423):\n",
    "#         #     print(candidateText)\n",
    "#         combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
    "#         if not ((candidateText in combined)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
    "#             self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),candidate.features,batch_number)\n",
    "\n",
    "#     NE_list_phase1+=ne_List_final\n",
    "\n",
    "#     UserMention_list+=userMention_List_final\n",
    "\n",
    "tweet_sentence_df= pd.DataFrame(df_holder,columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','tweetwordList', 'start_time','entry_batch','annotation','stanford_candidates','ritter_candidates','calai_candidates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = ['Guangdong Public University of Foreign Studies is located in Guangzhou.',\n",
    "#              'Lucy is in Kolkata with diamonds.','Bernie Sanders says his fight is for the working class.','elizabeth warren chaired the CBFC',\n",
    "#              'coronavirus is scary!','U.S. is struggling'\n",
    "#             ]\n",
    "\n",
    "# print(len(mentionList),len(tweetList),len(sentID_to_tweet_ID.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conll=[]\n",
    "try:\n",
    "# #     result_stanford = tweebo_api.parse_stanford(text_data)\n",
    "# #     result_conll = tweebo_api.parse_conll(text_data)\n",
    "    result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[:1000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[1000:2000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[2000:3000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[3000:])\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse done!\n",
      "3091\n"
     ]
    }
   ],
   "source": [
    "print('parse done!')\n",
    "print(len(sentenceList))\n",
    "# print(len(tweetList))\n",
    "# print(len(result_conll))\n",
    "\n",
    "# print(sentID_to_tweet_ID[15])\n",
    "# print(result_conll[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just printing the twokenized sentences\n",
    "# sentId=0\n",
    "# df_holder=[]\n",
    "# df_columns=['tweet_id','sentence_id','word']\n",
    "# for sentence in sentenceList:\n",
    "# #     print(sentence)\n",
    "#     sentence_tokens= flatten([split_apostrophe(elem) for elem in getWordsII(sentence)],[])\n",
    "# #     print(sentence_tokens)\n",
    "# #     result=result_conll[sentId]\n",
    "#     for token in sentence_tokens:\n",
    "# #     for result_line in result.split('\\n'):\n",
    "# #         tabs = result_line.split('\\t')\n",
    "#         df_dict={'tweet_id':str(sentID_to_tweet_ID[sentId]),'sentence_id':str(sentId), 'word':token}\n",
    "#         df_holder.append(df_dict)\n",
    "#     sentId+=1\n",
    "\n",
    "# df_out = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print('pre-encoding dataframe: ', len(df_out))\n",
    "\n",
    "# #align mentions with tweets and generate BIO encoding:\n",
    "# encoded_df_columns=['Sentence #','Word','Tag']\n",
    "# encoded_df_holder=[]\n",
    "\n",
    "# # file_text=''\n",
    "# for index, mentions in enumerate(mentionList):\n",
    "#     tweet_sentID_list= df_out[df_out['tweet_id']==str(index)].sentence_id.tolist()\n",
    "#     tweet_word_list= df_out[df_out['tweet_id']==str(index)].word.tolist()\n",
    "#     print('tweet ID:', index,mentions)\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, mentions)\n",
    "    \n",
    "# #     print('tallying list lengths: ',len(tweet_sentID_list),len(tweet_word_list),len(tweet_encoding_list))\n",
    "    \n",
    "#     for encoded_list_index, sentID in enumerate(tweet_sentID_list):\n",
    "#         encoded_df_dict={'Sentence #':tweet_sentID_list[encoded_list_index], 'Word':tweet_word_list[encoded_list_index], 'Tag':tweet_encoding_list[encoded_list_index]}\n",
    "#         encoded_df_holder.append(encoded_df_dict)\n",
    "# #         file_text+=tweet_word_list[encoded_list_index]+'\\t'+tweet_encoding_list[encoded_list_index]+'\\n'\n",
    "# #     file_text+='\\n'\n",
    "\n",
    "# encoded_df_out=pd.DataFrame(encoded_df_holder,columns=encoded_df_columns)\n",
    "# print('post-encoding dataframe: ', len(encoded_df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/tweets_3k_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/venezuela_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billnye_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/pikapika_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/ripcity_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/roevwade_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "# print(encoded_df_out.tail(40))\n",
    "\n",
    "# import re\n",
    "# mystr='Macron.'\n",
    "# print(split_apostrophe(mystr))\n",
    "# print(mystr.split('-'))\n",
    "# re.split(\"(-)\",mystr)\n",
    "# if((mystr.count('.')==1)&(mystr.endswith('.'))):\n",
    "#     temp=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",mystr)))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "# conll_nounPhrase_chunking(conll_results)\n",
    "\n",
    "def getConnectedComponents(visited, adjList):\n",
    "    cc=[]\n",
    "    cc_positions=[]\n",
    "    nodeList=list(visited.keys())\n",
    "#     print('**',visited,adjList)\n",
    "    for ind in range(len(nodeList)):\n",
    "        node=nodeList[ind]\n",
    "#         print('==>',node)\n",
    "        if not(visited[node][0]):\n",
    "            if(ind>0):\n",
    "                last.sort(key = int)\n",
    "                if('^' in posStr):\n",
    "#                     print('::',last)\n",
    "                    candidateStringInner=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "                    cc.append(candidateStringInner)\n",
    "                    cc_positions.append(last)\n",
    "            last=[]\n",
    "            posStr=''\n",
    "            bfs=[node]\n",
    "            while(bfs):\n",
    "                curr=bfs.pop(0)\n",
    "                visited[curr][0]=True\n",
    "                last.append(curr)\n",
    "                posStr+=visited[curr][2]\n",
    "                for neighbour in adjList[curr]:\n",
    "                    if(not visited[neighbour][0]):\n",
    "                        bfs.append(neighbour)\n",
    "        ind+=1\n",
    "    last.sort(key = int)\n",
    "    if('^' in posStr):\n",
    "        candidateString=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "        cc.append(candidateString)\n",
    "        cc_positions.append(last)\n",
    "#     print('connected components:')\n",
    "#     print(cc)\n",
    "    return cc, cc_positions\n",
    "\n",
    "def conll_nounPhrase_chunking(tabbed_entries):\n",
    "    spans=[]\n",
    "    span=[]\n",
    "    for tabbed_entry in tabbed_entries:\n",
    "        entry=[]\n",
    "        if((tabbed_entry[3]==proper_noun_tag)|(tabbed_entry[3]==common_noun_tag)):\n",
    "            entry=tabbed_entry\n",
    "        if(tabbed_entry[3]==prep_tag):\n",
    "            head=int(tabbed_entry[6])-1\n",
    "            if(head>0):\n",
    "                head_entry=tabbed_entries[head]\n",
    "                if((head_entry[3]==proper_noun_tag)|(head_entry[3]==common_noun_tag)):\n",
    "                    entry=tabbed_entry\n",
    "        if(entry):\n",
    "            if(int(entry[0])>1):\n",
    "                if(span):\n",
    "                    if((int(entry[0])-int(span[-1][0]))>1):\n",
    "                        spans.append(span)\n",
    "                        span=[entry]\n",
    "                    else:\n",
    "                        span.append(entry)\n",
    "                else:\n",
    "                    span=[entry]\n",
    "            else:\n",
    "                span=[entry]\n",
    "    if(spans):\n",
    "        if(spans[-1][0]!=span[0]):\n",
    "            spans.append(span)\n",
    "    else:\n",
    "        if(span):\n",
    "            spans.append(span)\n",
    "    \n",
    "    final_spans=[]\n",
    "    final_spans_positions=[]\n",
    "    for span in spans:\n",
    "        minIndex=int(span[0][0])\n",
    "        maxIndex=int(span[-1][0])\n",
    "        visited={}\n",
    "        adjList={}\n",
    "        for entry in span:\n",
    "            visited[entry[0]]=[False,entry[1],entry[3]]\n",
    "            if(entry[0] not in adjList.keys()):\n",
    "                adjList[entry[0]]=[]\n",
    "            dependency=entry[6]\n",
    "            if((int(dependency)>=minIndex)&(int(dependency)<=maxIndex)):\n",
    "                adjList[entry[0]].append(dependency)\n",
    "                if(dependency not in adjList.keys()):\n",
    "                    adjList[dependency]=[]\n",
    "                adjList[dependency].append(entry[0])\n",
    "        retTup=getConnectedComponents(visited,adjList)\n",
    "        final_spans.extend(retTup[0])\n",
    "        final_spans_positions.extend(retTup[1])\n",
    "    return final_spans,final_spans_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open(\"/Users/satadisha/Documents/GitHub/covid.emerging.test.conll.preproc.url\", \"w\")\n",
    "file_text1=''\n",
    "\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/covid.emerging.test.conll.preproc.url.postag\", \"w\")\n",
    "file_text=''\n",
    "\n",
    "candidates=[]\n",
    "sentId=0\n",
    "# tweetId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "for sentence in sentenceList:\n",
    "# for tweet in tweetList:\n",
    "    result=result_conll[sentId]\n",
    "#     result=result_conll[tweetId]\n",
    "    tweet_word_list=[]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    for result_line in conll_results:\n",
    "        file_text+=result_line[1]+'\\t'+result_line[3]+'\\n'\n",
    "        tweet_word_list+=result_line[1]\n",
    "        file_text1+=result_line[1]+'\\tO'+'\\n'\n",
    "#         print('result_line: ', result_line)\n",
    "    file_text+='\\n'\n",
    "#     tweetId+=1\n",
    "#     tweetMentions=mentionList[tweetId]\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, tweetMentions)\n",
    "#     print(len(tweet_word_list),len(tweet_encoding_list))\n",
    "#     for ind, word in enumerate(tweet_word_list):\n",
    "#         file_text1+=word+'\\t'+tweet_encoding_list[ind]+'\\n'\n",
    "    file_text1+='\\n'\n",
    "    sentId+=1\n",
    "#     tweetId+=1\n",
    "\n",
    "f.write(file_text)\n",
    "f.close()\n",
    "\n",
    "f1.write(file_text1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3091 1998 3091\n"
     ]
    }
   ],
   "source": [
    "candidates=[]\n",
    "sentId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "for sentence in sentenceList:\n",
    "    result=result_conll[sentId]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    sentence_candidates,sentence_candidates_positions=conll_nounPhrase_chunking(conll_results)\n",
    "#     print(sentence)\n",
    "#     print(conll_results)\n",
    "#     print(sentence_candidates)\n",
    "\n",
    "    candidate_ind=0\n",
    "    phase1Out=''\n",
    "#     for candidate in ne_List_allCheck:\n",
    "#         position = '*'+'*'.join(str(v) for v in candidate.position)\n",
    "#         position=position+'*'\n",
    "#         candidate.set_sen_index(sen_index)\n",
    "#         phase1Out+=(((candidate.phraseText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "\n",
    "    for candidateText in sentence_candidates:\n",
    "#         print(candidateText)\n",
    "        candidateText=candidateText.lower()\n",
    "        position = '*'+'*'.join(str(v) for v in sentence_candidates_positions[candidate_ind])\n",
    "        position=position+'*'\n",
    "#         print(candidateText,sentence_candidates_positions[candidate_ind])\n",
    "        CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "        candidate_ind+=1\n",
    "        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "        \n",
    "#     candidates.append(sentence_candidates)\n",
    "    candidates.append(phase1Out)\n",
    "\n",
    "    sentId+=1\n",
    "#     print('===========')\n",
    "print(len(sentenceList),len(tweets_unpartitoned),len(candidates))\n",
    "\n",
    "tweet_sentence_df['phase1Candidates']=candidates\n",
    "\n",
    "# print(tweet_sentence_df['phase1Candidates'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842\n"
     ]
    }
   ],
   "source": [
    "candidates=CTrie.displayTrie(\"\",[])\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/SVM.py:201: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.clf.fit(self.trainArr, self.trainRes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  3091 3091 696\n",
      "-0.056426916119059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  #candidate_featureBase_DF['status'] = candidate_featureBase_DF['probability'].apply(lambda x: set(x).issubset(good_candidates))\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status']='ne'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (318, 6)\n",
      "For non-entities:  (314, 6)\n",
      "For ambiguous:  (64, 6)\n",
      "For entities:  (318, 6)\n",
      "For non-entities:  (314, 6)\n",
      "For ambiguous:  (64, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n",
      "completed tweets:  2964 incomplete tweets:  127\n",
      "16\n",
      "16\n",
      "final tally:  3091 3091\n",
      "524:  524    [[]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2 = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted= Phase2.executor(max_batch_value,tweet_sentence_df,CTrie,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_unpartitoned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[ChelseaNYCPatch]</td>\n",
       "      <td>[In an effort to provide more space for social...</td>\n",
       "      <td>[friday::*20*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[DayerJoanne]</td>\n",
       "      <td>[The public did not follow social distancing, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[social distancing, social distancing, social ...</td>\n",
       "      <td>[milesgrace77, milesgrace77, milesgrace77]</td>\n",
       "      <td>[Social distancing is oddly having a therapeut...</td>\n",
       "      <td>[, , ]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(social, b)], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[Pushkinvarma, Pushkinvarma]</td>\n",
       "      <td>[Is he nuts?, How is this social distancing?]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[], [(social, b)]]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[JcantorWeinberg, JcantorWeinberg]</td>\n",
       "      <td>[@JuliaDavisNews @MaxBoot I am willing to sacr...</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(social, b)], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[grafinator]</td>\n",
       "      <td>[Social distancing-quarantine got me like...]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b), (quarantine, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[AzCHealthPlan, AzCHealthPlan]</td>\n",
       "      <td>[People of all ages can use technology to reac...</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(ones, b), (social, b)], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[ComplainingCow]</td>\n",
       "      <td>[Why are non key workers still being allowed t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[djw302]</td>\n",
       "      <td>[My girls refuse to maintain social distancing...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[aloubella]</td>\n",
       "      <td>[All this means is people still aren’t washing...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[MMoralio]</td>\n",
       "      <td>[@KenzoShibata \"radical social distancing\" \"wh...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[amandarose1980, amandarose1980]</td>\n",
       "      <td>[@scatpep54 @Independent So have mine now., Se...</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[MeaghanRacine, MeaghanRacine]</td>\n",
       "      <td>[So sad when you see kids out playing outside ...</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(social, b)], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[jan_nayak17]</td>\n",
       "      <td>[@sardesairajdeep @MamataOfficial The irony is...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[social distancing, social distancing, social ...</td>\n",
       "      <td>[RandyBrockway, RandyBrockway, RandyBrockway, ...</td>\n",
       "      <td>[Some ?@F3FranklinTN?, Virtual #2ndF happening...</td>\n",
       "      <td>[, 2ndf::*2*||, , ]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(social, b)], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[BelowDwhiteLine]</td>\n",
       "      <td>[Social distancing 101 @Vybz_Kartel Jiggle the...</td>\n",
       "      <td>[beyoncé wine::*11*12*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b), (beyoncé wine, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[social distancing, social distancing]</td>\n",
       "      <td>[lifeofaleo_, lifeofaleo_]</td>\n",
       "      <td>[Now as black ent tv why y’all highlight what ...</td>\n",
       "      <td>[, kylie::*4*||donati::*10*||]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], [kylie]]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[], [(kylie, g), (donati, b)]]</td>\n",
       "      <td>[[], [kylie]]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[theflyingsoul94]</td>\n",
       "      <td>[@maverick_wander Not blocked anywhere social ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[callumlaps]</td>\n",
       "      <td>[@eljonesuk Well social distancing looks all i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[social distancing]</td>\n",
       "      <td>[SabeenGeopol]</td>\n",
       "      <td>[If the government is failing to implement soc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(social, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweetID                 index   entry_batch        sentID  \\\n",
       "0         0                 [nan]           [0]           [0]   \n",
       "1         1                 [nan]           [0]           [0]   \n",
       "2         2       [nan, nan, nan]     [0, 0, 0]     [0, 1, 2]   \n",
       "3         3            [nan, nan]        [0, 0]        [0, 1]   \n",
       "4         4            [nan, nan]        [0, 0]        [0, 1]   \n",
       "5         5                 [nan]           [0]           [0]   \n",
       "6         6            [nan, nan]        [0, 0]        [0, 1]   \n",
       "7         7                 [nan]           [0]           [0]   \n",
       "8         8                 [nan]           [0]           [0]   \n",
       "9         9                 [nan]           [0]           [0]   \n",
       "10       10                 [nan]           [0]           [0]   \n",
       "11       11            [nan, nan]        [0, 0]        [0, 1]   \n",
       "12       12            [nan, nan]        [0, 0]        [0, 1]   \n",
       "13       13                 [nan]           [0]           [0]   \n",
       "14       14  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "15       15                 [nan]           [0]           [0]   \n",
       "16       16            [nan, nan]        [0, 0]        [0, 1]   \n",
       "17       17                 [nan]           [0]           [0]   \n",
       "18       18                 [nan]           [0]           [0]   \n",
       "19       19                 [nan]           [0]           [0]   \n",
       "\n",
       "                                             hashtags  \\\n",
       "0                                 [social distancing]   \n",
       "1                                 [social distancing]   \n",
       "2   [social distancing, social distancing, social ...   \n",
       "3              [social distancing, social distancing]   \n",
       "4              [social distancing, social distancing]   \n",
       "5                                 [social distancing]   \n",
       "6              [social distancing, social distancing]   \n",
       "7                                 [social distancing]   \n",
       "8                                 [social distancing]   \n",
       "9                                 [social distancing]   \n",
       "10                                [social distancing]   \n",
       "11             [social distancing, social distancing]   \n",
       "12             [social distancing, social distancing]   \n",
       "13                                [social distancing]   \n",
       "14  [social distancing, social distancing, social ...   \n",
       "15                                [social distancing]   \n",
       "16             [social distancing, social distancing]   \n",
       "17                                [social distancing]   \n",
       "18                                [social distancing]   \n",
       "19                                [social distancing]   \n",
       "\n",
       "                                                 user  \\\n",
       "0                                   [ChelseaNYCPatch]   \n",
       "1                                       [DayerJoanne]   \n",
       "2          [milesgrace77, milesgrace77, milesgrace77]   \n",
       "3                        [Pushkinvarma, Pushkinvarma]   \n",
       "4                  [JcantorWeinberg, JcantorWeinberg]   \n",
       "5                                        [grafinator]   \n",
       "6                      [AzCHealthPlan, AzCHealthPlan]   \n",
       "7                                    [ComplainingCow]   \n",
       "8                                            [djw302]   \n",
       "9                                         [aloubella]   \n",
       "10                                         [MMoralio]   \n",
       "11                   [amandarose1980, amandarose1980]   \n",
       "12                     [MeaghanRacine, MeaghanRacine]   \n",
       "13                                      [jan_nayak17]   \n",
       "14  [RandyBrockway, RandyBrockway, RandyBrockway, ...   \n",
       "15                                  [BelowDwhiteLine]   \n",
       "16                         [lifeofaleo_, lifeofaleo_]   \n",
       "17                                  [theflyingsoul94]   \n",
       "18                                       [callumlaps]   \n",
       "19                                     [SabeenGeopol]   \n",
       "\n",
       "                                        TweetSentence  \\\n",
       "0   [In an effort to provide more space for social...   \n",
       "1   [The public did not follow social distancing, ...   \n",
       "2   [Social distancing is oddly having a therapeut...   \n",
       "3       [Is he nuts?, How is this social distancing?]   \n",
       "4   [@JuliaDavisNews @MaxBoot I am willing to sacr...   \n",
       "5       [Social distancing-quarantine got me like...]   \n",
       "6   [People of all ages can use technology to reac...   \n",
       "7   [Why are non key workers still being allowed t...   \n",
       "8   [My girls refuse to maintain social distancing...   \n",
       "9   [All this means is people still aren’t washing...   \n",
       "10  [@KenzoShibata \"radical social distancing\" \"wh...   \n",
       "11  [@scatpep54 @Independent So have mine now., Se...   \n",
       "12  [So sad when you see kids out playing outside ...   \n",
       "13  [@sardesairajdeep @MamataOfficial The irony is...   \n",
       "14  [Some ?@F3FranklinTN?, Virtual #2ndF happening...   \n",
       "15  [Social distancing 101 @Vybz_Kartel Jiggle the...   \n",
       "16  [Now as black ent tv why y’all highlight what ...   \n",
       "17  [@maverick_wander Not blocked anywhere social ...   \n",
       "18  [@eljonesuk Well social distancing looks all i...   \n",
       "19  [If the government is failing to implement soc...   \n",
       "\n",
       "                  phase1Candidates        annotation stanford_candidates  \\\n",
       "0                 [friday::*20*||]              [[]]                [[]]   \n",
       "1                               []              [[]]                [[]]   \n",
       "2                           [, , ]      [[], [], []]        [[], [], []]   \n",
       "3                             [, ]          [[], []]            [[], []]   \n",
       "4                             [, ]          [[], []]            [[], []]   \n",
       "5                               []              [[]]                [[]]   \n",
       "6                             [, ]          [[], []]            [[], []]   \n",
       "7                               []              [[]]                [[]]   \n",
       "8                               []              [[]]                [[]]   \n",
       "9                               []              [[]]                [[]]   \n",
       "10                              []              [[]]                [[]]   \n",
       "11                            [, ]          [[], []]            [[], []]   \n",
       "12                            [, ]          [[], []]            [[], []]   \n",
       "13                              []              [[]]                [[]]   \n",
       "14             [, 2ndf::*2*||, , ]  [[], [], [], []]    [[], [], [], []]   \n",
       "15       [beyoncé wine::*11*12*||]              [[]]                [[]]   \n",
       "16  [, kylie::*4*||donati::*10*||]          [[], []]            [[], []]   \n",
       "17                              []              [[]]                [[]]   \n",
       "18                              []              [[]]                [[]]   \n",
       "19                              []              [[]]                [[]]   \n",
       "\n",
       "     output_mentions              completeness   current_minus_entry  \\\n",
       "0               [[]]                    [True]                 [0.0]   \n",
       "1               [[]]                    [True]                 [0.0]   \n",
       "2       [[], [], []]        [True, True, True]       [0.0, 0.0, 0.0]   \n",
       "3           [[], []]              [True, True]            [0.0, 0.0]   \n",
       "4           [[], []]              [True, True]            [0.0, 0.0]   \n",
       "5               [[]]                    [True]                 [0.0]   \n",
       "6           [[], []]              [True, True]            [0.0, 0.0]   \n",
       "7               [[]]                    [True]                 [0.0]   \n",
       "8               [[]]                    [True]                 [0.0]   \n",
       "9               [[]]                    [True]                 [0.0]   \n",
       "10              [[]]                    [True]                 [0.0]   \n",
       "11          [[], []]              [True, True]            [0.0, 0.0]   \n",
       "12          [[], []]              [True, True]            [0.0, 0.0]   \n",
       "13              [[]]                    [True]                 [0.0]   \n",
       "14  [[], [], [], []]  [True, True, True, True]  [0.0, 0.0, 0.0, 0.0]   \n",
       "15              [[]]                    [True]                 [0.0]   \n",
       "16     [[], [kylie]]              [True, True]            [0.0, 0.0]   \n",
       "17              [[]]                    [True]                 [0.0]   \n",
       "18              [[]]                    [True]                 [0.0]   \n",
       "19              [[]]                    [True]                 [0.0]   \n",
       "\n",
       "                 candidates_with_label only_good_candidates  \\\n",
       "0                      [[(social, b)]]                 [[]]   \n",
       "1                      [[(social, b)]]                 [[]]   \n",
       "2              [[(social, b)], [], []]         [[], [], []]   \n",
       "3                  [[], [(social, b)]]             [[], []]   \n",
       "4                  [[(social, b)], []]             [[], []]   \n",
       "5     [[(social, b), (quarantine, b)]]                 [[]]   \n",
       "6       [[(ones, b), (social, b)], []]             [[], []]   \n",
       "7                                 [[]]                 [[]]   \n",
       "8                      [[(social, b)]]                 [[]]   \n",
       "9                      [[(social, b)]]                 [[]]   \n",
       "10                     [[(social, b)]]                 [[]]   \n",
       "11                            [[], []]             [[], []]   \n",
       "12                 [[(social, b)], []]             [[], []]   \n",
       "13                                [[]]                 [[]]   \n",
       "14         [[], [], [(social, b)], []]     [[], [], [], []]   \n",
       "15  [[(social, b), (beyoncé wine, b)]]                 [[]]   \n",
       "16     [[], [(kylie, g), (donati, b)]]        [[], [kylie]]   \n",
       "17                     [[(social, b)]]                 [[]]   \n",
       "18                     [[(social, b)]]                 [[]]   \n",
       "19                     [[(social, b)]]                 [[]]   \n",
       "\n",
       "   ambiguous_candidates  \n",
       "0                  [[]]  \n",
       "1                  [[]]  \n",
       "2          [[], [], []]  \n",
       "3              [[], []]  \n",
       "4              [[], []]  \n",
       "5                  [[]]  \n",
       "6              [[], []]  \n",
       "7                  [[]]  \n",
       "8                  [[]]  \n",
       "9                  [[]]  \n",
       "10                 [[]]  \n",
       "11             [[], []]  \n",
       "12             [[], []]  \n",
       "13                 [[]]  \n",
       "14     [[], [], [], []]  \n",
       "15                 [[]]  \n",
       "16             [[], []]  \n",
       "17                 [[]]  \n",
       "18                 [[]]  \n",
       "19                 [[]]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761 595\n",
      "7 588 754\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list=flatten(complete_tweet_dataframe_grouped_df_sorted[complete_tweet_dataframe_grouped_df_sorted.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    total_tagged+=len(output_mentions_list)\n",
    "    total_annotations+=len(annotated_mention_list)\n",
    "    \n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count+=tp_counter_inner\n",
    "    false_positive_count+=fp_counter_inner\n",
    "    false_negative_count+=fn_counter_inner\n",
    "\n",
    "print(total_annotations,total_tagged)\n",
    "print(true_positive_count,false_positive_count,false_negative_count)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84 0.7252252252252253 0.7784045124899275\n"
     ]
    }
   ],
   "source": [
    "precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "f_measure=2*(precision*recall)/(precision+recall)\n",
    "print(precision,recall,f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-a150bcecdd2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-a150bcecdd2b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2425 487 905\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#RF\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#LR\n",
    "2415 460 915\n",
    "0.84 0.7252252252252253 0.7784045124899275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ritter- TwitterNLP in Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "ritter_annotator=pd.read_csv(\"/Users/satadisha/Documents/GitHub/my-baseline-setup/ritter_tweets_3k_annotated_output.csv\",sep =',',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3067\n",
      "['ID', 'HashTags', 'TweetText', 'Output', 'mentions_other', 'User', 'ritter_candidates', 'calai_candidates', 'stanford_candidates']\n"
     ]
    }
   ],
   "source": [
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "print(len(ritter_annotator))\n",
    "print(ritter_annotator.columns.tolist())\n",
    "CTrie_ritter=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n"
     ]
    }
   ],
   "source": [
    "ritter_annotated_candidates=ritter_annotator['Output'].tolist()\n",
    "for candidate in ritter_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_ritter.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesinRitterTrie=CTrie_ritter.displayTrie(\"\",[])\n",
    "print(len(candidatesinRitterTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 645\n",
      "-0.23599405851453567\n",
      "For entities:  (511, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "For entities:  (511, 6)\n",
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4566 incomplete tweets:  157\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_Ritter = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_Ritter, converted_candidates_w_Ritter, complete_tweet_dataframe_grouped_df_sorted_w_Ritter= Phase2_w_Ritter.executor(max_batch_value,tweet_sentence_df,CTrie_ritter,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[RussiaGate]</td>\n",
       "      <td>[JanKimbrough]</td>\n",
       "      <td>[REPORT: FBI Obtained FISA Warrant For Trump A...</td>\n",
       "      <td>[fbi::*3*||fisa warrant::*5*6*||trump aide::*8...</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(fbi, g), (fisa, g), (trump, g), (the russia...</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[[the russian government]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[zi_cam]</td>\n",
       "      <td>[BUSTED....]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[sicsemp4ever, sicsemp4ever]</td>\n",
       "      <td>[Carter Page is trending but 3 weeks ago there...</td>\n",
       "      <td>[carter page::*1*2*||team trump::*15*16*||, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(carter page, g), (fisa, g), (trump, g)], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[raponikoff]</td>\n",
       "      <td>[Looks like it might be time for some more tom...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[MsEntropy]</td>\n",
       "      <td>[Sean Spicer on Carter Page:]</td>\n",
       "      <td>[sean spicer on carter page::*1*2*3*4*5*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(sean spicer, g), (carter page, g)]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       index entry_batch  sentID      hashtags  \\\n",
       "0        0       [nan]         [0]     [0]  [RussiaGate]   \n",
       "1        1       [nan]         [0]     [0]            []   \n",
       "2        2  [nan, nan]      [0, 0]  [0, 1]          [, ]   \n",
       "3        3       [nan]         [0]     [0]            []   \n",
       "4        4       [nan]         [0]     [0]            []   \n",
       "\n",
       "                           user  \\\n",
       "0                [JanKimbrough]   \n",
       "1                      [zi_cam]   \n",
       "2  [sicsemp4ever, sicsemp4ever]   \n",
       "3                  [raponikoff]   \n",
       "4                   [MsEntropy]   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [REPORT: FBI Obtained FISA Warrant For Trump A...   \n",
       "1                                       [BUSTED....]   \n",
       "2  [Carter Page is trending but 3 weeks ago there...   \n",
       "3  [Looks like it might be time for some more tom...   \n",
       "4                      [Sean Spicer on Carter Page:]   \n",
       "\n",
       "                                    phase1Candidates annotation  \\\n",
       "0  [fbi::*3*||fisa warrant::*5*6*||trump aide::*8...       [[]]   \n",
       "1                                                 []       [[]]   \n",
       "2      [carter page::*1*2*||team trump::*15*16*||, ]   [[], []]   \n",
       "3                                                 []       [[]]   \n",
       "4        [sean spicer on carter page::*1*2*3*4*5*||]       [[]]   \n",
       "\n",
       "  stanford_candidates                   output_mentions  completeness  \\\n",
       "0                [[]]              [[fbi, fisa, trump]]       [False]   \n",
       "1                [[]]                              [[]]        [True]   \n",
       "2            [[], []]  [[carter page, fisa, trump], []]  [True, True]   \n",
       "3                [[]]                              [[]]        [True]   \n",
       "4                [[]]      [[sean spicer, carter page]]        [True]   \n",
       "\n",
       "  current_minus_entry                              candidates_with_label  \\\n",
       "0               [0.0]  [[(fbi, g), (fisa, g), (trump, g), (the russia...   \n",
       "1               [0.0]                                               [[]]   \n",
       "2          [0.0, 0.0]    [[(carter page, g), (fisa, g), (trump, g)], []]   \n",
       "3               [0.0]                                               [[]]   \n",
       "4               [0.0]             [[(sean spicer, g), (carter page, g)]]   \n",
       "\n",
       "               only_good_candidates        ambiguous_candidates  \n",
       "0              [[fbi, fisa, trump]]  [[the russian government]]  \n",
       "1                              [[]]                        [[]]  \n",
       "2  [[carter page, fisa, trump], []]                    [[], []]  \n",
       "3                              [[]]                        [[]]  \n",
       "4      [[sean spicer, carter page]]                        [[]]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted_w_Ritter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2770\n",
      "2385 385 437\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_ritter=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Ritter[complete_tweet_dataframe_grouped_df_sorted_w_Ritter.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list_ritter)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610108303249098 0.8451452870304749 0.8530042918454936\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM\n",
    "0.8605577689243028 0.8428520752039731 0.8516129032258064\n",
    "\n",
    "#RF\n",
    "0.8614435981138919 0.8433948863636364 0.8523237035707877\n",
    "\n",
    "#LR\n",
    "0.8610108303249098 0.8451452870304749 0.8530042918454936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaguilar as Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n",
    "\n",
    "\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/tweebo-parser/tweets_3k_annotated.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTrie_gaguilar=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text=f.read()\n",
    "Phase2_w_gaguilar = phase2.EntityResolver()\n",
    "\n",
    "output_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n'))) #conll\n",
    "print(len(output_sentences))\n",
    "gaguilar_annotated_candidates=[]\n",
    "for line in output_sentences:\n",
    "    if(line):\n",
    "        tabs=line.split('\\t')\n",
    "        if(tabs):\n",
    "            for candidate in tabs:\n",
    "                gaguilar_annotated_candidates.append(candidate)\n",
    "print(len(gaguilar_annotated_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795\n",
      "2724\n",
      "771\n"
     ]
    }
   ],
   "source": [
    "for candidate in gaguilar_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_gaguilar.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesingaguilarTrie=CTrie_gaguilar.displayTrie(\"\",[])\n",
    "print(len(candidatesingaguilarTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 731\n",
      "-0.23514899240820253\n",
      "For entities:  (590, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "For entities:  (590, 6)\n",
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4593 incomplete tweets:  130\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "candidate_base_post_Phase2_w_gaguilar, converted_candidates_w_gaguilar, complete_tweet_dataframe_grouped_df_sorted_w_gaguilar= Phase2_w_gaguilar.executor(max_batch_value,tweet_sentence_df,CTrie_gaguilar,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2986\n",
      "2484 502 471\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_gaguilar=0\n",
    "false_positive_count_gaguilar=0\n",
    "false_negative_count_gaguilar=0\n",
    "\n",
    "total_annotations_gaguilar=0\n",
    "total_tagged_gaguilar=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_gaguilar=[]\n",
    "    tp_counter_inner_gaguilar=0\n",
    "    fp_counter_inner_gaguilar=0\n",
    "    fn_counter_inner_gaguilar=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_gaguilar=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_gaguilar.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_gaguilar=flatten(complete_tweet_dataframe_grouped_df_sorted_w_gaguilar[complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_gaguilar=len(output_mentions_list_gaguilar)\n",
    "    total_tagged_gaguilar+=len(output_mentions_list_gaguilar)\n",
    "    total_annotations_gaguilar+=len(annotated_mention_list_gaguilar)\n",
    "    \n",
    "    while(annotated_mention_list_gaguilar):\n",
    "        if(len(output_mentions_list_gaguilar)):\n",
    "            annotated_candidate= annotated_mention_list_gaguilar.pop()\n",
    "            if(annotated_candidate in output_mentions_list_gaguilar):\n",
    "                output_mentions_list_gaguilar.pop(output_mentions_list_gaguilar.index(annotated_candidate))\n",
    "                tp_counter_inner_gaguilar+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_gaguilar.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_gaguilar.extend(annotated_mention_list_gaguilar)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_gaguilar=len(unrecovered_annotated_mention_list_gaguilar)\n",
    "    fp_counter_inner_gaguilar=all_postitive_counter_inner_gaguilar- tp_counter_inner_gaguilar\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_gaguilar+=tp_counter_inner_gaguilar\n",
    "    false_positive_count_gaguilar+=fp_counter_inner_gaguilar\n",
    "    false_negative_count_gaguilar+=fn_counter_inner_gaguilar\n",
    "    \n",
    "\n",
    "# print(total_annotations_gaguilar,total_tagged_gaguilar)\n",
    "# print(true_positive_count_gaguilar,false_positive_count_gaguilar,false_negative_count_gaguilar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8318821165438715 0.8406091370558376 0.8362228581046962\n"
     ]
    }
   ],
   "source": [
    "precision_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_positive_count_gaguilar)\n",
    "recall_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_negative_count_gaguilar)\n",
    "f_measure_gaguilar=2*(precision_gaguilar*recall_gaguilar)/(precision_gaguilar+recall_gaguilar)\n",
    "print(precision_gaguilar,recall_gaguilar,f_measure_gaguilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LR\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#RF\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#SVM\n",
    "0.8318821165438715 0.8406091370558376 0.8362228581046962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "3374 3849\n",
      "2501 1348 673\n"
     ]
    }
   ],
   "source": [
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# neuroner_file = open('mentions_output_tweets_3K.txt', 'r') \n",
    "# neuroner_lines = neuroner_file.readlines()\n",
    "# print(len(neuroner_lines))\n",
    "# line_count=0\n",
    "# neuroner_annotated_candidates=[]\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "#     neuroner_output=neuroner_lines[line_count]\n",
    "#     output_mentions_list_neuroner=[candidate.lower().strip(string.punctuation).strip() for candidate in neuroner_output.split(',') if(candidate.strip(string.punctuation).strip())]\n",
    "#     neuroner_annotated_candidates.extend(output_mentions_list_neuroner)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner - tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "    \n",
    "#     line_count+=1\n",
    "    \n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6497791634190699 0.7879647132955262 0.7122312402107361\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroNER as Phase I Entity Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "[]\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "# tweet_sentence_df_2nd_copy=tweet_sentence_df.copy(deep=True)\n",
    "# CTrie_neuroner=trie.Trie(\"ROOT\")\n",
    "# print(len(neuroner_annotated_candidates))\n",
    "# print(phase2stopwordList)\n",
    "\n",
    "# for candidateText in neuroner_annotated_candidates:\n",
    "# #     print(candidateText)\n",
    "#     if(candidateText not in all_stopwords):\n",
    "#         CTrie_neuroner.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "# candidatesinNeuronerTrie=CTrie_neuroner.displayTrie(\"\",[])\n",
    "# print(len(candidatesinNeuronerTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4721 4721 1057\n",
      "-0.2661675732774245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4525 incomplete tweets:  196\n",
      "16\n",
      "16\n",
      "final tally:  4721 4721\n",
      "524:  524    [[world news, fbi, fisa, trump, washington post]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "# Phase2_w_Neuroner = phase2.EntityResolver()\n",
    "# candidate_base_post_Phase2_w_Neuroner, converted_candidates_w_Neuroner, complete_tweet_dataframe_grouped_df_sorted_w_Neuroner= Phase2_w_Neuroner.executor(max_batch_value,tweet_sentence_df_2nd_copy,CTrie_neuroner,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_2nd_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted_w_Neuroner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 3129\n",
      "2507 622 549\n"
     ]
    }
   ],
   "source": [
    "# # from ast import literal_eval\n",
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     output_mentions_list_neuroner=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Neuroner[complete_tweet_dataframe_grouped_df_sorted_w_Neuroner.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "# #     print(row['TweetText'])\n",
    "# #     print(tweet_ID, annotated_mention_list)\n",
    "# #     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner- tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "\n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8012144455097475 0.8203534031413613 0.8106709781729993\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2095\n",
      "1571 524 875\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    output_mentions_list_ritter=[]\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "    candidate_list_ritter=flatten(ritter_annotator[ritter_annotator.ID==tweet_ID].Output.tolist(),[])\n",
    "    for ritter_candidate in candidate_list_ritter:\n",
    "        output_mentions_list_ritter+= [remAmpersand(elem).strip(string.punctuation).strip() for elem in ritter_candidate.lower().split(',') if(elem)]\n",
    "        \n",
    "#     output_mentions_list_ritter= [remAmpersand(elem).strip(string.punctuation).strip().lower() for elem in candidate_list_ritter]\n",
    "    \n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list_ritter)\n",
    "#     print(output_mentions_list_ritter)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498806682577566 0.6422730989370401 0.6919180797181238\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with different annotators:\n",
    "\n",
    "##With CS+ in phase 1:\n",
    "0.7982625482625483 0.7359833877187778 0.7658589288470442\n",
    "\n",
    "##With Turboparse chunker in phase 1:\n",
    "0.8381118881118881 0.7104327208061648 0.7690086621751684\n",
    "\n",
    "##Just TwitterNLP:\n",
    "0.7460620525059666 0.6335630320226996 0.6852257781674704\n",
    "\n",
    "##With TwitterNLP entity annotator in phase 1:\n",
    "0.856 0.8288732394366197 0.8422182468694097\n",
    "\n",
    "##Just NeuroNER:\n",
    "0.6497791634190699 0.7879647132955262 0.7122312402107361\n",
    "\n",
    "##With NeuroNER entity annotator in phase 1:\n",
    "0.8012144455097475 0.8203534031413613 0.8106709781729993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
