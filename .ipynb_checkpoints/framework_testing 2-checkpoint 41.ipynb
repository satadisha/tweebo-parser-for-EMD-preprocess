{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n"
     ]
    }
   ],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats\n",
    "\n",
    "# import phase2_Trie_baseline_reintroduction_effectiveness as phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=word_tokenize(\"Very well explained take on Carter/ Russia/ FISA/ Trump's sitch.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "        elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/venezuela.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/roevwade.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/wnut17test.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "# print(len(tweets_unpartitoned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004\n"
     ]
    }
   ],
   "source": [
    "# tweets_1=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_2=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_3=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned= pd.concat([tweets_1,tweets_2,tweets_3])\n",
    "print(len(tweets_unpartitoned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with TurboParser NP Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1=time.time()\n",
    "df_holder=[]\n",
    "batch_number=0\n",
    "# tweetList=[]\n",
    "sentenceList=[]\n",
    "sentID=0\n",
    "sentID_to_tweet_ID={}\n",
    "mentionList=[]\n",
    "\n",
    "for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "    index=row.Index\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    hashtags=str(row.HashTags)\n",
    "\n",
    "    user=str(row.User)\n",
    "    tweetText=str(row.TweetText)\n",
    "    annot_raw=\"\"\n",
    "    stanford_candidates=\"\"\n",
    "    ritter_candidates = \"\"\n",
    "    calai_candidates=\"\"\n",
    "\n",
    "    ne_List_final=[]\n",
    "    userMention_List_final=[]\n",
    "    tweetSentenceList=splitSentence(tweetText)\n",
    "    sentenceList.extend(tweetSentenceList)\n",
    "    \n",
    "    for sentence in tweetSentenceList:\n",
    "        sentID_to_tweet_ID[sentID]=int(index)\n",
    "        sentID+=1\n",
    "    \n",
    "    mentions=[]\n",
    "    \n",
    "#     if(len(tweetSentenceList)!=len(str(row.mentions_other_BIO).split(';'))):\n",
    "#         print('index: ',index)\n",
    "    \n",
    "    for sentence_level in str(row.mentions_other).split(';'):\n",
    "        if(sentence_level):\n",
    "            for mention in sentence_level.split(','):\n",
    "                if(mention):\n",
    "                    mentions.append(mention.strip())\n",
    "    \n",
    "#     if(len(tweetSentenceList)!= len(mentions)):\n",
    "#         print('tally: ',len(tweetSentenceList), len(mentions))\n",
    "#         print(tweetSentenceList)\n",
    "#         print(row.mentions_other)\n",
    "#     print(mentions)\n",
    "\n",
    "    mentionList.append(mentions)\n",
    "#     tweetList.append(tweetText)\n",
    "    \n",
    "    for sen_index in range(len(tweetSentenceList)):\n",
    "        sentence=tweetSentenceList[sen_index]\n",
    "        annotation=[]\n",
    "        tweetWordList=getWords(sentence)\n",
    "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
    "#         phase1Candidates\n",
    "        dict1 = {'tweetID':str(index), 'sentID':str(sen_index), 'hashtags':hashtags, 'user':user, 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList, 'start_time':now,'entry_batch':batch_number,'annotation':annotation,'stanford_candidates':stanford_candidates,'ritter_candidates':ritter_candidates,'calai_candidates':calai_candidates}\n",
    "        df_holder.append(dict1)\n",
    "\n",
    "#     for candidate in ne_List_final:\n",
    "#         #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
    "#         candidateText=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
    "#         candidateText=(candidateText.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "#         candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#         # if(index==9423):\n",
    "#         #     print(candidateText)\n",
    "#         combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
    "#         if not ((candidateText in combined)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
    "#             self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),candidate.features,batch_number)\n",
    "\n",
    "#     NE_list_phase1+=ne_List_final\n",
    "\n",
    "#     UserMention_list+=userMention_List_final\n",
    "\n",
    "tweet_sentence_df= pd.DataFrame(df_holder,columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','tweetwordList', 'start_time','entry_batch','annotation','stanford_candidates','ritter_candidates','calai_candidates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = ['Guangdong Public University of Foreign Studies is located in Guangzhou.',\n",
    "#              'Lucy is in Kolkata with diamonds.','Bernie Sanders says his fight is for the working class.','elizabeth warren chaired the CBFC',\n",
    "#              'coronavirus is scary!','U.S. is struggling'\n",
    "#             ]\n",
    "\n",
    "# print(len(mentionList),len(tweetList),len(sentID_to_tweet_ID.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conll=[]\n",
    "try:\n",
    "# #     result_stanford = tweebo_api.parse_stanford(text_data)\n",
    "# #     result_conll = tweebo_api.parse_conll(text_data)\n",
    "    result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[:1000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[1000:2000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[2000:3000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[3000:])\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse done!\n",
      "3084\n"
     ]
    }
   ],
   "source": [
    "print('parse done!')\n",
    "print(len(sentenceList))\n",
    "\n",
    "# print(len(tweetList))\n",
    "# print(len(result_conll))\n",
    "\n",
    "# print(sentID_to_tweet_ID[15])\n",
    "# print(result_conll[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just printing the twokenized sentences\n",
    "# sentId=0\n",
    "# df_holder=[]\n",
    "# df_columns=['tweet_id','sentence_id','word']\n",
    "# for sentence in sentenceList:\n",
    "# #     print(sentence)\n",
    "#     sentence_tokens= flatten([split_apostrophe(elem) for elem in getWordsII(sentence)],[])\n",
    "# #     print(sentence_tokens)\n",
    "# #     result=result_conll[sentId]\n",
    "#     for token in sentence_tokens:\n",
    "# #     for result_line in result.split('\\n'):\n",
    "# #         tabs = result_line.split('\\t')\n",
    "#         df_dict={'tweet_id':str(sentID_to_tweet_ID[sentId]),'sentence_id':str(sentId), 'word':token}\n",
    "#         df_holder.append(df_dict)\n",
    "#     sentId+=1\n",
    "\n",
    "# df_out = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print('pre-encoding dataframe: ', len(df_out))\n",
    "\n",
    "# #align mentions with tweets and generate BIO encoding:\n",
    "# encoded_df_columns=['Sentence #','Word','Tag']\n",
    "# encoded_df_holder=[]\n",
    "\n",
    "# # file_text=''\n",
    "# for index, mentions in enumerate(mentionList):\n",
    "#     tweet_sentID_list= df_out[df_out['tweet_id']==str(index)].sentence_id.tolist()\n",
    "#     tweet_word_list= df_out[df_out['tweet_id']==str(index)].word.tolist()\n",
    "#     print('tweet ID:', index,mentions)\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, mentions)\n",
    "    \n",
    "# #     print('tallying list lengths: ',len(tweet_sentID_list),len(tweet_word_list),len(tweet_encoding_list))\n",
    "    \n",
    "#     for encoded_list_index, sentID in enumerate(tweet_sentID_list):\n",
    "#         encoded_df_dict={'Sentence #':tweet_sentID_list[encoded_list_index], 'Word':tweet_word_list[encoded_list_index], 'Tag':tweet_encoding_list[encoded_list_index]}\n",
    "#         encoded_df_holder.append(encoded_df_dict)\n",
    "# #         file_text+=tweet_word_list[encoded_list_index]+'\\t'+tweet_encoding_list[encoded_list_index]+'\\n'\n",
    "# #     file_text+='\\n'\n",
    "\n",
    "# encoded_df_out=pd.DataFrame(encoded_df_holder,columns=encoded_df_columns)\n",
    "# print('post-encoding dataframe: ', len(encoded_df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/tweets_3k_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/venezuela_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billnye_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/pikapika_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/ripcity_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/roevwade_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "# print(encoded_df_out.tail(40))\n",
    "\n",
    "# import re\n",
    "# mystr='Macron.'\n",
    "# print(split_apostrophe(mystr))\n",
    "# print(mystr.split('-'))\n",
    "# re.split(\"(-)\",mystr)\n",
    "# if((mystr.count('.')==1)&(mystr.endswith('.'))):\n",
    "#     temp=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",mystr)))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "# conll_nounPhrase_chunking(conll_results)\n",
    "\n",
    "def getConnectedComponents(visited, adjList):\n",
    "    cc=[]\n",
    "    cc_positions=[]\n",
    "    nodeList=list(visited.keys())\n",
    "#     print('**',visited,adjList)\n",
    "    for ind in range(len(nodeList)):\n",
    "        node=nodeList[ind]\n",
    "#         print('==>',node)\n",
    "        if not(visited[node][0]):\n",
    "            if(ind>0):\n",
    "                last.sort(key = int)\n",
    "                if('^' in posStr):\n",
    "#                     print('::',last)\n",
    "                    candidateStringInner=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "                    cc.append(candidateStringInner)\n",
    "                    cc_positions.append(last)\n",
    "            last=[]\n",
    "            posStr=''\n",
    "            bfs=[node]\n",
    "            while(bfs):\n",
    "                curr=bfs.pop(0)\n",
    "                visited[curr][0]=True\n",
    "                last.append(curr)\n",
    "                posStr+=visited[curr][2]\n",
    "                for neighbour in adjList[curr]:\n",
    "                    if(not visited[neighbour][0]):\n",
    "                        bfs.append(neighbour)\n",
    "        ind+=1\n",
    "    last.sort(key = int)\n",
    "    if('^' in posStr):\n",
    "        candidateString=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "        cc.append(candidateString)\n",
    "        cc_positions.append(last)\n",
    "#     print('connected components:')\n",
    "#     print(cc)\n",
    "    return cc, cc_positions\n",
    "\n",
    "def conll_nounPhrase_chunking(tabbed_entries):\n",
    "    spans=[]\n",
    "    span=[]\n",
    "    for tabbed_entry in tabbed_entries:\n",
    "        entry=[]\n",
    "        if((tabbed_entry[3]==proper_noun_tag)|(tabbed_entry[3]==common_noun_tag)):\n",
    "            entry=tabbed_entry\n",
    "        if(tabbed_entry[3]==prep_tag):\n",
    "            head=int(tabbed_entry[6])-1\n",
    "            if(head>0):\n",
    "                head_entry=tabbed_entries[head]\n",
    "                if((head_entry[3]==proper_noun_tag)|(head_entry[3]==common_noun_tag)):\n",
    "                    entry=tabbed_entry\n",
    "        if(entry):\n",
    "            if(int(entry[0])>1):\n",
    "                if(span):\n",
    "                    if((int(entry[0])-int(span[-1][0]))>1):\n",
    "                        spans.append(span)\n",
    "                        span=[entry]\n",
    "                    else:\n",
    "                        span.append(entry)\n",
    "                else:\n",
    "                    span=[entry]\n",
    "            else:\n",
    "                span=[entry]\n",
    "    if(spans):\n",
    "        if(spans[-1][0]!=span[0]):\n",
    "            spans.append(span)\n",
    "    else:\n",
    "        if(span):\n",
    "            spans.append(span)\n",
    "    \n",
    "    final_spans=[]\n",
    "    final_spans_positions=[]\n",
    "    for span in spans:\n",
    "        minIndex=int(span[0][0])\n",
    "        maxIndex=int(span[-1][0])\n",
    "        visited={}\n",
    "        adjList={}\n",
    "        for entry in span:\n",
    "            visited[entry[0]]=[False,entry[1],entry[3]]\n",
    "            if(entry[0] not in adjList.keys()):\n",
    "                adjList[entry[0]]=[]\n",
    "            dependency=entry[6]\n",
    "            if((int(dependency)>=minIndex)&(int(dependency)<=maxIndex)):\n",
    "                adjList[entry[0]].append(dependency)\n",
    "                if(dependency not in adjList.keys()):\n",
    "                    adjList[dependency]=[]\n",
    "                adjList[dependency].append(entry[0])\n",
    "        retTup=getConnectedComponents(visited,adjList)\n",
    "        final_spans.extend(retTup[0])\n",
    "        final_spans_positions.extend(retTup[1])\n",
    "    return final_spans,final_spans_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(annotated_mention_list,output_mentions_list):\n",
    "\n",
    "    # print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "    print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "    \n",
    "    precision=(tp_counter_inner)/(tp_counter_inner+fp_counter_inner)\n",
    "    recall=(tp_counter_inner)/(tp_counter_inner+fn_counter_inner)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized.txt\", \"w\")\n",
    "# file_text1=''\n",
    "\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized_POSTAG.txt\", \"w\")\n",
    "# file_text=''\n",
    "\n",
    "# candidates=[]\n",
    "# # sentId=0\n",
    "# tweetId=0\n",
    "# CTrie=trie.Trie(\"ROOT\")\n",
    "# # for sentence in sentenceList:\n",
    "# for tweet in tweetList:\n",
    "# #     result=result_conll[sentId]\n",
    "#     result=result_conll[tweetId]\n",
    "#     tweet_word_list=[]\n",
    "#     conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "#     for result_line in conll_results:\n",
    "#         file_text+=result_line[1]+'\\t'+result_line[3]+'\\n'\n",
    "#         tweet_word_list+=result_line[1]\n",
    "# #         file_text1+=result_line[1]+'\\tO'+'\\n'\n",
    "# #         print('result_line: ', result_line)\n",
    "#     file_text+='\\n'\n",
    "#     tweetId+=1\n",
    "#     tweetMentions=mentionList[tweetId]\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, tweetMentions)\n",
    "#     print(len(tweet_word_list),len(tweet_encoding_list))\n",
    "#     for ind, word in enumerate(tweet_word_list):\n",
    "#         file_text1+=word+'\\t'+tweet_encoding_list[ind]+'\\n'\n",
    "#     file_text1+='\\n'\n",
    "# #     sentId+=1\n",
    "#     tweetId+=1\n",
    "\n",
    "# f.write(file_text)\n",
    "# f.close()\n",
    "\n",
    "# f1.write(file_text1)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084 1004 3084\n"
     ]
    }
   ],
   "source": [
    "candidates=[]\n",
    "sentId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "phase1outputs=[]\n",
    "for sentence in sentenceList:\n",
    "    result=result_conll[sentId]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    sentence_candidates,sentence_candidates_positions=conll_nounPhrase_chunking(conll_results)\n",
    "#     print(sentence)\n",
    "#     print(conll_results)\n",
    "#     print(sentence_candidates)\n",
    "\n",
    "    candidate_ind=0\n",
    "    phase1Out=''\n",
    "#     for candidate in ne_List_allCheck:\n",
    "#         position = '*'+'*'.join(str(v) for v in candidate.position)\n",
    "#         position=position+'*'\n",
    "#         candidate.set_sen_index(sen_index)\n",
    "#         phase1Out+=(((candidate.phraseText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "\n",
    "    for candidateText in sentence_candidates:\n",
    "#         print(candidateText)\n",
    "        candidateText=candidateText.lower()\n",
    "        phase1outputs.append((((candidateText).lstrip(string.punctuation)).strip()))\n",
    "        position = '*'+'*'.join(str(v) for v in sentence_candidates_positions[candidate_ind])\n",
    "        position=position+'*'\n",
    "#         print(candidateText,sentence_candidates_positions[candidate_ind])\n",
    "        CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "        candidate_ind+=1\n",
    "        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "        \n",
    "#     candidates.append(sentence_candidates)\n",
    "    candidates.append(phase1Out)\n",
    "\n",
    "    sentId+=1\n",
    "#     print('===========')\n",
    "print(len(sentenceList),len(tweets_unpartitoned),len(candidates))\n",
    "\n",
    "tweet_sentence_df['phase1Candidates']=candidates\n",
    "\n",
    "# print(tweet_sentence_df['phase1Candidates'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "candidates=CTrie.displayTrie(\"\",[])\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2=time.time()\n",
    "\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  3754 3754 157\n",
      "-0.16049603488843672\n",
      "For entities:  (101, 6)\n",
      "For non-entities:  (41, 6)\n",
      "For ambiguous:  (15, 6)\n",
      "For entities:  (101, 6)\n",
      "For non-entities:  (41, 6)\n",
      "For ambiguous:  (15, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  3672 incomplete tweets:  82\n",
      "16\n",
      "16\n",
      "final tally:  3754 3754\n",
      "524:  524    [[nyc]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2 = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted= Phase2.executor(max_batch_value,tweet_sentence_df,CTrie,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df)\n",
    "time3=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_unpartitoned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 538 427\n",
      "precision:  0.40486725663716816\n",
      "recall:  0.46153846153846156\n",
      "f_measure:  0.4313494401885681\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "\n",
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    \n",
    "#     output_mentions_list=flatten(complete_tweet_dataframe_grouped_df_sorted[complete_tweet_dataframe_grouped_df_sorted.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "    all_mentions+=annotated_mention_list\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner=len(output_mentions_list)\n",
    "#     total_tagged+=len(output_mentions_list)\n",
    "#     total_annotations+=len(annotated_mention_list)\n",
    "    \n",
    "#     while(annotated_mention_list):\n",
    "#         if(len(output_mentions_list)):\n",
    "#             annotated_candidate= annotated_mention_list.pop()\n",
    "#             if(annotated_candidate in output_mentions_list):\n",
    "#                 output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "#                 tp_counter_inner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "#     fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count+=tp_counter_inner\n",
    "#     false_positive_count+=fp_counter_inner\n",
    "#     false_negative_count+=fn_counter_inner\n",
    "\n",
    "# print(total_annotations,total_tagged)\n",
    "# print(true_positive_count,false_positive_count,false_negative_count)\n",
    "# print(phase1outputs)\n",
    "get_F1(all_mentions,phase1outputs)\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.1327850818634\n",
      "3.5350229740142822\n"
     ]
    }
   ],
   "source": [
    "# precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "# recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "# f_measure=2*(precision*recall)/(precision+recall)\n",
    "# print(precision,recall,f_measure)\n",
    "\n",
    "print(time2-time1)\n",
    "print(time3-time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4703423992849669\n",
      "0.5892456223684872\n",
      "0.5177327012634292\n"
     ]
    }
   ],
   "source": [
    "#SVM phase 1\n",
    "precision_arr=[0.40486725663716816,0.4861477572559367,0.6225225225225225,0.38872549019607844,0.4494489698131289]\n",
    "recall_arr=[0.46153846153846156,0.6470588235294118,0.6175156389633601,0.4345205479452055,0.7855946398659966]\n",
    "f1_arr=[0.4313494401885681,0.5551789077212806,0.6200089726334679,0.4103492884864166,0.5717768972874124]\n",
    "#SVM full system\n",
    "# precision_arr=[0.6342794759825328,0.7948482060717571,0.8933189655172413,0.9364406779661016,0.90633608815427]\n",
    "# recall_arr=[0.7326607818411097,0.7585601404741001,0.7408400357462019,0.6054794520547945,0.8266331658291457]\n",
    "# f1_arr=[0.6799297834991223,0.7762803234501349,0.8099658036150463,0.7354409317803661,0.8646517739816031]\n",
    "\n",
    "print(sum(precision_arr)/len(precision_arr))\n",
    "print(sum(recall_arr)/len(recall_arr))\n",
    "print(sum(f1_arr)/len(f1_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-a150bcecdd2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-a150bcecdd2b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2425 487 905\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#3K\n",
    "#SVM\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#RF\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#LR\n",
    "2415 460 915\n",
    "0.84 0.7252252252252253 0.7784045124899275\n",
    "\n",
    "#1K\n",
    "#SVM\n",
    "# precision:  0.8075117370892019\n",
    "# recall:  0.63003663003663\n",
    "# f_measure:  0.7078189300411523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ritter- TwitterNLP in Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "ritter_annotator=pd.read_csv(\"/Users/satadisha/Documents/GitHub/my-baseline-setup/ritter-billdeblasio-output.csv\",sep =',',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n",
      "['ID', 'HashTags', 'TweetText', 'Output', 'mentions_other', 'URLs', 'User']\n"
     ]
    }
   ],
   "source": [
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "print(len(ritter_annotator))\n",
    "print(ritter_annotator.columns.tolist())\n",
    "CTrie_ritter=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "ritter_annotated_candidates=ritter_annotator['Output'].tolist()\n",
    "for candidate in ritter_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_ritter.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesinRitterTrie=CTrie_ritter.displayTrie(\"\",[])\n",
    "print(len(candidatesinRitterTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  3754 3754 112\n",
      "-0.18265485071086004\n",
      "For entities:  (98, 6)\n",
      "For non-entities:  (8, 6)\n",
      "For ambiguous:  (6, 6)\n",
      "For entities:  (98, 6)\n",
      "For non-entities:  (8, 6)\n",
      "For ambiguous:  (6, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  3701 incomplete tweets:  53\n",
      "16\n",
      "16\n",
      "final tally:  3754 3754\n",
      "524:  524    [[nyc]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_Ritter = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_Ritter, converted_candidates_w_Ritter, complete_tweet_dataframe_grouped_df_sorted_w_Ritter= Phase2_w_Ritter.executor(max_batch_value,tweet_sentence_df,CTrie_ritter,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...</td>\n",
       "      <td>[RobertBelfi, RobertBelfi, RobertBelfi, Robert...</td>\n",
       "      <td>[Presidential candidate 😂😂 #BilldeBlasio wife ...</td>\n",
       "      <td>[, since #obama::*9*10*||, , de blasio::*1*2*||]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [], [(de blasio, g), (mayor, b)]]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wildbil42007816]</td>\n",
       "      <td>[Say it isn't so, a commie corrupt,  shirley y...</td>\n",
       "      <td>[shirley::*10*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(shirley, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, nyc, BilldeBlasio, nyc, BilldeB...</td>\n",
       "      <td>[jnrbllc, jnrbllc, jnrbllc, jnrbllc]</td>\n",
       "      <td>[@BilldeBlasio To our wonderful Mayor #BilldeB...</td>\n",
       "      <td>[mayor #billdeblasio::*5*6*||, nyc::*11*||, pr...</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(mayor, b)], [], [(city, b)], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...</td>\n",
       "      <td>[TheMtljo, TheMtljo, TheMtljo, TheMtljo]</td>\n",
       "      <td>[Presidential candidate 😂😂 #BilldeBlasio wife ...</td>\n",
       "      <td>[, obama::*10*||, , de blasio::*1*2*||]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [], [(de blasio, g), (mayor, b)]]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[, , ]</td>\n",
       "      <td>[PTiddylicker, PTiddylicker, PTiddylicker]</td>\n",
       "      <td>[if you love CORRUPT RACIST HYPOCRITES WHO ARE...</td>\n",
       "      <td>[, nyc::*3*||, ]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [nyc], []]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [(nyc, g)], []]</td>\n",
       "      <td>[[], [nyc], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID                 index   entry_batch        sentID  \\\n",
       "0        0  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "1        1                 [nan]           [0]           [0]   \n",
       "2        2  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "3        3  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "4        4       [nan, nan, nan]     [0, 0, 0]     [0, 1, 2]   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...   \n",
       "1                                                 []   \n",
       "2  [BilldeBlasio, nyc, BilldeBlasio, nyc, BilldeB...   \n",
       "3  [BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...   \n",
       "4                                             [, , ]   \n",
       "\n",
       "                                                user  \\\n",
       "0  [RobertBelfi, RobertBelfi, RobertBelfi, Robert...   \n",
       "1                                  [Wildbil42007816]   \n",
       "2               [jnrbllc, jnrbllc, jnrbllc, jnrbllc]   \n",
       "3           [TheMtljo, TheMtljo, TheMtljo, TheMtljo]   \n",
       "4         [PTiddylicker, PTiddylicker, PTiddylicker]   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [Presidential candidate 😂😂 #BilldeBlasio wife ...   \n",
       "1  [Say it isn't so, a commie corrupt,  shirley y...   \n",
       "2  [@BilldeBlasio To our wonderful Mayor #BilldeB...   \n",
       "3  [Presidential candidate 😂😂 #BilldeBlasio wife ...   \n",
       "4  [if you love CORRUPT RACIST HYPOCRITES WHO ARE...   \n",
       "\n",
       "                                    phase1Candidates        annotation  \\\n",
       "0   [, since #obama::*9*10*||, , de blasio::*1*2*||]  [[], [], [], []]   \n",
       "1                                  [shirley::*10*||]              [[]]   \n",
       "2  [mayor #billdeblasio::*5*6*||, nyc::*11*||, pr...  [[], [], [], []]   \n",
       "3            [, obama::*10*||, , de blasio::*1*2*||]  [[], [], [], []]   \n",
       "4                                   [, nyc::*3*||, ]      [[], [], []]   \n",
       "\n",
       "  stanford_candidates            output_mentions              completeness  \\\n",
       "0    [[], [], [], []]  [[], [], [], [de blasio]]  [True, True, True, True]   \n",
       "1                [[]]                       [[]]                    [True]   \n",
       "2    [[], [], [], []]           [[], [], [], []]  [True, True, True, True]   \n",
       "3    [[], [], [], []]  [[], [], [], [de blasio]]  [True, True, True, True]   \n",
       "4        [[], [], []]            [[], [nyc], []]        [True, True, True]   \n",
       "\n",
       "    current_minus_entry                       candidates_with_label  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0]  [[], [], [], [(de blasio, g), (mayor, b)]]   \n",
       "1                 [0.0]                            [[(shirley, b)]]   \n",
       "2  [0.0, 0.0, 0.0, 0.0]         [[(mayor, b)], [], [(city, b)], []]   \n",
       "3  [0.0, 0.0, 0.0, 0.0]  [[], [], [], [(de blasio, g), (mayor, b)]]   \n",
       "4       [0.0, 0.0, 0.0]                        [[], [(nyc, g)], []]   \n",
       "\n",
       "        only_good_candidates ambiguous_candidates  \n",
       "0  [[], [], [], [de blasio]]     [[], [], [], []]  \n",
       "1                       [[]]                 [[]]  \n",
       "2           [[], [], [], []]     [[], [], [], []]  \n",
       "3  [[], [], [], [de blasio]]     [[], [], [], []]  \n",
       "4            [[], [nyc], []]         [[], [], []]  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted_w_Ritter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949 94 245\n",
      "precision:  0.909875359539789\n",
      "recall:  0.7948073701842546\n",
      "f_measure:  0.8484577559231113\n"
     ]
    }
   ],
   "source": [
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted_w_Ritter.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    all_mentions+=annotated_mention_list\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8988614198410645\n",
      "0.638884707457936\n",
      "0.7415294076458718\n"
     ]
    }
   ],
   "source": [
    "precision_arr=[0.8303964757709251,0.8766716196136701,0.9215896885069818,0.9557739557739557,0.909875359539789]\n",
    "recall_arr=[0.47540983606557374,0.517998244073749,0.7667560321715817,0.6394520547945205,0.7948073701842546]\n",
    "f1_arr=[0.6046511627906976,0.6512141280353201,0.8370731707317073,0.7662508207485227,0.8484577559231113]\n",
    "\n",
    "print(sum(precision_arr)/len(precision_arr))\n",
    "print(sum(recall_arr)/len(recall_arr))\n",
    "print(sum(f1_arr)/len(f1_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 1086\n",
      "25 1061 642\n",
      "0 205 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-81ec6a8db503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_annotations_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_tagged_ritter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfalse_positive_count_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfalse_negative_count_ritter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mget_F1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_mentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-90434dcfeead>\u001b[0m in \u001b[0;36mget_F1\u001b[0;34m(annotated_mention_list, output_mentions_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mrecall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mf_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_ritter=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Ritter[complete_tweet_dataframe_grouped_df_sorted_w_Ritter.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610108303249098 0.8451452870304749 0.8530042918454936\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM\n",
    "0.8605577689243028 0.8428520752039731 0.8516129032258064\n",
    "\n",
    "#RF\n",
    "0.8614435981138919 0.8433948863636364 0.8523237035707877\n",
    "\n",
    "#LR\n",
    "0.8610108303249098 0.8451452870304749 0.8530042918454936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaguilar as Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/satadisha/Documents/GitHub/tweebo-parser/billnye_outputs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-a02b9d553217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'…‘’'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/satadisha/Documents/GitHub/tweebo-parser/billnye_outputs.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# f = open(\"/Users/satadisha/Documents/GitHub/3A/split2_output.txt\",'r')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/satadisha/Documents/GitHub/tweebo-parser/billnye_outputs.txt'"
     ]
    }
   ],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n",
    "\n",
    "\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/tweebo-parser/billnye_outputs.txt\",'r')\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/3A/split2_output.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTrie_gaguilar=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588\n",
      "588\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "file_text=f.read()\n",
    "\n",
    "output_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n'))) #conll\n",
    "print(len(output_sentences))\n",
    "gaguilar_annotated_candidates=[]\n",
    "for line in output_sentences:\n",
    "    if(line):\n",
    "        tabs=line.split('\\t')\n",
    "        if(tabs):\n",
    "            for candidate in tabs:\n",
    "                gaguilar_annotated_candidates.append(candidate)\n",
    "print(len(gaguilar_annotated_candidates))\n",
    "for candidate in gaguilar_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_gaguilar.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesingaguilarTrie=CTrie_gaguilar.displayTrie(\"\",[])\n",
    "print(len(candidatesingaguilarTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  3084 3084 145\n",
      "-0.2450478489671107\n",
      "For entities:  (108, 6)\n",
      "For non-entities:  (16, 6)\n",
      "For ambiguous:  (21, 6)\n",
      "For entities:  (108, 6)\n",
      "For non-entities:  (16, 6)\n",
      "For ambiguous:  (21, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  2909 incomplete tweets:  175\n",
      "16\n",
      "16\n",
      "final tally:  3084 3084\n",
      "524:  524    [[], [], [save us]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_gaguilar = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_gaguilar, converted_candidates_w_gaguilar, complete_tweet_dataframe_grouped_df_sorted_w_gaguilar= Phase2_w_gaguilar.executor(max_batch_value,tweet_sentence_df,CTrie_gaguilar,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385 401 408\n",
      "precision:  0.48982188295165396\n",
      "recall:  0.4854981084489281\n",
      "f_measure:  0.48765041165294487\n"
     ]
    }
   ],
   "source": [
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    all_mentions+=annotated_mention_list\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-326-e892b8ef4cc4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-326-e892b8ef4cc4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    precision=[ ,0.9312906220984215]\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "precision=[ ,0.9312906220984215]\n",
    "recall=[,0.8400335008375209]\n",
    "f1=[,0.8833113166006163]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2986\n",
      "2484 502 471\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_gaguilar=0\n",
    "false_positive_count_gaguilar=0\n",
    "false_negative_count_gaguilar=0\n",
    "\n",
    "total_annotations_gaguilar=0\n",
    "total_tagged_gaguilar=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_gaguilar=[]\n",
    "    tp_counter_inner_gaguilar=0\n",
    "    fp_counter_inner_gaguilar=0\n",
    "    fn_counter_inner_gaguilar=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_gaguilar=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_gaguilar.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_gaguilar=flatten(complete_tweet_dataframe_grouped_df_sorted_w_gaguilar[complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_gaguilar=len(output_mentions_list_gaguilar)\n",
    "    total_tagged_gaguilar+=len(output_mentions_list_gaguilar)\n",
    "    total_annotations_gaguilar+=len(annotated_mention_list_gaguilar)\n",
    "    \n",
    "    while(annotated_mention_list_gaguilar):\n",
    "        if(len(output_mentions_list_gaguilar)):\n",
    "            annotated_candidate= annotated_mention_list_gaguilar.pop()\n",
    "            if(annotated_candidate in output_mentions_list_gaguilar):\n",
    "                output_mentions_list_gaguilar.pop(output_mentions_list_gaguilar.index(annotated_candidate))\n",
    "                tp_counter_inner_gaguilar+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_gaguilar.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_gaguilar.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_gaguilar=len(unrecovered_annotated_mention_list_gaguilar)\n",
    "    fp_counter_inner_gaguilar=all_postitive_counter_inner_gaguilar- tp_counter_inner_gaguilar\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_gaguilar+=tp_counter_inner_gaguilar\n",
    "    false_positive_count_gaguilar+=fp_counter_inner_gaguilar\n",
    "    false_negative_count_gaguilar+=fn_counter_inner_gaguilar\n",
    "\n",
    "print(total_annotations_gaguilar,total_tagged_gaguilar)\n",
    "print(true_positive_count_gaguilar,false_positive_count_gaguilar,false_negative_count_gaguilar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8318821165438715 0.8406091370558376 0.8362228581046962\n"
     ]
    }
   ],
   "source": [
    "precision_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_positive_count_gaguilar)\n",
    "recall_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_negative_count_gaguilar)\n",
    "f_measure_gaguilar=2*(precision_gaguilar*recall_gaguilar)/(precision_gaguilar+recall_gaguilar)\n",
    "print(precision_gaguilar,recall_gaguilar,f_measure_gaguilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LR\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#RF\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#SVM\n",
    "0.8318821165438715 0.8406091370558376 0.8362228581046962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "3374 3849\n",
      "2501 1348 673\n"
     ]
    }
   ],
   "source": [
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# neuroner_file = open('mentions_output_tweets_3K.txt', 'r') \n",
    "# neuroner_lines = neuroner_file.readlines()\n",
    "# print(len(neuroner_lines))\n",
    "# line_count=0\n",
    "# neuroner_annotated_candidates=[]\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "#     neuroner_output=neuroner_lines[line_count]\n",
    "#     output_mentions_list_neuroner=[candidate.lower().strip(string.punctuation).strip() for candidate in neuroner_output.split(',') if(candidate.strip(string.punctuation).strip())]\n",
    "#     neuroner_annotated_candidates.extend(output_mentions_list_neuroner)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner - tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "    \n",
    "#     line_count+=1\n",
    "    \n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6497791634190699 0.7879647132955262 0.7122312402107361\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroNER as Phase I Entity Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "[]\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "# tweet_sentence_df_2nd_copy=tweet_sentence_df.copy(deep=True)\n",
    "# CTrie_neuroner=trie.Trie(\"ROOT\")\n",
    "# print(len(neuroner_annotated_candidates))\n",
    "# print(phase2stopwordList)\n",
    "\n",
    "# for candidateText in neuroner_annotated_candidates:\n",
    "# #     print(candidateText)\n",
    "#     if(candidateText not in all_stopwords):\n",
    "#         CTrie_neuroner.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "# candidatesinNeuronerTrie=CTrie_neuroner.displayTrie(\"\",[])\n",
    "# print(len(candidatesinNeuronerTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4721 4721 1057\n",
      "-0.2661675732774245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4525 incomplete tweets:  196\n",
      "16\n",
      "16\n",
      "final tally:  4721 4721\n",
      "524:  524    [[world news, fbi, fisa, trump, washington post]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "# Phase2_w_Neuroner = phase2.EntityResolver()\n",
    "# candidate_base_post_Phase2_w_Neuroner, converted_candidates_w_Neuroner, complete_tweet_dataframe_grouped_df_sorted_w_Neuroner= Phase2_w_Neuroner.executor(max_batch_value,tweet_sentence_df_2nd_copy,CTrie_neuroner,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_2nd_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted_w_Neuroner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 3129\n",
      "2507 622 549\n"
     ]
    }
   ],
   "source": [
    "# # from ast import literal_eval\n",
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     output_mentions_list_neuroner=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Neuroner[complete_tweet_dataframe_grouped_df_sorted_w_Neuroner.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "# #     print(row['TweetText'])\n",
    "# #     print(tweet_ID, annotated_mention_list)\n",
    "# #     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner- tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "\n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8012144455097475 0.8203534031413613 0.8106709781729993\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2095\n",
      "1571 524 875\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    output_mentions_list_ritter=[]\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "    candidate_list_ritter=flatten(ritter_annotator[ritter_annotator.ID==tweet_ID].Output.tolist(),[])\n",
    "    for ritter_candidate in candidate_list_ritter:\n",
    "        output_mentions_list_ritter+= [remAmpersand(elem).strip(string.punctuation).strip() for elem in ritter_candidate.lower().split(',') if(elem)]\n",
    "        \n",
    "#     output_mentions_list_ritter= [remAmpersand(elem).strip(string.punctuation).strip().lower() for elem in candidate_list_ritter]\n",
    "    \n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list_ritter)\n",
    "#     print(output_mentions_list_ritter)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498806682577566 0.6422730989370401 0.6919180797181238\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with different annotators:\n",
    "\n",
    "##With CS+ in phase 1:\n",
    "0.7982625482625483 0.7359833877187778 0.7658589288470442\n",
    "\n",
    "##With Turboparse chunker in phase 1:\n",
    "0.8381118881118881 0.7104327208061648 0.7690086621751684\n",
    "\n",
    "##Just TwitterNLP:\n",
    "0.7460620525059666 0.6335630320226996 0.6852257781674704\n",
    "\n",
    "##With TwitterNLP entity annotator in phase 1:\n",
    "0.856 0.8288732394366197 0.8422182468694097\n",
    "\n",
    "##Just NeuroNER:\n",
    "0.6497791634190699 0.7879647132955262 0.7122312402107361\n",
    "\n",
    "##With NeuroNER entity annotator in phase 1:\n",
    "0.8012144455097475 0.8203534031413613 0.8106709781729993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
