{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n"
     ]
    }
   ],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats\n",
    "\n",
    "# import phase2_Trie_baseline_reintroduction_effectiveness as phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=word_tokenize(\"Very well explained take on Carter/ Russia/ FISA/ Trump's sitch.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "        elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/venezuela.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/roevwade.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/wnut17test.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "# print(len(tweets_unpartitoned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287\n"
     ]
    }
   ],
   "source": [
    "# tweets_1=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_2=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_3=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned= pd.concat([tweets_1,tweets_2,tweets_3])\n",
    "print(len(tweets_unpartitoned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with TurboParser NP Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1=time.time()\n",
    "df_holder=[]\n",
    "batch_number=0\n",
    "# tweetList=[]\n",
    "sentenceList=[]\n",
    "sentID=0\n",
    "sentID_to_tweet_ID={}\n",
    "mentionList=[]\n",
    "\n",
    "for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "    index=row.Index\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    hashtags=str(row.HashTags)\n",
    "\n",
    "    user=str(row.User)\n",
    "    tweetText=str(row.TweetText)\n",
    "    annot_raw=\"\"\n",
    "    stanford_candidates=\"\"\n",
    "    ritter_candidates = \"\"\n",
    "    calai_candidates=\"\"\n",
    "\n",
    "    ne_List_final=[]\n",
    "    userMention_List_final=[]\n",
    "    tweetSentenceList=splitSentence(tweetText)\n",
    "    sentenceList.extend(tweetSentenceList)\n",
    "    \n",
    "    for sentence in tweetSentenceList:\n",
    "        sentID_to_tweet_ID[sentID]=int(index)\n",
    "        sentID+=1\n",
    "    \n",
    "    mentions=[]\n",
    "    \n",
    "#     if(len(tweetSentenceList)!=len(str(row.mentions_other_BIO).split(';'))):\n",
    "#         print('index: ',index)\n",
    "    \n",
    "    for sentence_level in str(row.mentions_other).split(';'):\n",
    "        if(sentence_level):\n",
    "            for mention in sentence_level.split(','):\n",
    "                if(mention):\n",
    "                    mentions.append(mention.strip())\n",
    "    \n",
    "#     if(len(tweetSentenceList)!= len(mentions)):\n",
    "#         print('tally: ',len(tweetSentenceList), len(mentions))\n",
    "#         print(tweetSentenceList)\n",
    "#         print(row.mentions_other)\n",
    "#     print(mentions)\n",
    "\n",
    "    mentionList.append(mentions)\n",
    "#     tweetList.append(tweetText)\n",
    "    \n",
    "    for sen_index in range(len(tweetSentenceList)):\n",
    "        sentence=tweetSentenceList[sen_index]\n",
    "        annotation=[]\n",
    "        tweetWordList=getWords(sentence)\n",
    "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
    "#         phase1Candidates\n",
    "        dict1 = {'tweetID':str(index), 'sentID':str(sen_index), 'hashtags':hashtags, 'user':user, 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList, 'start_time':now,'entry_batch':batch_number,'annotation':annotation,'stanford_candidates':stanford_candidates,'ritter_candidates':ritter_candidates,'calai_candidates':calai_candidates}\n",
    "        df_holder.append(dict1)\n",
    "\n",
    "#     for candidate in ne_List_final:\n",
    "#         #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
    "#         candidateText=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
    "#         candidateText=(candidateText.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "#         candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#         # if(index==9423):\n",
    "#         #     print(candidateText)\n",
    "#         combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
    "#         if not ((candidateText in combined)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
    "#             self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),candidate.features,batch_number)\n",
    "\n",
    "#     NE_list_phase1+=ne_List_final\n",
    "\n",
    "#     UserMention_list+=userMention_List_final\n",
    "\n",
    "tweet_sentence_df= pd.DataFrame(df_holder,columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','tweetwordList', 'start_time','entry_batch','annotation','stanford_candidates','ritter_candidates','calai_candidates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = ['Guangdong Public University of Foreign Studies is located in Guangzhou.',\n",
    "#              'Lucy is in Kolkata with diamonds.','Bernie Sanders says his fight is for the working class.','elizabeth warren chaired the CBFC',\n",
    "#              'coronavirus is scary!','U.S. is struggling'\n",
    "#             ]\n",
    "\n",
    "# print(len(mentionList),len(tweetList),len(sentID_to_tweet_ID.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conll=[]\n",
    "try:\n",
    "# #     result_stanford = tweebo_api.parse_stanford(text_data)\n",
    "# #     result_conll = tweebo_api.parse_conll(text_data)\n",
    "    result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[:1000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[1000:2000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[2000:3000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[3000:])\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse done!\n",
      "1863\n"
     ]
    }
   ],
   "source": [
    "print('parse done!')\n",
    "print(len(sentenceList))\n",
    "\n",
    "# print(len(tweetList))\n",
    "# print(len(result_conll))\n",
    "\n",
    "# print(sentID_to_tweet_ID[15])\n",
    "# print(result_conll[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just printing the twokenized sentences\n",
    "# sentId=0\n",
    "# df_holder=[]\n",
    "# df_columns=['tweet_id','sentence_id','word']\n",
    "# for sentence in sentenceList:\n",
    "# #     print(sentence)\n",
    "#     sentence_tokens= flatten([split_apostrophe(elem) for elem in getWordsII(sentence)],[])\n",
    "# #     print(sentence_tokens)\n",
    "# #     result=result_conll[sentId]\n",
    "#     for token in sentence_tokens:\n",
    "# #     for result_line in result.split('\\n'):\n",
    "# #         tabs = result_line.split('\\t')\n",
    "#         df_dict={'tweet_id':str(sentID_to_tweet_ID[sentId]),'sentence_id':str(sentId), 'word':token}\n",
    "#         df_holder.append(df_dict)\n",
    "#     sentId+=1\n",
    "\n",
    "# df_out = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print('pre-encoding dataframe: ', len(df_out))\n",
    "\n",
    "# #align mentions with tweets and generate BIO encoding:\n",
    "# encoded_df_columns=['Sentence #','Word','Tag']\n",
    "# encoded_df_holder=[]\n",
    "\n",
    "# # file_text=''\n",
    "# for index, mentions in enumerate(mentionList):\n",
    "#     tweet_sentID_list= df_out[df_out['tweet_id']==str(index)].sentence_id.tolist()\n",
    "#     tweet_word_list= df_out[df_out['tweet_id']==str(index)].word.tolist()\n",
    "#     print('tweet ID:', index,mentions)\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, mentions)\n",
    "    \n",
    "# #     print('tallying list lengths: ',len(tweet_sentID_list),len(tweet_word_list),len(tweet_encoding_list))\n",
    "    \n",
    "#     for encoded_list_index, sentID in enumerate(tweet_sentID_list):\n",
    "#         encoded_df_dict={'Sentence #':tweet_sentID_list[encoded_list_index], 'Word':tweet_word_list[encoded_list_index], 'Tag':tweet_encoding_list[encoded_list_index]}\n",
    "#         encoded_df_holder.append(encoded_df_dict)\n",
    "# #         file_text+=tweet_word_list[encoded_list_index]+'\\t'+tweet_encoding_list[encoded_list_index]+'\\n'\n",
    "# #     file_text+='\\n'\n",
    "\n",
    "# encoded_df_out=pd.DataFrame(encoded_df_holder,columns=encoded_df_columns)\n",
    "# print('post-encoding dataframe: ', len(encoded_df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/tweets_3k_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/venezuela_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billnye_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/pikapika_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/ripcity_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/roevwade_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "# print(encoded_df_out.tail(40))\n",
    "\n",
    "# import re\n",
    "# mystr='Macron.'\n",
    "# print(split_apostrophe(mystr))\n",
    "# print(mystr.split('-'))\n",
    "# re.split(\"(-)\",mystr)\n",
    "# if((mystr.count('.')==1)&(mystr.endswith('.'))):\n",
    "#     temp=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",mystr)))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "# conll_nounPhrase_chunking(conll_results)\n",
    "\n",
    "def getConnectedComponents(visited, adjList):\n",
    "    cc=[]\n",
    "    cc_positions=[]\n",
    "    nodeList=list(visited.keys())\n",
    "#     print('**',visited,adjList)\n",
    "    for ind in range(len(nodeList)):\n",
    "        node=nodeList[ind]\n",
    "#         print('==>',node)\n",
    "        if not(visited[node][0]):\n",
    "            if(ind>0):\n",
    "                last.sort(key = int)\n",
    "                if('^' in posStr):\n",
    "#                     print('::',last)\n",
    "                    candidateStringInner=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "                    cc.append(candidateStringInner)\n",
    "                    cc_positions.append(last)\n",
    "            last=[]\n",
    "            posStr=''\n",
    "            bfs=[node]\n",
    "            while(bfs):\n",
    "                curr=bfs.pop(0)\n",
    "                visited[curr][0]=True\n",
    "                last.append(curr)\n",
    "                posStr+=visited[curr][2]\n",
    "                for neighbour in adjList[curr]:\n",
    "                    if(not visited[neighbour][0]):\n",
    "                        bfs.append(neighbour)\n",
    "        ind+=1\n",
    "    last.sort(key = int)\n",
    "    if('^' in posStr):\n",
    "        candidateString=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "        cc.append(candidateString)\n",
    "        cc_positions.append(last)\n",
    "#     print('connected components:')\n",
    "#     print(cc)\n",
    "    return cc, cc_positions\n",
    "\n",
    "def conll_nounPhrase_chunking(tabbed_entries):\n",
    "    spans=[]\n",
    "    span=[]\n",
    "    for tabbed_entry in tabbed_entries:\n",
    "        entry=[]\n",
    "        if((tabbed_entry[3]==proper_noun_tag)|(tabbed_entry[3]==common_noun_tag)):\n",
    "            entry=tabbed_entry\n",
    "        if(tabbed_entry[3]==prep_tag):\n",
    "            head=int(tabbed_entry[6])-1\n",
    "            if(head>0):\n",
    "                head_entry=tabbed_entries[head]\n",
    "                if((head_entry[3]==proper_noun_tag)|(head_entry[3]==common_noun_tag)):\n",
    "                    entry=tabbed_entry\n",
    "        if(entry):\n",
    "            if(int(entry[0])>1):\n",
    "                if(span):\n",
    "                    if((int(entry[0])-int(span[-1][0]))>1):\n",
    "                        spans.append(span)\n",
    "                        span=[entry]\n",
    "                    else:\n",
    "                        span.append(entry)\n",
    "                else:\n",
    "                    span=[entry]\n",
    "            else:\n",
    "                span=[entry]\n",
    "    if(spans):\n",
    "        if(spans[-1][0]!=span[0]):\n",
    "            spans.append(span)\n",
    "    else:\n",
    "        if(span):\n",
    "            spans.append(span)\n",
    "    \n",
    "    final_spans=[]\n",
    "    final_spans_positions=[]\n",
    "    for span in spans:\n",
    "        minIndex=int(span[0][0])\n",
    "        maxIndex=int(span[-1][0])\n",
    "        visited={}\n",
    "        adjList={}\n",
    "        for entry in span:\n",
    "            visited[entry[0]]=[False,entry[1],entry[3]]\n",
    "            if(entry[0] not in adjList.keys()):\n",
    "                adjList[entry[0]]=[]\n",
    "            dependency=entry[6]\n",
    "            if((int(dependency)>=minIndex)&(int(dependency)<=maxIndex)):\n",
    "                adjList[entry[0]].append(dependency)\n",
    "                if(dependency not in adjList.keys()):\n",
    "                    adjList[dependency]=[]\n",
    "                adjList[dependency].append(entry[0])\n",
    "        retTup=getConnectedComponents(visited,adjList)\n",
    "        final_spans.extend(retTup[0])\n",
    "        final_spans_positions.extend(retTup[1])\n",
    "    return final_spans,final_spans_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(annotated_mention_list,output_mentions_list):\n",
    "\n",
    "    # print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "    print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "    \n",
    "    precision=(tp_counter_inner)/(tp_counter_inner+fp_counter_inner)\n",
    "    recall=(tp_counter_inner)/(tp_counter_inner+fn_counter_inner)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized.txt\", \"w\")\n",
    "# file_text1=''\n",
    "\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized_POSTAG.txt\", \"w\")\n",
    "# file_text=''\n",
    "\n",
    "# candidates=[]\n",
    "# # sentId=0\n",
    "# tweetId=0\n",
    "# CTrie=trie.Trie(\"ROOT\")\n",
    "# # for sentence in sentenceList:\n",
    "# for tweet in tweetList:\n",
    "# #     result=result_conll[sentId]\n",
    "#     result=result_conll[tweetId]\n",
    "#     tweet_word_list=[]\n",
    "#     conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "#     for result_line in conll_results:\n",
    "#         file_text+=result_line[1]+'\\t'+result_line[3]+'\\n'\n",
    "#         tweet_word_list+=result_line[1]\n",
    "# #         file_text1+=result_line[1]+'\\tO'+'\\n'\n",
    "# #         print('result_line: ', result_line)\n",
    "#     file_text+='\\n'\n",
    "#     tweetId+=1\n",
    "#     tweetMentions=mentionList[tweetId]\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, tweetMentions)\n",
    "#     print(len(tweet_word_list),len(tweet_encoding_list))\n",
    "#     for ind, word in enumerate(tweet_word_list):\n",
    "#         file_text1+=word+'\\t'+tweet_encoding_list[ind]+'\\n'\n",
    "#     file_text1+='\\n'\n",
    "# #     sentId+=1\n",
    "#     tweetId+=1\n",
    "\n",
    "# f.write(file_text)\n",
    "# f.close()\n",
    "\n",
    "# f1.write(file_text1)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1863 1287 1863\n"
     ]
    }
   ],
   "source": [
    "candidates=[]\n",
    "sentId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "phase1outputs=[]\n",
    "for sentence in sentenceList:\n",
    "    result=result_conll[sentId]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    sentence_candidates,sentence_candidates_positions=conll_nounPhrase_chunking(conll_results)\n",
    "#     print(sentence)\n",
    "#     print(conll_results)\n",
    "#     print(sentence_candidates)\n",
    "\n",
    "    candidate_ind=0\n",
    "    phase1Out=''\n",
    "#     for candidate in ne_List_allCheck:\n",
    "#         position = '*'+'*'.join(str(v) for v in candidate.position)\n",
    "#         position=position+'*'\n",
    "#         candidate.set_sen_index(sen_index)\n",
    "#         phase1Out+=(((candidate.phraseText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "\n",
    "    for candidateText in sentence_candidates:\n",
    "#         print(candidateText)\n",
    "        candidateText=candidateText.lower()\n",
    "        phase1outputs.append((((candidateText).lstrip(string.punctuation)).strip()))\n",
    "        position = '*'+'*'.join(str(v) for v in sentence_candidates_positions[candidate_ind])\n",
    "        position=position+'*'\n",
    "#         print(candidateText,sentence_candidates_positions[candidate_ind])\n",
    "        CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "        candidate_ind+=1\n",
    "        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "        \n",
    "#     candidates.append(sentence_candidates)\n",
    "    candidates.append(phase1Out)\n",
    "\n",
    "    sentId+=1\n",
    "#     print('===========')\n",
    "print(len(sentenceList),len(tweets_unpartitoned),len(candidates))\n",
    "\n",
    "tweet_sentence_df['phase1Candidates']=candidates\n",
    "\n",
    "# print(tweet_sentence_df['phase1Candidates'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    }
   ],
   "source": [
    "candidates=CTrie.displayTrie(\"\",[])\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2=time.time()\n",
    "\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  1863 1863 696\n",
      "-0.25818769558749105\n",
      "For entities:  (412, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (243, 6)\n",
      "For ambiguous:  (41, 6)\n",
      "For entities:  (412, 6)\n",
      "For non-entities:  (243, 6)\n",
      "For ambiguous:  (41, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n",
      "completed tweets:  1788 incomplete tweets:  75\n",
      "16\n",
      "16\n",
      "final tally:  1863 1863\n",
      "524:  524    [[]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2 = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted= Phase2.executor(max_batch_value,tweet_sentence_df,CTrie,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df)\n",
    "time3=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_unpartitoned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 200 727\n",
      "precision:  0.6363636363636364\n",
      "recall:  0.32497678737233054\n",
      "f_measure:  0.430239704978488\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "\n",
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    \n",
    "#     output_mentions_list=flatten(complete_tweet_dataframe_grouped_df_sorted[complete_tweet_dataframe_grouped_df_sorted.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "    all_mentions+=annotated_mention_list\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner=len(output_mentions_list)\n",
    "#     total_tagged+=len(output_mentions_list)\n",
    "#     total_annotations+=len(annotated_mention_list)\n",
    "    \n",
    "#     while(annotated_mention_list):\n",
    "#         if(len(output_mentions_list)):\n",
    "#             annotated_candidate= annotated_mention_list.pop()\n",
    "#             if(annotated_candidate in output_mentions_list):\n",
    "#                 output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "#                 tp_counter_inner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "#     fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count+=tp_counter_inner\n",
    "#     false_positive_count+=fp_counter_inner\n",
    "#     false_negative_count+=fn_counter_inner\n",
    "\n",
    "# print(total_annotations,total_tagged)\n",
    "# print(true_positive_count,false_positive_count,false_negative_count)\n",
    "# print(phase1outputs)\n",
    "# get_F1(all_mentions,phase1outputs)\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296.32473587989807\n",
      "3.342034101486206\n"
     ]
    }
   ],
   "source": [
    "# precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "# recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "# f_measure=2*(precision*recall)/(precision+recall)\n",
    "# print(precision,recall,f_measure)\n",
    "\n",
    "print(time2-time1)\n",
    "print(time3-time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4703423992849669\n",
      "0.5892456223684872\n",
      "0.5177327012634292\n"
     ]
    }
   ],
   "source": [
    "#SVM phase 1\n",
    "precision_arr=[0.40486725663716816,0.4861477572559367,0.6225225225225225,0.38872549019607844,0.4494489698131289]\n",
    "recall_arr=[0.46153846153846156,0.6470588235294118,0.6175156389633601,0.4345205479452055,0.7855946398659966]\n",
    "f1_arr=[0.4313494401885681,0.5551789077212806,0.6200089726334679,0.4103492884864166,0.5717768972874124]\n",
    "#SVM full system\n",
    "# precision_arr=[0.6342794759825328,0.7948482060717571,0.8933189655172413,0.9364406779661016,0.90633608815427]\n",
    "# recall_arr=[0.7326607818411097,0.7585601404741001,0.7408400357462019,0.6054794520547945,0.8266331658291457]\n",
    "# f1_arr=[0.6799297834991223,0.7762803234501349,0.8099658036150463,0.7354409317803661,0.8646517739816031]\n",
    "\n",
    "print(sum(precision_arr)/len(precision_arr))\n",
    "print(sum(recall_arr)/len(recall_arr))\n",
    "print(sum(f1_arr)/len(f1_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-a150bcecdd2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-a150bcecdd2b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2425 487 905\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#3K\n",
    "#SVM\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#RF\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#LR\n",
    "2415 460 915\n",
    "0.84 0.7252252252252253 0.7784045124899275\n",
    "\n",
    "#1K\n",
    "#SVM\n",
    "# precision:  0.8075117370892019\n",
    "# recall:  0.63003663003663\n",
    "# f_measure:  0.7078189300411523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ritter- TwitterNLP in Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "ritter_annotator=pd.read_csv(\"/Users/satadisha/Documents/GitHub/my-baseline-setup/ritter-billdeblasio-output.csv\",sep =',',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1054\n",
      "['ID', 'HashTags', 'TweetText', 'Output', 'mentions_other', 'URLs', 'User']\n"
     ]
    }
   ],
   "source": [
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "print(len(ritter_annotator))\n",
    "print(ritter_annotator.columns.tolist())\n",
    "CTrie_ritter=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    }
   ],
   "source": [
    "ritter_annotated_candidates=ritter_annotator['Output'].tolist()\n",
    "for candidate in ritter_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_ritter.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesinRitterTrie=CTrie_ritter.displayTrie(\"\",[])\n",
    "print(len(candidatesinRitterTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  3754 3754 112\n",
      "-0.18265485071086004\n",
      "For entities:  (98, 6)\n",
      "For non-entities:  (8, 6)\n",
      "For ambiguous:  (6, 6)\n",
      "For entities:  (98, 6)\n",
      "For non-entities:  (8, 6)\n",
      "For ambiguous:  (6, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  3701 incomplete tweets:  53\n",
      "16\n",
      "16\n",
      "final tally:  3754 3754\n",
      "524:  524    [[nyc]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_Ritter = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_Ritter, converted_candidates_w_Ritter, complete_tweet_dataframe_grouped_df_sorted_w_Ritter= Phase2_w_Ritter.executor(max_batch_value,tweet_sentence_df,CTrie_ritter,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...</td>\n",
       "      <td>[RobertBelfi, RobertBelfi, RobertBelfi, Robert...</td>\n",
       "      <td>[Presidential candidate 😂😂 #BilldeBlasio wife ...</td>\n",
       "      <td>[, since #obama::*9*10*||, , de blasio::*1*2*||]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [], [(de blasio, g), (mayor, b)]]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Wildbil42007816]</td>\n",
       "      <td>[Say it isn't so, a commie corrupt,  shirley y...</td>\n",
       "      <td>[shirley::*10*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(shirley, b)]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, nyc, BilldeBlasio, nyc, BilldeB...</td>\n",
       "      <td>[jnrbllc, jnrbllc, jnrbllc, jnrbllc]</td>\n",
       "      <td>[@BilldeBlasio To our wonderful Mayor #BilldeB...</td>\n",
       "      <td>[mayor #billdeblasio::*5*6*||, nyc::*11*||, pr...</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(mayor, b)], [], [(city, b)], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3]</td>\n",
       "      <td>[BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...</td>\n",
       "      <td>[TheMtljo, TheMtljo, TheMtljo, TheMtljo]</td>\n",
       "      <td>[Presidential candidate 😂😂 #BilldeBlasio wife ...</td>\n",
       "      <td>[, obama::*10*||, , de blasio::*1*2*||]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [], [(de blasio, g), (mayor, b)]]</td>\n",
       "      <td>[[], [], [], [de blasio]]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[, , ]</td>\n",
       "      <td>[PTiddylicker, PTiddylicker, PTiddylicker]</td>\n",
       "      <td>[if you love CORRUPT RACIST HYPOCRITES WHO ARE...</td>\n",
       "      <td>[, nyc::*3*||, ]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [nyc], []]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [(nyc, g)], []]</td>\n",
       "      <td>[[], [nyc], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID                 index   entry_batch        sentID  \\\n",
       "0        0  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "1        1                 [nan]           [0]           [0]   \n",
       "2        2  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "3        3  [nan, nan, nan, nan]  [0, 0, 0, 0]  [0, 1, 2, 3]   \n",
       "4        4       [nan, nan, nan]     [0, 0, 0]     [0, 1, 2]   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0  [BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...   \n",
       "1                                                 []   \n",
       "2  [BilldeBlasio, nyc, BilldeBlasio, nyc, BilldeB...   \n",
       "3  [BilldeBlasio, BilldeBlasio, BilldeBlasio, Bil...   \n",
       "4                                             [, , ]   \n",
       "\n",
       "                                                user  \\\n",
       "0  [RobertBelfi, RobertBelfi, RobertBelfi, Robert...   \n",
       "1                                  [Wildbil42007816]   \n",
       "2               [jnrbllc, jnrbllc, jnrbllc, jnrbllc]   \n",
       "3           [TheMtljo, TheMtljo, TheMtljo, TheMtljo]   \n",
       "4         [PTiddylicker, PTiddylicker, PTiddylicker]   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [Presidential candidate 😂😂 #BilldeBlasio wife ...   \n",
       "1  [Say it isn't so, a commie corrupt,  shirley y...   \n",
       "2  [@BilldeBlasio To our wonderful Mayor #BilldeB...   \n",
       "3  [Presidential candidate 😂😂 #BilldeBlasio wife ...   \n",
       "4  [if you love CORRUPT RACIST HYPOCRITES WHO ARE...   \n",
       "\n",
       "                                    phase1Candidates        annotation  \\\n",
       "0   [, since #obama::*9*10*||, , de blasio::*1*2*||]  [[], [], [], []]   \n",
       "1                                  [shirley::*10*||]              [[]]   \n",
       "2  [mayor #billdeblasio::*5*6*||, nyc::*11*||, pr...  [[], [], [], []]   \n",
       "3            [, obama::*10*||, , de blasio::*1*2*||]  [[], [], [], []]   \n",
       "4                                   [, nyc::*3*||, ]      [[], [], []]   \n",
       "\n",
       "  stanford_candidates            output_mentions              completeness  \\\n",
       "0    [[], [], [], []]  [[], [], [], [de blasio]]  [True, True, True, True]   \n",
       "1                [[]]                       [[]]                    [True]   \n",
       "2    [[], [], [], []]           [[], [], [], []]  [True, True, True, True]   \n",
       "3    [[], [], [], []]  [[], [], [], [de blasio]]  [True, True, True, True]   \n",
       "4        [[], [], []]            [[], [nyc], []]        [True, True, True]   \n",
       "\n",
       "    current_minus_entry                       candidates_with_label  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0]  [[], [], [], [(de blasio, g), (mayor, b)]]   \n",
       "1                 [0.0]                            [[(shirley, b)]]   \n",
       "2  [0.0, 0.0, 0.0, 0.0]         [[(mayor, b)], [], [(city, b)], []]   \n",
       "3  [0.0, 0.0, 0.0, 0.0]  [[], [], [], [(de blasio, g), (mayor, b)]]   \n",
       "4       [0.0, 0.0, 0.0]                        [[], [(nyc, g)], []]   \n",
       "\n",
       "        only_good_candidates ambiguous_candidates  \n",
       "0  [[], [], [], [de blasio]]     [[], [], [], []]  \n",
       "1                       [[]]                 [[]]  \n",
       "2           [[], [], [], []]     [[], [], [], []]  \n",
       "3  [[], [], [], [de blasio]]     [[], [], [], []]  \n",
       "4            [[], [nyc], []]         [[], [], []]  "
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted_w_Ritter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949 94 245\n",
      "precision:  0.909875359539789\n",
      "recall:  0.7948073701842546\n",
      "f_measure:  0.8484577559231113\n"
     ]
    }
   ],
   "source": [
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted_w_Ritter.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    all_mentions+=annotated_mention_list\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8988614198410645\n",
      "0.638884707457936\n",
      "0.7415294076458718\n"
     ]
    }
   ],
   "source": [
    "precision_arr=[0.8303964757709251,0.8766716196136701,0.9215896885069818,0.9557739557739557,0.909875359539789]\n",
    "recall_arr=[0.47540983606557374,0.517998244073749,0.7667560321715817,0.6394520547945205,0.7948073701842546]\n",
    "f1_arr=[0.6046511627906976,0.6512141280353201,0.8370731707317073,0.7662508207485227,0.8484577559231113]\n",
    "\n",
    "print(sum(precision_arr)/len(precision_arr))\n",
    "print(sum(recall_arr)/len(recall_arr))\n",
    "print(sum(f1_arr)/len(f1_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1365 1086\n",
      "25 1061 642\n",
      "0 205 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-81ec6a8db503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_annotations_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_tagged_ritter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfalse_positive_count_ritter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfalse_negative_count_ritter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mget_F1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_mentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-90434dcfeead>\u001b[0m in \u001b[0;36mget_F1\u001b[0;34m(annotated_mention_list, output_mentions_list)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mrecall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_counter_inner\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn_counter_inner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mf_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_ritter=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Ritter[complete_tweet_dataframe_grouped_df_sorted_w_Ritter.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610108303249098 0.8451452870304749 0.8530042918454936\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM\n",
    "0.8605577689243028 0.8428520752039731 0.8516129032258064\n",
    "\n",
    "#RF\n",
    "0.8614435981138919 0.8433948863636364 0.8523237035707877\n",
    "\n",
    "#LR\n",
    "0.8610108303249098 0.8451452870304749 0.8530042918454936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaguilar as Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n",
    "\n",
    "\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/tweebo-parser/wnut17test_outputs.txt\",'r')\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/3A/split2_output.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTrie_gaguilar=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672\n",
      "672\n",
      "672\n"
     ]
    }
   ],
   "source": [
    "file_text=f.read()\n",
    "\n",
    "output_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n'))) #conll\n",
    "print(len(output_sentences))\n",
    "gaguilar_annotated_candidates=[]\n",
    "for line in output_sentences:\n",
    "    if(line):\n",
    "        tabs=line.split('\\t')\n",
    "        if(tabs):\n",
    "            for candidate in tabs:\n",
    "                gaguilar_annotated_candidates.append(candidate)\n",
    "print(len(gaguilar_annotated_candidates))\n",
    "for candidate in gaguilar_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_gaguilar.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesingaguilarTrie=CTrie_gaguilar.displayTrie(\"\",[])\n",
    "print(len(candidatesingaguilarTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  1863 1863 520\n",
      "-0.23809985274647932\n",
      "For entities:  (417, 6)\n",
      "For non-entities:  (69, 6)\n",
      "For ambiguous:  (34, 6)\n",
      "For entities:  (417, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (69, 6)\n",
      "For ambiguous:  (34, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  1798 incomplete tweets:  65\n",
      "16\n",
      "16\n",
      "final tally:  1863 1863\n",
      "524:  524    [[]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_gaguilar = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_gaguilar, converted_candidates_w_gaguilar, complete_tweet_dataframe_grouped_df_sorted_w_gaguilar= Phase2_w_gaguilar.executor(max_batch_value,tweet_sentence_df,CTrie_gaguilar,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408 137 669\n",
      "precision:  0.7486238532110092\n",
      "recall:  0.3788300835654596\n",
      "f_measure:  0.5030826140567202\n"
     ]
    }
   ],
   "source": [
    "# all_mentions=[]\n",
    "# all_outputs=[]\n",
    "\n",
    "# true_positive_count=0\n",
    "# false_positive_count=0\n",
    "# false_negative_count=0\n",
    "\n",
    "# total_annotations=0\n",
    "# total_tagged=0\n",
    "# for index, row in complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.iterrows():\n",
    "#     output=flatten(list(row.output_mentions),[])\n",
    "# #     print(output)\n",
    "#     all_outputs+=output\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list=[]\n",
    "#     tp_counter_inner=0\n",
    "#     fp_counter_inner=0\n",
    "#     fn_counter_inner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     all_mentions+=annotated_mention_list\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8729290882790055\n",
      "0.6908351448884786\n",
      "0.7696562568677873\n"
     ]
    }
   ],
   "source": [
    "precision_arr=[0.8555160142348754, 0.816622691292876,0.8360128617363344,0.9252032520325203,0.9312906220984215]\n",
    "recall_arr=[0.6586301369863014,0.5531724754244861,0.684811237928007,0.7175283732660782,0.8400335008375209]\n",
    "f1_arr=[0.7442724458204334, 0.6595631326584976,0.752895752895753,0.8082386363636364,0.8833113166006163]\n",
    "\n",
    "print(sum(precision_arr)/len(precision_arr))\n",
    "print(sum(recall_arr)/len(recall_arr))\n",
    "print(sum(f1_arr)/len(f1_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .\n",
      "0 []\n",
      "['sonmarg']\n",
      "& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .\n",
      "1 []\n",
      "[]\n",
      "& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .\n",
      "2 []\n",
      "[]\n",
      "& gt ; * The four civilians killed included two children of a family whose house was hit by a separate avalanche , also on Wednesday , a police spokesman said .\n",
      "3 []\n",
      "[]\n",
      "The bodies of the soldiers were recovered after the concerted efforts of the Avalanche Rescue Teams ( ART ) , which is equipped to work in inhospitable terrain and weather conditions .\n",
      "4 []\n",
      "['avalanche rescue']\n",
      "& gt ; * Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , Defence Spokesman Colonel Rajesh Kalia said .\n",
      "5 []\n",
      "['colonel rajesh kalia']\n",
      "Visuals of the avalanche site in Gurez sector .\n",
      "6 []\n",
      "['gurez']\n",
      "( Source : ANI ) Visuals of the avalanche site in Gurez sector .\n",
      "7 []\n",
      "['gurez']\n",
      "( Source : ANI )\n",
      "8 []\n",
      "[]\n",
      "“ Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , ” Defence Spokesman Colonel Rajesh Kalia said .\n",
      "9 []\n",
      "['colonel rajesh kalia']\n",
      "Watch What Else is Making News\n",
      "10 []\n",
      "[]\n",
      "“ Swift action by a few soldiers at the post and timely help from the villagers of Mahazgund ensured saving of six soldiers .\n",
      "11 []\n",
      "[]\n",
      "Unfortunately , three soldiers could not be saved whose bodies were retrieved on January 26 by the ARTs that were rushed to the post , ” he said .\n",
      "12 []\n",
      "[]\n",
      "^ Function ^ : ^ I ^ post ^ the ^ article's ^ text ^ as ^ a ^ comment ^ if ^ the ^ website ^ is ^ adblocker ^ unfriendly .\n",
      "13 []\n",
      "[]\n",
      "[ ^ I ^ accept ^ commands !\n",
      "14 []\n",
      "[]\n",
      "Road and airport closure isolate Srinagar as avalanche risk remains high\n",
      "15 []\n",
      "['srinagar']\n",
      "[ IMAGE ] ( http://images.indianexpress.com/2015/05/drdo-logo-thumb.jpg?w=480 )\n",
      "16 []\n",
      "[]\n",
      "The DRDO is working on projects to develop new technologies to predict avalanches in a much precise manner .\n",
      "17 []\n",
      "['the drdo']\n",
      "( File Photo ) The DRDO is working on projects to develop new technologies to predict avalanches in a much precise manner .\n",
      "18 []\n",
      "['the drdo']\n",
      "( File Photo )\n",
      "19 []\n",
      "[]\n",
      "The Defence Research Development Organisation ( DRDO ) is working on four projects to develop new technologies for more accurate prediction of avalanches , the government said on Friday .\n",
      "20 []\n",
      "['drdo']\n",
      "“ Presently , the DRDO is working on four projects to develop new technologies for more accurate prediction of avalanches , ” he said in a written response .\n",
      "21 []\n",
      "['the drdo']\n",
      "Replying to another question , Bhamre said the jawans deployed at places such as Siachen Glacier are provided with the best-quality winter clothing .\n",
      "22 []\n",
      "['bhamre', 'siachen glacier']\n",
      "He said the soldiers deployed in Siachen are being provided pre-fabricated insulated shelters .\n",
      "23 []\n",
      "['siachen']\n",
      "“ In Navy , advisories on mental health are issued from time to time and stress relieving activities like yoga , art of living are conducted periodically .\n",
      "24 []\n",
      "['navy']\n",
      "^ Function ^ : ^ I ^ post ^ the ^ article's ^ text ^ as ^ a ^ comment ^ if ^ the ^ website ^ is ^ adblocker ^ unfriendly .\n",
      "25 []\n",
      "[]\n",
      "[ ^ I ^ accept ^ commands !\n",
      "26 []\n",
      "[]\n",
      "& gt ; Umm , where are all these alleged criticisms ?\n",
      "27 []\n",
      "[]\n",
      "This fucker should be sent up the fucking river because of what he did not because he likes Trump .\n",
      "28 []\n",
      "['trump']\n",
      "Bhangra is Punjabi .\n",
      "29 []\n",
      "[]\n",
      "A lot of people thought it was a joke but did n't think it was appropriate for a PR exec to make .\n",
      "30 []\n",
      "[]\n",
      "Toni Kroos - [ here ] ( @url ) is the ( near-certain ) full XI .\n",
      "31 []\n",
      "['toni kroos']\n",
      "[ Rip Chad ] ( @url )\n",
      "32 []\n",
      "[]\n",
      "[ link to thread ] ( @url )\n",
      "33 []\n",
      "[]\n",
      "@url\n",
      "34 []\n",
      "[]\n",
      "Nope , [ I have multiple sources .\n",
      "35 []\n",
      "[]\n",
      "[ Action plan already in place ] ( @url )\n",
      "36 []\n",
      "[]\n",
      "KEINE BREMSEN !\n",
      "37 []\n",
      "[]\n",
      "All those in favor say \" Jina did it \" .\n",
      "38 []\n",
      "['jina']\n",
      "What country in Europe has n't she stabbed in the back or undermined their interests ? \"\n",
      "39 []\n",
      "['europe']\n",
      "He got sent off in the last game vs Bordeaux .\n",
      "40 []\n",
      "['bordeaux']\n",
      "@url\n",
      "41 []\n",
      "[]\n",
      "COYB !\n",
      "42 []\n",
      "[]\n",
      "Oh well .\n",
      "43 []\n",
      "[]\n",
      "/ r / politics yesterday : Trump has n't had a press conference in a long time , he's avoiding the media\n",
      "44 []\n",
      "['trump']\n",
      "It's been a hit out in the world since Fragrant posted it here , Daily Edge Sid a piece too even - @url\n",
      "45 []\n",
      "['daily edge sid']\n",
      "Guy sounds like a Grade A Douche\n",
      "46 []\n",
      "[]\n",
      "Biased , but I think the best part of this is that a MetaCanadian has the top comment .\n",
      "47 []\n",
      "['metacanadian']\n",
      "@url\n",
      "48 []\n",
      "[]\n",
      "It's actually copypasta from a post I did a couple April fools ago when r / soccer was all troll posts .\n",
      "49 []\n",
      "[]\n",
      "I hope y ' all take back what you said at Glenn Greenwald in the last thread .\n",
      "50 []\n",
      "['glenn greenwald']\n",
      "Comme ça :\n",
      "51 []\n",
      "[]\n",
      "[ New MatchThread here .\n",
      "52 []\n",
      "[]\n",
      "LMAOOOOOOOO , are the Democrats really doubling down on the thing that got Trump elected in first place ?\n",
      "53 []\n",
      "['trump']\n",
      "Living Computer Museum + Labs , Skyview Observatory , Smith Tower and Observation Deck ,\n",
      "54 []\n",
      "['skyview observatory']\n",
      "You posted this in TD so why are you still posting here ?\n",
      "55 []\n",
      "['td']\n",
      "Aww shit this gonna go into overdrive now\n",
      "56 []\n",
      "[]\n",
      "CVS selling their own version of epipen at 1 / 6 th price .\n",
      "57 []\n",
      "['cvs']\n",
      "Trump : I ' m going to bring back manufacturing jobs to Michigan !\n",
      "58 []\n",
      "['trump', 'michigan']\n",
      "Becky in a Snickers advert ?\n",
      "59 []\n",
      "['snickers']\n",
      "] ( @url )\n",
      "60 []\n",
      "[]\n",
      "I looked this up yesterday actually ; the average household income for Auckland in 2016 was $ 104 k .\n",
      "61 []\n",
      "['auckland']\n",
      "You mean they had rain [ 4 years ago ] ( @url ) in Ireland .\n",
      "62 []\n",
      "['ireland']\n",
      "I remember having parliament on the radio in my car and hearing Leyonhjelm [ give this speach ] ( @url ) which was impressive .\n",
      "63 []\n",
      "['leyonhjelm']\n",
      "Looks like Grimaldo is linked with MU too\n",
      "64 []\n",
      "['grimaldo']\n",
      "But I thought he had joined Boro ?\n",
      "65 []\n",
      "['boro']\n",
      "You posted in / r / calgary too . .\n",
      "66 []\n",
      "['calgary']\n",
      "[ This interview ] ( @url ) from June was hilarious until you realize that he makes decision that affect the direction the city moves in .\n",
      "67 []\n",
      "[]\n",
      "& gt ; Cavani has the highest conversion rate in Europe\n",
      "68 []\n",
      "['cavani', 'europe']\n",
      "Cheney's back .\n",
      "69 []\n",
      "['cheney']\n",
      "& gt ; It will be very telling ( though probably unsurprising ) how folks like Trey Gowdy and Jason Chaffetz react to this .\n",
      "70 []\n",
      "['trey gowdy', 'jason chaffetz']\n",
      "* * redditor * * : NLWastedLink\n",
      "71 []\n",
      "[]\n",
      "Dzeko penalty miss will be [ r / soccer ] ( @url ) top post .\n",
      "72 []\n",
      "[]\n",
      "Wow , / r / news totally kicked the shit out of / r / politics on this topic .\n",
      "73 []\n",
      "[]\n",
      "Love Lynda .\n",
      "74 []\n",
      "['lynda']\n",
      "Here's a link to a Redditor's experience in Calgary :\n",
      "75 []\n",
      "['calgary']\n",
      "[ OP of the previous threads could n't handle the Ronaldo criticism .\n",
      "76 []\n",
      "['op', 'ronaldo']\n",
      "You ' re speaking from a Brit's point of view .\n",
      "77 []\n",
      "['brit']\n",
      "* * redditor * * : Quickloot\n",
      "78 []\n",
      "[]\n",
      "* * submission title * * : Battle underway for return of Napoleon's horse Marengo to Ireland\n",
      "79 []\n",
      "['napoleon', 'marengo', 'ireland']\n",
      "[ Yo .\n",
      "80 []\n",
      "[]\n",
      "* * submission title * * : Does anyone know if anyone has started protesting at CVG yet ?\n",
      "81 []\n",
      "['cvg']\n",
      "Reddit's tough guys say yes .\n",
      "82 []\n",
      "[]\n",
      "Media pounces on it .\n",
      "83 []\n",
      "[]\n",
      "they changed all the topics to Zlatan .\n",
      "84 []\n",
      "['zlatan']\n",
      "The mods deleted [ this story about activists being beaten by Trump supporters ] ( @url ) for being off topic .\n",
      "85 []\n",
      "['trump']\n",
      "Yeah , and they get all misty-eyed about government-owned businesses and how awful it is that Costello sold them off etc , only because they ' re not old enough to remember having to deal with government-owned businesses and how fucking useless they are :\n",
      "86 []\n",
      "['costello']\n",
      "Step 2 : [ Google ] ( http://www.google.co.nz )\n",
      "87 []\n",
      "[]\n",
      "Lol this is what city pussis think of living outside a major city is like\n",
      "88 []\n",
      "[]\n",
      "You could try and send an email to the teams and see what they reply : D\n",
      "89 []\n",
      "[]\n",
      "Resultat . dk\n",
      "90 []\n",
      "[]\n",
      "* * Edit : * * Wow , that blew up .\n",
      "91 []\n",
      "[]\n",
      "I asked Bellerin v walker in a match thread a few months ago , the results might surprise you .\n",
      "92 []\n",
      "[]\n",
      "& gt ; Barca flair\n",
      "93 []\n",
      "['barca']\n",
      "Where's the one from ~ ~ Syria ~ ~ Serbia where some guy banged it over the bar from the goalline ?\n",
      "94 []\n",
      "['syria', 'serbia']\n",
      "Hey Tom , can you comment on this thread ?\n",
      "95 []\n",
      "['tom']\n",
      "& gt ; you cannot infer much from a single quarter's GDP statistics .\n",
      "96 []\n",
      "[]\n",
      "True , but I imagine it would be a lot lower and as I pointed out to Andrew Little would be cheaper than [ eliminating fees .\n",
      "97 []\n",
      "['andrew little']\n",
      "She's been making me question my sexuality since The X-Files started .\n",
      "98 []\n",
      "[]\n",
      "* * submission title * * : PSG tell Inter & amp ; Juve Verratti will cost € 100 M\n",
      "99 []\n",
      "['psg', 'inter', 'juve verratti']\n",
      "Not sure if trust Donald on this .\n",
      "100 []\n",
      "['donald']\n",
      "EDIT : No actually screw that , i'd like to ask why you'd want attacks on Trump supporters to continue ?\n",
      "101 []\n",
      "['trump']\n",
      "Perhaps this paper instead actually suggests that students born in July are underdiagnosed ?\n",
      "102 []\n",
      "[]\n",
      "If PNG is undeveloped , would n't resettling the boats full of surgeons be exactly what they need ?\n",
      "103 []\n",
      "['png']\n",
      "Maybe you could try contacting the producer ( Great Southern Television )\n",
      "104 []\n",
      "[]\n",
      "& gt ; I ' m an ex Muslim\n",
      "105 []\n",
      "[]\n",
      "There is nothing wrong with the Huron disposal site .\n",
      "106 []\n",
      "['huron']\n",
      "Her name is Scout .\n",
      "107 []\n",
      "['scout']\n",
      "Morocco here we come let's make that 4 - 11 - 13 not 14 loss\n",
      "108 []\n",
      "[]\n",
      "98 . 2 % of Costa Rica's electricity came from renewable sources in 2016 .\n",
      "109 []\n",
      "['costa rica']\n",
      "Was on the DSP and found his / her way back to a normal life with his / her own income .\n",
      "110 []\n",
      "['dsp']\n",
      "* * submission title * * : Proposed class action against B . C .\n",
      "111 []\n",
      "['b']\n",
      "[ Source ] ( @url ) at / r / southafrica\n",
      "112 []\n",
      "[]\n",
      "The idea that one IRA was kinder or more moral than the other is ludicrous .\n",
      "113 []\n",
      "['ira']\n",
      "I hear audio in the first AA streamable link but not the one with full build up .\n",
      "114 []\n",
      "[]\n",
      "Surprisingly no PMs yet , got tagged in [ this ] ( @url ) though , and it's pretty thorough .\n",
      "115 []\n",
      "[]\n",
      "The rest of your post has already been refuted by yourself , as you acknowledged [ in another post ] ( @url ) that what Paul Manafort did is * * a fucking crime * * .\n",
      "116 []\n",
      "['paul manafort']\n",
      "For those complaining about Rule 6 ( not related to Calgary ) . . . . this story is an update to this previous / r / Calgary submission which did not get removed :\n",
      "117 []\n",
      "['calgary', 'calgary']\n",
      "You do realize this was published by CBC Manitoba .\n",
      "118 []\n",
      "['cbc manitoba']\n",
      "* * submission title * * : Diego Rolan goal vs PSG ( 1 - 1 )\n",
      "119 []\n",
      "['diego rolan', 'psg']\n",
      "Dude [ definitely definitely ] ( @url ) does n't understand the housing market at all .\n",
      "120 []\n",
      "[]\n",
      "Here is the background on Rastus and Pete .\n",
      "121 []\n",
      "['rastus', 'pete']\n",
      "For people asking about the torture video : [ Click here ] ( @url )\n",
      "122 []\n",
      "[]\n",
      "Rudder breaks .\n",
      "123 []\n",
      "[]\n",
      "Hey check out this : @url\n",
      "124 []\n",
      "[]\n",
      "* * submission title * * : Concorde over Calgary 1977\n",
      "125 []\n",
      "['calgary']\n",
      "That's why I have np at the start of the URL as per the sub's T & amp ; C's .\n",
      "126 []\n",
      "[]\n",
      "[ Oh look .\n",
      "127 []\n",
      "[]\n",
      "[ Staaaahp ] ( http://www.reddit.com/r/news/comments/5m863w/-/dc25xfj )\n",
      "128 []\n",
      "[]\n",
      "[ It's compulsory ] ( @url ) for OP's course .\n",
      "129 []\n",
      "['op']\n",
      "[ The link to the AmA is at the top of this thread .\n",
      "130 []\n",
      "[]\n",
      "Ah , the ol ' Adelaide [ Digeridoo !\n",
      "131 []\n",
      "['adelaide']\n",
      "Would take Ntep in a heartbeat and frankly Costil and Sagna would fix two of the weakest spots in the team right now .\n",
      "132 []\n",
      "['ntep', 'costil', 'sagna']\n",
      "Обсуждение на английском @url\n",
      "133 []\n",
      "[]\n",
      "Just ask him about his [ trip to North Korea ] ( @url )\n",
      "134 []\n",
      "['north korea']\n",
      "* * submission title * * : Lost RC plane .\n",
      "135 []\n",
      "[]\n",
      "& gt ; I believe Peter Hitchens ( Fervent euroskeptic who voted remain )\n",
      "136 []\n",
      "['peter hitchens']\n",
      "Long story short , whats to keep republicans from scrapping the preexisting conditions clause for those with BLD – paid for from some other pocket of govt money – but not for everyone else ?\n",
      "137 []\n",
      "['bld']\n",
      "* [ Stew's Self Service Garage ] ( http://stewsgarage.com/ )\n",
      "138 []\n",
      "[]\n",
      "It is well-known that Snickers bewilder and [ delight ] ( @url ) the Irish .\n",
      "139 []\n",
      "['snickers']\n",
      "Colluding with Putin , hahaha !\n",
      "140 []\n",
      "['putin']\n",
      "Source - @url\n",
      "141 []\n",
      "[]\n",
      "I'd like to see him try that in the NHL\n",
      "142 []\n",
      "['nhl']\n",
      "TL ; D R - Do n't get your phone from your carrier .\n",
      "143 []\n",
      "[]\n",
      "There has been a lot of discussion about Uber in general , but I have n't seen much in the way of people talking about becoming a driver .\n",
      "144 []\n",
      "['uber']\n",
      "you ' re really late , OP :\n",
      "145 []\n",
      "['op']\n",
      "@url\n",
      "146 []\n",
      "[]\n",
      "[ Warum das eine absolut beschissene Idee ist .\n",
      "147 []\n",
      "[]\n",
      "( personally , I think Rastus is his jilted lover . . . ; )\n",
      "148 []\n",
      "['rastus']\n",
      "Edit : [ Context in case anyone was wondering ] ( @url )\n",
      "149 []\n",
      "[]\n",
      "Yet in [ another post ] ( @url ) you lambast Labor for * * only * * three of their front bench publicly denouncing this debacle .\n",
      "150 []\n",
      "[]\n",
      "[ Aaaaaand there it is ] ( @url )\n",
      "151 []\n",
      "[]\n",
      "BuzzFeed took the bait .\n",
      "152 []\n",
      "[]\n",
      "/ r / canada used to say the CBC went clickbait because those evil cons tried to defund them , but the rut is clearly much deeper and more permanent .\n",
      "153 []\n",
      "['cbc']\n",
      "* * comment content * * : Marseille has .\n",
      "154 []\n",
      "['marseille']\n",
      "Original post : [ ' Many wounded ' in Istanbul nightclub attack ] ( @url )\n",
      "155 []\n",
      "['istanbul']\n",
      "It's got the GIFs of the goals .\n",
      "156 []\n",
      "[]\n",
      "Velvet has such a thin skin he actually deleted and resubmitted an article because someone disagreed with him :\n",
      "157 []\n",
      "[]\n",
      "Hector was holding back I assume playing it safe with injuries / fitness .\n",
      "158 []\n",
      "[]\n",
      "Exatamente .\n",
      "159 []\n",
      "[]\n",
      "Narsingh to see here , [ r / soccer thread ] ( @url ) totally brutal\n",
      "160 []\n",
      "[]\n",
      "you mean Tomi Lahren , I just referenced her here\n",
      "161 []\n",
      "['tomi lahren']\n",
      "* * redditor * * : JamesakaNoah\n",
      "162 []\n",
      "[]\n",
      "Wow , I ' m an idiot .\n",
      "163 []\n",
      "[]\n",
      "Most women in Muslim countries are [ happy with their situation ] ( @url ) .\n",
      "164 []\n",
      "[]\n",
      "I remember around a year ago there was a post about how Ronaldo had scored over 40 % of Madrid's goals since 2009 .\n",
      "165 []\n",
      "['ronaldo', 'madrid']\n",
      "They actually regularly upvote posts to the front page asking \" why do so many right leaning people post here REEEE \"\n",
      "166 []\n",
      "[]\n",
      "As you can see the Stoke mafia ganged up on him big time .\n",
      "167 []\n",
      "[]\n",
      "Best Pho in Calgary\n",
      "168 []\n",
      "['calgary']\n",
      "The title should just be a literal copy of the tweet , which mentions Swansea .\n",
      "169 []\n",
      "['swansea']\n",
      "Hopefully , Bex negotiated a deal with the Snickers company to supply our island with their delicious chocalate .\n",
      "170 []\n",
      "['bex', 'snickers']\n",
      "* * submission title * * : Ireland votes to be world's first country to fully divest from fossil fuels\n",
      "171 []\n",
      "['ireland']\n",
      "Pls no Tom\n",
      "172 []\n",
      "['tom']\n",
      "[ Snickers ] ( @url )\n",
      "173 []\n",
      "['snickers']\n",
      "Original post : [ Andy Carroll goal vs Crystal Palace ( 2 - 0 ) ] ( @url )\n",
      "174 []\n",
      "['andy carroll', 'crystal palace']\n",
      "Uh , they ' re actually pretty well off compared to the rest of the country .\n",
      "175 []\n",
      "[]\n",
      "Did you miss [ this ] ( @url ) yesterday ?\n",
      "176 []\n",
      "[]\n",
      "[ Ritchie McClaw ] ( @url )\n",
      "177 []\n",
      "['ritchie mcclaw']\n",
      "Crosspost do r / worldnews , @url\n",
      "178 []\n",
      "[]\n",
      "Ca serait meme plutot le contraire , sans ce feu , il n ' y aurait eu probablement pas de survivant : @url\n",
      "179 []\n",
      "[]\n",
      "I ' m saying this guy is assuming that they assumed he supported Trump because he's white .\n",
      "180 []\n",
      "['trump']\n",
      "Hey u / nicklovettnz - Stuff have [ nicked your picture .\n",
      "181 []\n",
      "[]\n",
      "* * submission title * * : Middleborough in talks to sign Jese Rodriguez\n",
      "182 []\n",
      "['middleborough', 'jese rodriguez']\n",
      "The TLDR is all you really need , but the best 3 word definition is\n",
      "183 []\n",
      "['the tldr']\n",
      "[ From Barcelona , the stand-out youngsters are Deulofeu , Dongou and Grimaldo .\n",
      "184 []\n",
      "['barcelona', 'deulofeu', 'dongou', 'grimaldo']\n",
      "- Jürgen Klopp\n",
      "185 []\n",
      "['jürgen klopp']\n",
      "It was n't even funny like the Multinational Corporation one yesterday .\n",
      "186 []\n",
      "[]\n",
      "Lol , this is the dick whose parents left him a house\n",
      "187 []\n",
      "[]\n",
      "* * submission title * * : Official : PSG sign Gonçalo Guedes for 25 million €\n",
      "188 []\n",
      "['psg', 'gonçalo guedes']\n",
      "@url\n",
      "189 []\n",
      "[]\n",
      "Is it the Shiba Inu extensively described in the top post right now ?\n",
      "190 []\n",
      "['shiba inu']\n",
      "The main problems can be summed up with Peter Lim though from my limited knowledge .\n",
      "191 []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['peter lim']\n",
      "* * submission title * * : ZM ( possibly other radio stations ) adding rap features to normal songs ?\n",
      "192 []\n",
      "['zm']\n",
      "Well looks like Demolition Man has come true .\n",
      "193 []\n",
      "['demolition man']\n",
      "I do n't really understand your problem with the SPLC though .\n",
      "194 []\n",
      "['splc']\n",
      "[ Ah , the ol ' reddit skin-a-roo !\n",
      "195 []\n",
      "[]\n",
      "Also , check the FIFA XI thread where a lot of people were shitting on him when he actually had a fantastic 2016\n",
      "196 []\n",
      "['fifa xi']\n",
      "Will you criticize Trump when he makes mistakes or will you be silent\n",
      "197 []\n",
      "['trump']\n",
      "On Reddit , I do n't think people care as much as they think they do regarding minorities .\n",
      "198 []\n",
      "[]\n",
      "It came into effect on the 30 th of December 2016 .\n",
      "199 []\n",
      "[]\n",
      "More at @url\n",
      "200 []\n",
      "[]\n",
      "EDIT : Heh guess who is already at - 2 in 5 minutes .\n",
      "201 []\n",
      "[]\n",
      "[ I know ] ( @url ) Part of the problem is my brief skim meant that I did n't notice they had it because , like Italy , they called it something else .\n",
      "202 []\n",
      "['italy']\n",
      "[ Credit ] ( @url ) to / u / uncle_retardo\n",
      "203 []\n",
      "[]\n",
      "For more info about this and local views on the matter check out where OP took this from .\n",
      "204 []\n",
      "['op']\n",
      "Edit : Because OP's kindness and interest in accommodating other people might rub off on the entitled prick .\n",
      "205 []\n",
      "['op']\n",
      "There is [ literally a Mises quote ] ( @url ) in the thread .\n",
      "206 []\n",
      "['mises']\n",
      "Is doch eh nur heiße Luft .\n",
      "207 []\n",
      "[]\n",
      "@Maronti\n",
      "208 []\n",
      "[]\n",
      "Hi .\n",
      "209 []\n",
      "[]\n",
      "FYI about a month ago there was a post about general info on protection orders :\n",
      "210 []\n",
      "[]\n",
      "[ Live Goal + Replays - AA Mirror HD ] ( @url )\n",
      "211 []\n",
      "['aa mirror hd']\n",
      "[ AA Mirror Replays HD - Mixtape ] ( @url )\n",
      "212 []\n",
      "[]\n",
      "one of the [ top posts ] ( @url ) on / r / soccer right now is about who is still playing from the 2002 World Cup .\n",
      "213 []\n",
      "[]\n",
      "[ / r / Ireland ] ( @url ) - & gt ; [ / r / Iceland ] ( @url )\n",
      "214 []\n",
      "['ireland', 'iceland']\n",
      "* * submission title * * : British man dies working on Qatar 2022 World Cup stadium\n",
      "215 []\n",
      "['qatar 2022 world cup']\n",
      "[ I just think Trump is better than Hillary ] ( @url )\n",
      "216 []\n",
      "['trump', 'hillary']\n",
      "Also , do n't forget [ drinks this Friday at Kelly's in Newtown ] ( @url )\n",
      "217 []\n",
      "['kelly', 'newtown']\n",
      "How do you know your ROZ post wo n't be removed for editorialising ?\n",
      "218 []\n",
      "[]\n",
      "[ Haha , because he deserves it ] ( @url ) .\n",
      "219 []\n",
      "[]\n",
      "Heh , it did n't go down [ well ] ( @url )\n",
      "220 []\n",
      "[]\n",
      "[ Aah , the old Reddit deatharoo ] ( @url )\n",
      "221 []\n",
      "[]\n",
      "I could only find this Reddit thread with the most comments @url\n",
      "222 []\n",
      "[]\n",
      "* * redditor * * : Thorvirdh\n",
      "223 []\n",
      "[]\n",
      "Searching . . . searching . . . Nope , not an argument in sight .\n",
      "224 []\n",
      "[]\n",
      "See response from / u / imkatnotcat [ above ] ( @url ) .\n",
      "225 []\n",
      "[]\n",
      "They charged him douchebaggery , and he [ proved it , ] ( @url ) so they had to STFU .\n",
      "226 []\n",
      "[]\n",
      "[ Reference ] ( @url )\n",
      "227 []\n",
      "[]\n",
      "The OP here deleted it ( dunno why ) - @url\n",
      "228 []\n",
      "['op']\n",
      "Obligatory spruik for drinks Friday\n",
      "229 []\n",
      "[]\n",
      "[ OP is a dirty karma whoring bundle of sticks ] ( @url )\n",
      "230 []\n",
      "['op']\n",
      "Obligatory link to [ Oldies Night ] ( @url ) next week .\n",
      "231 []\n",
      "[]\n",
      "They are having a field day over in News .\n",
      "232 []\n",
      "[]\n",
      "I want anyteam to win it except PSG .\n",
      "233 []\n",
      "['psg']\n",
      "See also [ Will António Guterres be the UN's best ever secretary general ?\n",
      "234 []\n",
      "['guterres']\n",
      "It's Uber , but for chinese speaking folk only and its illegal .\n",
      "235 []\n",
      "['uber']\n",
      "The media's narrative was not that that extreme but it definitely was attacking Trump and his supporters pretty much non- stop .\n",
      "236 []\n",
      "['trump']\n",
      "That seems like something someone who supposedly [ works in Sandringham ] ( @url ) would know .\n",
      "237 []\n",
      "['sandringham']\n",
      "This is a post I made in December .\n",
      "238 []\n",
      "[]\n",
      "It was removed within 2 minutes , flagged as \" Off-topic \" .\n",
      "239 []\n",
      "[]\n",
      "Hey , whoever screen captured the page with the 6 months listed want to link it again for the people who are going to get owned in the coming months ?\n",
      "240 []\n",
      "[]\n",
      "[ Worldnews ] ( @url ) .\n",
      "241 []\n",
      "[]\n",
      "Hi there / u / Mo-bot , the actual AMA [ can be found here ] ( @url ) .\n",
      "242 []\n",
      "[]\n",
      "AMA with the leader of the Conservative Party .\n",
      "243 []\n",
      "[]\n",
      "[ Live in Dalhousie ] ( @url ) ?\n",
      "244 []\n",
      "['dalhousie']\n",
      "I was just looking at the wayback machine on / r / politics : here's the front page for September on / r / politics :\n",
      "245 []\n",
      "[]\n",
      "Hey jackass , [ check this out ] ( @url )\n",
      "246 []\n",
      "[]\n",
      "[ Da nAh NAH NAAAHHH !\n",
      "247 []\n",
      "[]\n",
      "This appartently is n't political enough to be on / r / politics despite the man being forced to yell \" fuck Trump \" .\n",
      "248 []\n",
      "['trump']\n",
      "To their credit , they recently doubled their space in Yaletown .\n",
      "249 []\n",
      "['yaletown']\n",
      "The Prem is better , but only slightly .\n",
      "250 []\n",
      "['the prem']\n",
      "/ r / politics is not at all alt-left at present , IMO .\n",
      "251 []\n",
      "[]\n",
      "3 weeks from now a Centrelink recipient kills himself .\n",
      "252 []\n",
      "[]\n",
      "[ Source ] ( @url ) at / r / ireland\n",
      "253 []\n",
      "['ireland']\n",
      "Hello and welcome to / r / southafrica !\n",
      "254 []\n",
      "[]\n",
      "Edit : [ Bingo .\n",
      "255 []\n",
      "[]\n",
      "[ Well shit I was wrong ] ( @url )\n",
      "256 []\n",
      "[]\n",
      "Here you go [ Source ] ( @url )\n",
      "257 []\n",
      "[]\n",
      "[ Source ] ( @url ) at / r / newzealand\n",
      "258 []\n",
      "[]\n",
      "Wow , just wow .\n",
      "259 []\n",
      "[]\n",
      "The guy who ate Sudocrem :\n",
      "260 []\n",
      "[]\n",
      "Bingo .\n",
      "261 []\n",
      "[]\n",
      "I like u / Womble_Don's post about why Lahm is a good captain .\n",
      "262 []\n",
      "['lahm']\n",
      "Chuck this a read @url\n",
      "263 []\n",
      "[]\n",
      "They said \" Yea because catching the entire force of your body with your arms is always a great idea . \"\n",
      "264 []\n",
      "[]\n",
      "[ Alternative format ] ( @url )\n",
      "265 []\n",
      "[]\n",
      "Thanks to u / BordNaMonaLisa for reminding me of this .\n",
      "266 []\n",
      "[]\n",
      "I just wrote a fucking novel regarding BLM's use of blanket statements [ here .\n",
      "267 []\n",
      "['blm']\n",
      " Many wounded ' in Istanbul nightclub attack\n",
      "268 []\n",
      "['istanbul']\n",
      "Its inconclusive IMO .\n",
      "269 []\n",
      "[]\n",
      "So discussion related to , say , \" New Zealand \" could all go in one place , such as reddit . com / r / newzealand .\n",
      "270 []\n",
      "['new zealand']\n",
      "Best of all ^ ^ ^ ( / s ) , they've completely fucked over the [ Statutory Limitation Period ] ( @url ) that limited them from chasing you up to within six years .\n",
      "271 []\n",
      "[]\n",
      "Possibly some Internode solutions for you [ here ] ( @url ) ?\n",
      "272 []\n",
      "['internode']\n",
      "I wonder how many [ Ticket Operations Coordinators ] ( @url ) work for the Clippers\n",
      "273 []\n",
      "['clippers']\n",
      "ITT : mutual respect between Tottenham and Chelsea fans and everyone else shitting on Giroud .\n",
      "274 []\n",
      "['tottenham', 'chelsea', 'giroud']\n",
      "& gt ; Well like you say they put a bald-faced lie right in the headline .\n",
      "275 []\n",
      "[]\n",
      "& gt ; Report antagonizing comments .\n",
      "276 []\n",
      "[]\n",
      "Just remember we should be warier of Trump opponents , who also ignore facts , and are [ bigger Nazis than Trump .\n",
      "277 []\n",
      "['trump', 'trump']\n",
      "* * subreddit * * : Wellington\n",
      "278 []\n",
      "['wellington']\n",
      "Also , a shameless reminder that [ Ladies ' Night is on this Friday ] ( @url ) rain or shine !\n",
      "279 []\n",
      "[]\n",
      "It's like the Twilight Zone of reddit .\n",
      "280 []\n",
      "['twilight zone']\n",
      "Im Video auf [ worldnews ] ( @url ) kann man sehen , dass der nicht nur in die Menge fährt , sondern auch zurücksetzt und nochmal in die Liegengebliebenen fährt . . . da fehlen mir die Worte .\n",
      "281 []\n",
      "[]\n",
      "& gt ; [ I voted Bernie .\n",
      "282 []\n",
      "['bernie']\n",
      "[ Brit clouds ] ( @url )\n",
      "283 []\n",
      "['brit']\n",
      "/ r / movies : Film / theatre teacher\n",
      "284 []\n",
      "[]\n",
      "This post has like 10 comments and no one except the OP ( who seems to [ shit their pants about the ess jay dubs , just like you ] ( @url ) ) is talking about e-cigs or saying this guy is a saint .\n",
      "285 []\n",
      "['op']\n",
      "/ r / politics : \" blablabla political bullshit Hur dur \"\n",
      "286 []\n",
      "[]\n",
      "@url\n",
      "287 []\n",
      "[]\n",
      "I was referencing a Famous Reddit Post @url\n",
      "288 []\n",
      "[]\n",
      "Jesus christ .\n",
      "289 []\n",
      "[]\n",
      "Chelsea lost yesterday , now I ' m hiding from the match and post match threads on [ / r / soccer / ] ( @url )\n",
      "290 []\n",
      "['chelsea']\n",
      "Ken King said it himself when he started talking about CalgaryNEXT that he wants * less * seats in the new arena .\n",
      "291 []\n",
      "['ken king']\n",
      "LOL .\n",
      "292 []\n",
      "[]\n",
      "Writing a fake interview with Messi is easy , [ I even did it myself here a few years ago . . . ] ( @url )\n",
      "293 []\n",
      "['messi']\n",
      "* * submission title * * : Adebayor signs for Turkish side Basaksehir\n",
      "294 []\n",
      "['adebayor']\n",
      "One last reminder for [ Oldies Night ] ( @url ) - still time to join in .\n",
      "295 []\n",
      "[]\n",
      "[ Fake News , eh ?\n",
      "296 []\n",
      "[]\n",
      "[ Please do n't forget that liberals and Democrats are Nazi Useful Idiots who believe government / CIA propaganda and Fake News sites .\n",
      "297 []\n",
      "['cia']\n",
      "[ Okay .\n",
      "298 []\n",
      "[]\n",
      "Even though I've only been once through a reciprocal membership , I 'll nominate Virgin Active at Moore Park / Zetland for you .\n",
      "299 []\n",
      "['moore park', 'zetland']\n",
      "[ Answered here .\n",
      "300 []\n",
      "[]\n",
      "& gt ; It's the body of a 2 time Super Bowl MVP , goddammit .\n",
      "301 []\n",
      "[]\n",
      "Geez .\n",
      "302 []\n",
      "[]\n",
      "He's a ~ ~ not-so- ~ ~ silent guardian , a watchful protector , a Dark Knight .\n",
      "303 []\n",
      "['dark knight']\n",
      "err you commented on Zlatan [ right here ] ( @url ) in a thread where the headline literally contains the phrase \" alleged doping allegation \" .\n",
      "304 []\n",
      "['zlatan']\n",
      "Lel\n",
      "305 []\n",
      "[]\n",
      "There's no problem wanting to feed them , but please , read the guide from Zealandia before doing so : [ Feeding birds at home ] ( http://www.visitzealandia.com/Portals/0/Feeding%20Birds%20at%20Home.pdf )\n",
      "306 []\n",
      "['zealandia']\n",
      "It happens on United related posts .\n",
      "307 []\n",
      "['united']\n",
      "Trudeau hanging out with his brother . . .\n",
      "308 []\n",
      "[]\n",
      "[ So that random website Sky Sports quoted was correct ] ( @url )\n",
      "309 []\n",
      "['sky sports']\n",
      "* * redditor * * : Jay 911\n",
      "310 []\n",
      "[]\n",
      "What MontansKittenSighs says has nothing to do with me .\n",
      "311 []\n",
      "['montanskittensighs']\n",
      "[ Oh .\n",
      "312 []\n",
      "[]\n",
      "/ r / politics has no problem with threads about hate crimes , as long at they ' re being attributed to Trump supporters .\n",
      "313 []\n",
      "['trump']\n",
      "Looking at you , / u / TruFalcon .\n",
      "314 []\n",
      "[]\n",
      "& gt ; Are you hoping that someone in R / Calgary just happens to be a stock shelfer there or has an extremely intimate knowledge of all of their 1 million items ?\n",
      "315 []\n",
      "['calgary']\n",
      "I am not a huge supporter of Corbyn , ive been rather critical of a lot of his moves recently but I like the idea of a lot of his policies .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316 []\n",
      "['corbyn']\n",
      "Of course there are some Leave supporters who are not against immigration .\n",
      "317 []\n",
      "[]\n",
      "Hate crime confirmed\n",
      "318 []\n",
      "[]\n",
      "I ' m pretty sure this rarely happens in Europe but it's still wrong .\n",
      "319 []\n",
      "['europe']\n",
      "Despite being a politically motivated hate crime this was removed by / u / lucastars because of \" Off Topic . \"\n",
      "320 []\n",
      "[]\n",
      "[ Source ] ( @url ) at / r / sydney\n",
      "321 []\n",
      "[]\n",
      "Fantasy Drama Movie about a Girl Looking for her Father\n",
      "322 []\n",
      "[]\n",
      "How does this answer the question ? This is commentary at best .\n",
      "323 []\n",
      "[]\n",
      "Path integral measure in Coleman's \" Aspects of symmetry \"\n",
      "324 []\n",
      "['coleman']\n",
      "No . It would be p equals some specified value at some z elevation . This has to correspond to some known pressure at some known elevation .\n",
      "325 []\n",
      "[]\n",
      "This is wrong , Electric field is nonconservative when magnetic field is changing and you cannot apply Kirchhoff's law then .\n",
      "326 []\n",
      "['kirchhoff']\n",
      "Looking up touchscreen history - found this 1981 computer that used Infrared to detect finger movement . Clearly Star Trek was an inspiration . @url\n",
      "327 []\n",
      "['star trek']\n",
      "I ' m referring to the ones newly introduced in Rogue One - I ca n't remember if they were given a specific name - I only remember them being referred to as ' Imperial Droids '\n",
      "328 []\n",
      "['rogue one', 'imperial droids']\n",
      "Religious persecution is hardly unique to communist countries . It's practically universal .\n",
      "329 []\n",
      "[]\n",
      "How would you classify the documentary Transcendent Man ?\n",
      "330 []\n",
      "[]\n",
      "Which protocol should I use for secure message transfers between two servers ?\n",
      "331 []\n",
      "[]\n",
      "As you mention , it the fact that the first polariser ' converts ' unpolarised light to polarised light that does the majority of the perceived attenuation . For an interesting test ( for the OP who has the sheets . . ) , take the first two sheets and arrange them as a crossed polariser at maximum extinction , then insert a third polariser between them at a 45 degree angle . What happens and Why . . . ?\n",
      "332 []\n",
      "['op']\n",
      "@notstoreboughtdirt - Probably not . \" [ C ] onduct or motive unworthy or unbecoming a Senator \" does n't cover anything related to the job they will be taking or specific facts or commentary on the nominee's qualifications . It only restricts discussing * things that were done * ( conduct ) or the * reasons for them * ( motive ) . So \" You have no experience with_______\" is valid , but \" You took bribes from the_____industry \" is not .\n",
      "333 []\n",
      "['senator']\n",
      "Just wanted add , anyone notice that in the finale , French is dressed in the exact outfit that Homer was in from Cuba onwards ?\n",
      "334 []\n",
      "['french', 'homer', 'cuba']\n",
      "@SmedleyDSlap , that sounds suspiciously similar to Universal Basic Income , but not as good .\n",
      "335 []\n",
      "[]\n",
      "Science fiction novel about an assassin's guild that claims to be apolitical ( while doing the local tyrant's dirty work )\n",
      "336 []\n",
      "[]\n",
      "What exactly is Trump's economic policy , and is it consistent with his previous statements ?\n",
      "337 []\n",
      "['trump']\n",
      "@alexwlchan - You ' re right . I've added it to my answer below .\n",
      "338 []\n",
      "[]\n",
      "I vote to reopen . This does show effort and is about a physics concept .\n",
      "339 []\n",
      "[]\n",
      "But keep in mind that trump expressed his hatred for snowden\n",
      "340 []\n",
      "['trump']\n",
      "Identify The Day of the Doctor old shots\n",
      "341 []\n",
      "[]\n",
      "$ SU ( 2 )_L $ Gauge theory and particle-antiparticle annihilation\n",
      "342 []\n",
      "[]\n",
      "hmmnn . . . maybe . Sorry , I ' m not sure to be able to help you but maybe would be as simple as \" give it to them if you trust them \" . If not , ask why and why and why until they give you a reasonable reason or if not , do n't give to them the documents .\n",
      "343 []\n",
      "[]\n",
      "how long is \" a few years back \" ?\n",
      "344 []\n",
      "[]\n",
      "@pela Ok Thanku . Will search for it . Learn it when it calls . Thanku\n",
      "345 []\n",
      "[]\n",
      "@Gallifreyan Give the people what they want .\n",
      "346 []\n",
      "[]\n",
      "When did Clay become a phoner ?\n",
      "347 []\n",
      "['clay']\n",
      "In the Travel Ban Executive order - What is the \" Biometric Entry- Exit Tracking System ? \"\n",
      "348 []\n",
      "[]\n",
      "Why were Olive and Emma's powers changed in Miss Peregrine's Home for Peculiar Children ?\n",
      "349 []\n",
      "['olive', 'emma', 'miss peregrine']\n",
      "This is a great example of a scale error .\n",
      "350 []\n",
      "[]\n",
      "What explains the connection between the boat's name and Lee's daughter ?\n",
      "351 []\n",
      "['lee']\n",
      "Why is n't a standard for encrypted but open WiFi developed ?\n",
      "352 []\n",
      "[]\n",
      "Old novel of a hunter on a man-eating island ; he has to escape from another hunter who would try to kill him\n",
      "353 []\n",
      "[]\n",
      "senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . the republicans warned that without the nuclear option , the democrats could lose a powerful means to stop the republican administration nominees if / when the table is turned . well , the democrats today would have loved to undo what they enthusiastically did a few year back .\n",
      "354 []\n",
      "[]\n",
      "It's best to consider * Voyager * as a whole as non-canon .\n",
      "355 []\n",
      "['voyager']\n",
      "Why was porygon in this episode of Pokémon\n",
      "356 []\n",
      "[]\n",
      "About role of a minister in Parliament who is not a member of either house of Parliament .\n",
      "357 []\n",
      "[]\n",
      "Server key length in OpenSSH\n",
      "358 []\n",
      "['openssh']\n",
      "In my judgment , the goal is to determine the torque about the COM . This can then be set equal to the angular acceleration times the moment of inertial about the COM . We can get this torque about the COM using any other point as the axis of rotation by including the pseudo force through the COM and the moment it produces .\n",
      "359 []\n",
      "[]\n",
      "And one might also ask . . . * what was his plan there , anyway ? ! * How on earth did he know where 'll hit and who 'll hurt ?\n",
      "360 []\n",
      "[]\n",
      "what do we mean by ' regular person ' . Has n't Rick only ever explained what Dimension he is from to those individuals who are aware of multiple Dimensions ?\n",
      "361 []\n",
      "['rick']\n",
      "Ok , thanks . What's the difference between ballistic galvanometer and classic galvanometer ?\n",
      "362 []\n",
      "[]\n",
      "Hi Garvan and welcome to the Physics SE ! Please note that this is not a homework help site . Please see [ this Meta post on asking homework questions ] ( http://meta.physics.stackexchange.com/questions/714/how-do-i-ask-homework-questions-on-physics-stack-exchange ) and [ this Meta post for \" check my work \" problems ] ( http://meta.physics.stackexchange.com/questions/6093/should-any-check-my-work-questions-be-made-on-topic ) .\n",
      "363 []\n",
      "[]\n",
      "Is it safe to download internet files over TOR\n",
      "364 []\n",
      "['tor']\n",
      "Java Object encryption\n",
      "365 []\n",
      "[]\n",
      "I ' m not so sure that there is a significant anti- Eastern Europe vibe among the parties mentioned . Politicians like Orban are viewed as leaders acting in their national interest , which might not align with Western European interests but they are still respected for by other nationalists .\n",
      "366 []\n",
      "['eastern europe', 'orban']\n",
      "Is it OK to pass credentials to the client to allow it to upload files to Amazon S 3 ?\n",
      "367 []\n",
      "[]\n",
      "This is no joke my friend !\n",
      "368 []\n",
      "[]\n",
      "the reason why I ask is because I want to be able to create a java . security . PublicKey object with the bytes and I do n't know how to do that .\n",
      "369 []\n",
      "[]\n",
      "Can the president appoint him- or herself to the Supreme Court ?\n",
      "370 []\n",
      "['supreme court']\n",
      "\" A rather larger body , varying at need , was employed to ' beat the bounds ' , and to see that Outsiders of any kind , great or small , did not make themselves a nuisance . \" Not sure exactly how that needs to be interpreted .\n",
      "371 []\n",
      "[]\n",
      "But is ryll mined on Kessel ? They mention that planet in ANH , and then never again .\n",
      "372 []\n",
      "['kessel', 'anh']\n",
      "@Xantec . You ' re right , it is . The Hogwarts staff member delivering the letter would be expected to mention Diagon Alley ( as Dumbledore did ) . Hagrid neglected to do this by just posting the letter through the letterbox . So really it's an oversight on the part of Hagrid ( although obviously in reality he did take Harry to DA in person ) .\n",
      "373 []\n",
      "['the hogwarts', 'diagon alley', 'dumbledore', 'hagrid', 'hagrid', 'harry']\n",
      "Good , I got it right then . Thanks for the answer !\n",
      "374 []\n",
      "[]\n",
      "Aliens , capable of interstellar travel . Im sure forging some documents are easy enough .\n",
      "375 []\n",
      "[]\n",
      "How do we know the number of photons in a decay ?\n",
      "376 []\n",
      "[]\n",
      "What do you mean by \" the average force normal to surface on the ball cancels ( due to assumption of uniformity in the field ) \" ?\n",
      "377 []\n",
      "[]\n",
      "Why do you think surface charge density should not change ? \" By my knowledge \" is not a reason .\n",
      "378 []\n",
      "[]\n",
      "Hmm , but is n't the [ Vrĳzinnige Partĳ ] ( @url ) in parliament under the name [ Groep Klein ] ( @url ) ? Then why Nieuwe Wegen / Monasch but not Vrĳzinnige Partĳ / Klein ?\n",
      "379 []\n",
      "['groep klein']\n",
      "Why not ? I ' m just accepting observations . If A's angle is vertical and B's angle is 60 ° , then B gives opposite spins 3 / 4 of the time . That's real , I ca n't disagree with that . I ' m not saying the inequality is wrong , as I said in the first paragraph , I ' m saying the experiment ca n't reproduce it . Check what I've answered to the other guy .\n",
      "380 []\n",
      "['b', 'b']\n",
      "How does the state variables $ ( P , V ) $ remain constant for a system with adiabatic wall\n",
      "381 []\n",
      "[]\n",
      "[ This page ] ( http://sf-encyclopedia.com/entry/time_loop ) lists a 1932 short story , but I ' m certain the genre is much older than that .\n",
      "382 []\n",
      "[]\n",
      "Why did n't the T- 1000 terminator shoot the eyes of the T- 800 ?\n",
      "383 []\n",
      "[]\n",
      "You are right , the notation is pretty awful , I just tried to follow that of the book though . Now I rewrite It , also because I think I can handle the first part .\n",
      "384 []\n",
      "[]\n",
      "Image of diverging lens\n",
      "385 []\n",
      "[]\n",
      "@Steeven added a sketch .\n",
      "386 []\n",
      "[]\n",
      "Physical chemistry problem\n",
      "387 []\n",
      "[]\n",
      "This answer seems speculative , more of an opinion piece ; but the OP is requesting fact checking . Supposing at present the facts are obscured by mystery , there's no harm in leaving it unanswered .\n",
      "388 []\n",
      "['op']\n",
      "[ The version found in the CoS game ] ( http://i.imgur.com/LhhRTyG.jpg ) ( which has been [ confirmed to be written by Jo ] ( http://web.archive.org/web/20100310145702/ http://www.jkrowling.com/textonly/en/faq_view.cfm?id=96 ) ) did n't mention the Order . The version packaged with [ the Hasbro chocolate frog cards ] ( @url ) ( not confirmed to be from Jo ) did . This is correctly explained in [ the Lexicon's updated article ] ( @url ) .\n",
      "389 []\n",
      "['cos', 'jo', 'jo']\n",
      "What is the electric potential at the point charge , at the source of field ?\n",
      "390 []\n",
      "[]\n",
      "Can u please elaborate or suggest any online resource where i could find more about this ?\n",
      "391 []\n",
      "[]\n",
      "@Carpetsmoker : That's specifically against Eastern Europeans working in Western Europe .\n",
      "392 []\n",
      "['eastern europeans', 'western europe']\n",
      "I gave a source ( the link ) . Here's another : @url\n",
      "393 []\n",
      "[]\n",
      "How to ensure wildcard certificate private key security ?\n",
      "394 []\n",
      "[]\n",
      "@Brythan - I do n't think this is a duplicate , but I did n't do a good job of separating my question from that one . Hopefully now it's more clear that I am not asking for advantages ( although some advantages may be relevant ) .\n",
      "395 []\n",
      "[]\n",
      "@Qmechanic yeah , I waffled over this , but ultimately chose Physics since it seemed to have more continuum-mechanics related questions .\n",
      "396 []\n",
      "[]\n",
      "Is redirecting in htaccess providing enough security for sensitive pages ?\n",
      "397 []\n",
      "[]\n",
      "@CPHPython It works when your intrusion scanner is just grepping for ' assert ' . How many of them do that , I have no idea .\n",
      "398 []\n",
      "[]\n",
      "Difference between Executive Orders and Presidential Memoranda\n",
      "399 []\n",
      "['presidential memoranda']\n",
      "Why was Thorin so interested in gold in the film , when he was really just after the Arkenstone ?\n",
      "400 []\n",
      "['thorin']\n",
      "Possible anomaly in Cryptonomicon\n",
      "401 []\n",
      "['cryptonomicon']\n",
      "@Trilarion Negative . Torture has been illegal for a while . There was never an attempt to make torture legal . Rather , they categorized the actions they were taking as \" not torture \" but enhanced interrogation techniques .\n",
      "402 []\n",
      "[]\n",
      "I ' m voting to close this question as off-topic because this is a legal question and belongs on [ Law . SE ] .\n",
      "403 []\n",
      "[]\n",
      "This is a good explanation , thanks .\n",
      "404 []\n",
      "[]\n",
      "Welcome to Physics Stack Exchange ! [ This related question ] ( http://physics.stackexchange.com/q/5326/56299 ) may help you .\n",
      "405 []\n",
      "['physics stack exchange']\n",
      "[ Professor Monkey- For- A-Head ] ( http://earthwormjim.wikia.com/wiki/Professor_Monkey-For-A-Head ) from_Earthworm Jim_preceded him , but I do n't if that counts : - )\n",
      "406 []\n",
      "[]\n",
      "But , I guess we look at this emphasizing very different principles . I appreciate the time you put into answering my comments . Looks like the moderators think the exchange is too long , so may be we can return to this in the context of some thread .\n",
      "407 []\n",
      "[]\n",
      "What's meant by this bill ( H . R . 720 ) text ?\n",
      "408 []\n",
      "[]\n",
      "What was funny about Hanzo's hand in the film Predators ?\n",
      "409 []\n",
      "['hanzo', 'predators']\n",
      "I ' m uncomfortable with this kind of question . It's asking for direct help in cracking someone's wifi network .\n",
      "410 []\n",
      "[]\n",
      "In the context of FIDO U 2 F , when is a new ephemeral key reused , or cached ?\n",
      "411 []\n",
      "['fido']\n",
      "Were the Borg Always intended to be a Delta Quadrant power ?\n",
      "412 []\n",
      "['borg', 'delta quadrant']\n",
      "Amazon Echo / Dot on your network\n",
      "413 []\n",
      "['amazon echo']\n",
      "Percent difference does n't work for my data\n",
      "414 []\n",
      "[]\n",
      "Which public key encryption method should I choose to sign ?\n",
      "415 []\n",
      "[]\n",
      "Your responses are much appreciated .\n",
      "416 []\n",
      "[]\n",
      "In \" The Six Thatchers \" , why does Watson blame Sherlock rather than himself ?\n",
      "417 []\n",
      "['the six thatchers', 'watson', 'sherlock']\n",
      "Network administrator knowing all user passwords\n",
      "418 []\n",
      "[]\n",
      "I recommend rewording your question to \" What technical means are available to deal with state actors breaking TLS ? \" . The question as it is currently formulated is better suited for law . SE .\n",
      "419 []\n",
      "[]\n",
      "Are the TIE Fighter pilots clones ?\n",
      "420 []\n",
      "[]\n",
      "Does energy of photon change due to some external magnetic field ?\n",
      "421 []\n",
      "[]\n",
      "The form \" Q \" takes is cosmetic as implied by the episode , [ Déjà Q ] ( @url ) in which another \" Q \" appears and remarks at the bipedal nature of their circumstance .\n",
      "422 []\n",
      "[]\n",
      "Why is visa-free travel such a big deal in politics ?\n",
      "423 []\n",
      "[]\n",
      "buddy do n't say xmen 3 as disaster I like original trilogy .\n",
      "424 []\n",
      "[]\n",
      "Do countries frame their contributions in climate change negotations in reference to others ?\n",
      "425 []\n",
      "[]\n",
      "I feel like other animals could kick a field goal given proper training . Esp . since the FG distance wasnt very far for Snowflake\n",
      "426 []\n",
      "[]\n",
      "I think this answer is wrong — I certainly took it to be meant literally . The last line in your answer is key , but it's entirely possible that the worth of market value of small amounts of mithril is so ridiculously high that , scaled to a whole armour made of mithril , its value would indeed dwarf an unimportant economy such as the Shire's , without requiring an exact calculation . This isn ’ t * that * unrealistic for an item's value ( as a gift , it kind of is though ) .\n",
      "427 []\n",
      "['shire']\n",
      "Do politicians ever admit they made a bad decision ?\n",
      "428 []\n",
      "[]\n",
      "I still think that with more details from your actual proof , you might have a better chance of getting something which is useful for you .\n",
      "429 []\n",
      "[]\n",
      "Running a local server inside a virtual machine disconnected from the Internet - is this secure ?\n",
      "430 []\n",
      "[]\n",
      "What non- Star Trek movie used the TOS font for the end credits ?\n",
      "431 []\n",
      "['star trek']\n",
      "Book narrated by the devil\n",
      "432 []\n",
      "[]\n",
      "Does someone have it out for me , or is this router \" features \" ?\n",
      "433 []\n",
      "[]\n",
      "Store SQL database credentials in a webserver\n",
      "434 []\n",
      "[]\n",
      "That's not . p 12 format . Just . PEM format with the ( wrong ) . p 12 file name extension . Do you really , really need p 12 format ? ( If what you did works , then you don't , since it never was P 12 . )\n",
      "435 []\n",
      "[]\n",
      "[ Related answer ] ( http://movies.stackexchange.com/a/33905/1190 ) for you bonus sidenote with word of god .\n",
      "436 []\n",
      "[]\n",
      "Anything else you can remember about th's book ? [ This ] ( http://meta.scifi.stackexchange.com/questions/9335/how-to-ask-a-good-story-id-question ) could help .\n",
      "437 []\n",
      "[]\n",
      "What did Tarzan say to Cheeta when Cheeta did a good job ?\n",
      "438 []\n",
      "['tarzan']\n",
      "Why is there a disclaimer about not accepting money from tobacco companies at the end of movies ?\n",
      "439 []\n",
      "[]\n",
      "How does the President of the United States get a budget authorised ?\n",
      "440 []\n",
      "['united states']\n",
      "When did the practice of having a joke with no music at the end of a trailer begin ?\n",
      "441 []\n",
      "[]\n",
      "If at some point President Trump was impeached , what would it take to actually be removed ?\n",
      "442 []\n",
      "['president trump']\n",
      "What security considerations are there when developing a random password generator ?\n",
      "443 []\n",
      "[]\n",
      "A little update regarding this : I have not been called a single time by any kind of advertising .\n",
      "444 []\n",
      "[]\n",
      "Comment deleted and made into an answer .\n",
      "445 []\n",
      "[]\n",
      "All right , seems we will remain with different opinions on this , as to me , TOS * Metamorphosis * appears to be the one-off episode .\n",
      "446 []\n",
      "[]\n",
      "It seems to me this answer confirms what the restrictions on a 501 ( c ) 3 are w . r . t a candidate ; I think the more relevant issue here is whether the filing makes Trump a \" candidate \" for purposes of said restrictions , especially in light of the first phrase in the filing , \" While this does not constitute a formal announcement of my candidacy for the 2020 election . \"\n",
      "447 []\n",
      "['trump']\n",
      "Animated movie about kid who lost his parents in a car accident with a deer ( got reunited via time travel )\n",
      "448 []\n",
      "[]\n",
      "Access Control Primitives\n",
      "449 []\n",
      "['access control primitives']\n",
      "unfortunately not , this is also not a series like I described . Arghh its bugging me haha I hope I find it\n",
      "450 []\n",
      "[]\n",
      "Duplicate of http://physics.stackexchange.com/q/43293/176 ?\n",
      "451 []\n",
      "[]\n",
      "Why is a sleeping bag so cold when you first get in ?\n",
      "452 []\n",
      "[]\n",
      "Which Sci- Fi work introduced the idea of \" Touchscreen used by fingers \" ?\n",
      "453 []\n",
      "[]\n",
      "Thanks . A better question : did you recognise her as fishing before , or after , finding the full-sized photo ?\n",
      "454 []\n",
      "[]\n",
      "Commutation Relation between the Dirac Hamiltonian and Helicity operator\n",
      "455 []\n",
      "['dirac hamiltonian', 'helicity']\n",
      "What about the other AGRA drives ?\n",
      "456 []\n",
      "['agra']\n",
      "Does that imply presidents sometimes push agendas that are n't meaningful to them ?\n",
      "457 []\n",
      "[]\n",
      "Interesting question , and I agree with you that it deserves a separate question . Please do , I will reply in this separate post .\n",
      "458 []\n",
      "[]\n",
      "Why do so many kids in Digimon wear gloves ?\n",
      "459 []\n",
      "['digimon']\n",
      "Firstly R & R signifies Rest and Recreation . So the recreation part takes care of the radio programs part of the question . Secondly , as you can see my last quoted question , the user asked why radio programs were being run at Vietnam and not why the army has its own program . Please correct me if I understood the question wrong .\n",
      "460 []\n",
      "['vietnam']\n",
      "How did my professor sniff my Gmail password ?\n",
      "461 []\n",
      "['gmail']\n",
      "Ghost from their kindergarden will follow them in their dreams\n",
      "462 []\n",
      "[]\n",
      "Do we know the timeline on the Sci- Fi names in the question ? I would guess that early ones were doctors , and later ones could have just as easily been inspired by the early ones as by the actual doctor .\n",
      "463 []\n",
      "[]\n",
      "I have worked with multiple frameworks , but still ca n't see what your question is . . .\n",
      "464 []\n",
      "[]\n",
      "@Paul leave the actors enough time in the snow and you wo n't have to pay them anymore ! : - D\n",
      "465 []\n",
      "[]\n",
      "How to protect an offline proprietary software from being stolen\n",
      "466 []\n",
      "[]\n",
      "@poisson I see . That type of symmetry breaking is not what you asked about in your question , which is why my answer discusses a different type of symmetry breaking . But your comment here certainly implies a fair additional question . The answer is complicated enough that I wo n't have room for it in this comment , so 'll add another section to my answer headed \" Buckling , Crumbling , and Toppling \" . But . . .\n",
      "467 []\n",
      "[]\n",
      "They also used ' magic'to accomplish their works . Minas Tirith ( The First Age one ) had a spell bounding stone to stone , and it collapsed after Lúthien had broken it .\n",
      "468 []\n",
      "['minas tirith', 'lúthien']\n",
      "Visualizing the $ H $ field\n",
      "469 []\n",
      "[]\n",
      "How steady is your hand ? But seriously , this question is entirely wrong-headed if you are concerned about safety . If anything is going to get close to an electrical contact , it must be de-powered . No exceptions !\n",
      "470 []\n",
      "[]\n",
      "\" * It's all down to Neville . He really gets this room . You've got to ask it for exactly what you need - like , ' I do n't want any Carrow supporters to be able to get in ' - and 'll do it for you ! You've just got to make sure you close the loopholes ! Neville's the man ! * \" - Seamus\n",
      "471 []\n",
      "['neville', 'carrow', 'neville', 'seamus']\n",
      "Why is the Labour Party in such significant decline ?\n",
      "472 []\n",
      "[]\n",
      "The spin-orbit interaction for a classical magnetic dipole moving in an electric field\n",
      "473 []\n",
      "[]\n",
      "Because light sabers glow in the dark . Search TV Tropes for \" Rule of Cool . \"\n",
      "474 []\n",
      "['rule of cool']\n",
      "What exactly did the federal judge do to Trump's executive order ?\n",
      "475 []\n",
      "['trump']\n",
      "I've edited the question with some links and rephrasing to try to clarify ; feel free to revert if you disagree .\n",
      "476 []\n",
      "[]\n",
      "Agreed . Nothing I've read has referenced a prior similar event .\n",
      "477 []\n",
      "[]\n",
      "We differ politically but this is a great and concise answer . + 1\n",
      "478 []\n",
      "[]\n",
      "Why do fusion cross sections drop after a certain temperature ?\n",
      "479 []\n",
      "[]\n",
      "could you try ' / / \\ nalert ( 1 ) ; window . history . back ( ) ? . if that does n't work , it might be a 500 error with a redirect to root as default behaviour\n",
      "480 []\n",
      "[]\n",
      "Spooky behaviour with Authy\n",
      "481 []\n",
      "['authy']\n",
      "Thank you for the quick reply . Just wanted to confirm one more thing . How do you mitigate this issue ? Is it done by mandating auth for all the mail sending actions ?\n",
      "482 []\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive charge has high potential why ?\n",
      "483 []\n",
      "[]\n",
      "Why did n't Snape just kill Voldemort to buy more time while the latter was recuperating as a Horcrux ?\n",
      "484 []\n",
      "['snape', 'voldemort']\n",
      "I hate your answer , but it is correct . I , naively , had a different reaction upon reading that part . Namely : \" I ca n't wait to see that in film \" .\n",
      "485 []\n",
      "[]\n",
      "@Snowman \" They mostly come at night . Mostly . \" Excellent reference !\n",
      "486 []\n",
      "[]\n",
      "Suggestions on how I would detect that , or rather ; or whether there would be a Windows Event log confirming this ? ?\n",
      "487 []\n",
      "['windows']\n",
      "Where was the planet inhabited by \" the Giants \" located relative to Earth ?\n",
      "488 []\n",
      "['giants']\n",
      "Why is the perpendicular part a scalar in the electrodynamic boundary conditions at surfaces ?\n",
      "489 []\n",
      "[]\n",
      "Animated TV series with King Arthur / Sword in Stone theme ; ' Excalibur ' replaced by AI robot scepter\n",
      "490 []\n",
      "['king', 'excalibur']\n",
      "Is the story plotline or arc for the Star Wars series going to be overhauled with Carrie Fisher's death ?\n",
      "491 []\n",
      "['star wars', 'carrie fisher']\n",
      "What 1980's kids book series about boy learning magic spells ?\n",
      "492 []\n",
      "[]\n",
      "Why was n't the line of succession for cabinet members followed after they died ?\n",
      "493 []\n",
      "[]\n",
      "can a nation encompassing a diaspora with no geographical delineation also be or have its own state\n",
      "494 []\n",
      "[]\n",
      "Hm , pest should not be a problem for witchers , as their mutation makes them immune to diseases . However , there is another reason to avoid cities or even villages : Most people do n't like witchers and highly distrust them ( yeah , they might be useful to get rid of mosters , but they are somehow monsters themselves and may rob your children if you ' re not careful ) . So it is safer for witchers to stay among themselves , far away from other people .\n",
      "495 []\n",
      "[]\n",
      "What is the meaning if all IoT hacked by Mirai all over the world identify themselves on my server since 15 days in the rhythm of 180 unique IoT / h ?\n",
      "496 []\n",
      "['mirai']\n",
      "@MPS Yep , that's the one ( for Ubuntu ) .\n",
      "497 []\n",
      "['ubuntu']\n",
      "Usually clouds move across the sky , sometimes very fast , sometimes slow . A stationary cloud is not common .\n",
      "498 []\n",
      "[]\n",
      "Do lekku grow with age ?\n",
      "499 []\n",
      "[]\n",
      "Is the soft iron core inside a solenoid moved ?\n",
      "500 []\n",
      "[]\n",
      "< more prayers or religious speeches happened , and one of them was by a Rabbi . this goes to the point I made below - there are many different flavors of \" secular \" , just as there are many different flavors of \" atheist \" .\n",
      "501 []\n",
      "['rabbi']\n",
      "How did the media know the method Batman used to catch the Joker ?\n",
      "502 []\n",
      "['batman', 'joker']\n",
      "Cartoon in which Donald Duck tries to stop another duck from committing suicide\n",
      "503 []\n",
      "['donald duck']\n",
      "Standardized Data Formats for specifying bandstructure and Fermi surfaces\n",
      "504 []\n",
      "['fermi']\n",
      "We do . That's how dams work .\n",
      "505 []\n",
      "[]\n",
      "Is password cracking noisy ?\n",
      "506 []\n",
      "[]\n",
      "The short answer is * No , adding effort to solve the problem wo n't help reopen the question * . See [ this Meta post on asking homework questions ] ( http://meta.physics.stackexchange.com/q/714/ ) and [ this Meta post for \" check my work \" problems ] ( http://meta.physics.stackexchange.com/q/6093/ ) .\n",
      "507 []\n",
      "[]\n",
      "If someone transfers money from a bank account to a prepaid master card , can he trace the master card back\n",
      "508 []\n",
      "[]\n",
      "Hmm ? Please flesh out your question * before * posting it - that's also in your own interest , since people will vote on the version they currently see , and will not necessarily return once you have finalized it to correct their vote .\n",
      "509 []\n",
      "[]\n",
      "As soon as I saw your answer I jumped out of bed to look for this movie . I could n't possibly thank you enough , you have no idea of the service you have done me , I've been looking for this movie ever since it came out , and you gave me the answer in less than 2 hours . Best of luck to you , glad to be a member of this community .\n",
      "510 []\n",
      "[]\n",
      "@skooba your are glossing over all of the complexities . Because something is easy for you does not mean it is for everyone else .\n",
      "511 []\n",
      "[]\n",
      "What's different between two formulas in my problem . Which one shall I use . ?\n",
      "512 []\n",
      "[]\n",
      "@theguest It's an exchange specifically for science fiction . I was trying to be helpful .\n",
      "513 []\n",
      "[]\n",
      " ll certainly check it out . But superior ? As a native kiwi that will be my judgement to make : - P\n",
      "514 []\n",
      "[]\n",
      "probably not the original reference : @url\n",
      "515 []\n",
      "[]\n",
      "If it is called Mandalorian , I would think that the Mandalorians designed it .\n",
      "516 []\n",
      "['mandalorians']\n",
      "I do n't know what is the area element i must use in spherical or cylindrical ones .\n",
      "517 []\n",
      "[]\n",
      "Thanks a lot , but can you give me a link for the first site , GhostSec's pentest labs ?\n",
      "518 []\n",
      "['ghostsec']\n",
      "Please provide as much information as possible . When and where you watched it , language , color or black & white etc\n",
      "519 []\n",
      "[]\n",
      "On a molecular level , what is the difference between a liquid and a powder ?\n",
      "520 []\n",
      "[]\n",
      "The Matrix : How did the Nebuchadnezzar know where to find Neo in the real world ?\n",
      "521 []\n",
      "['the matrix', 'nebuchadnezzar', 'neo']\n",
      "and I'd disagree w / @Edlothiad's response - it is absolutely based more on myth than sci-fi concepts - but , ultimately , that does n't matter since Valorum is correct : )\n",
      "522 []\n",
      "['valorum']\n",
      "important addition — if anyone touches your unencrypted installation , consider it trojaned — wipe drive completely and install fresh OS afterwards\n",
      "523 []\n",
      "[]\n",
      "Distribution of keys for a python messaging application\n",
      "524 []\n",
      "[]\n",
      "I've always thought that this is one of the few occasions where you need to read the book to fully understand the film . . though you also need to see the film to fully understand the book . You've got to viddy to kopat , that's my soviet , my old droogs ; ) [ You also need one finger in the glossary , right the way through ]\n",
      "525 []\n",
      "[]\n",
      "Is mankind still doomed at the end of \" Oblivion \" ?\n",
      "526 []\n",
      "['oblivion']\n",
      "And what is not useful with this answer ? Downvotes with no explanation are not useful to anyone .\n",
      "527 []\n",
      "[]\n",
      "What does the ending scene of The Sopranos mean ?\n",
      "528 []\n",
      "['the sopranos']\n",
      "@defalt I do n't think OP was asking about the VPN server's port ( which I addressed , too ) .\n",
      "529 []\n",
      "['op']\n",
      "Understood . I ' m looking more at knowledge management and consumption for people . C\n",
      "530 []\n",
      "[]\n",
      "How replaceable are \" career \" bureaucrats by new administrations ?\n",
      "531 []\n",
      "[]\n",
      "Ah , good catch . The last one looks suspiciously familiar , too .\n",
      "532 []\n",
      "[]\n",
      "Is it secure to login to your online banking through a third party ?\n",
      "533 []\n",
      "[]\n",
      "Hi , welcome to the site . You say the Hulk saw'something messed up ' - do you have any quotes or information from the film that would help explain what is was he actually saw ?\n",
      "534 []\n",
      "['hulk']\n",
      "I wish they'd spend research efforts on how to get more voters to act rationally . : )\n",
      "535 []\n",
      "[]\n",
      "Is there any evolutionary advantage to the position of the Vulcan heart ?\n",
      "536 []\n",
      "[]\n",
      "What was the Donnager's escort doing during the battle ?\n",
      "537 []\n",
      "[]\n",
      "What does this mean \" upending standard committee rules \" ?\n",
      "538 []\n",
      "[]\n",
      "How much information is stored in a genome and how much in the distribution of genomes in a species ?\n",
      "539 []\n",
      "[]\n",
      "What's the most accurate account of the current President's monetary debts ?\n",
      "540 []\n",
      "[]\n",
      "How did the doctor know the cause of death just by putting his ear on the dead body ?\n",
      "541 []\n",
      "[]\n",
      "@Valorum : Do you want to grab any or all of my memory erasure bit ?\n",
      "542 []\n",
      "[]\n",
      "Why do all Potterwatch contributors ' codenames start with \" R \" ?\n",
      "543 []\n",
      "[]\n",
      "Buffer overflow attack\n",
      "544 []\n",
      "[]\n",
      "Why was Hogwarts so adamant about having Harry go there ?\n",
      "545 []\n",
      "['hogwarts', 'harry']\n",
      "If you do n't have a specific question then why ' re you posting here ? What do you expect the community to help you with ?\n",
      "546 []\n",
      "[]\n",
      "Well this should be some coefficient or a number that I could use to solve the second part of my \" problem \" . I have added the volume figure .\n",
      "547 []\n",
      "[]\n",
      "The blue shifting could even get to the point that the microwave background is visible , I really do n't know . I think as far as the more extremely warped paths of light coming up from near the black hole , you might look at the black hole from interstellar . They modeled all of the light really well . However , I think you have some creative leeway here given just how complex the paths of rays near a black hole can be . Best of luck\n",
      "548 []\n",
      "[]\n",
      "A futuristic book I am sure was titled \" Grandfather Bank \"\n",
      "549 []\n",
      "['grandfather bank']\n",
      "Please use mathjax to format mathematical expressions . To learn more about mathjax , please read [ MathJax basic tutorial and quick reference ] ( http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference ) .\n",
      "550 []\n",
      "[]\n",
      "You do know what Borking means , right ?\n",
      "551 []\n",
      "[]\n",
      "I believe in that world I'd say \" I ' m Negan , \" wander a while and then try to kill him . But definitely not after he caught a bullet with a bat , after that I'd probably decide not to mess with him .\n",
      "552 []\n",
      "['negan']\n",
      "Define democracy . The majority rules , but are \" representatives \" OK to the very notion of democracy ? If so , are the types operating today - legislators , governors etc OK ? Also , should we include full transparency of such a state in its definition ?\n",
      "553 []\n",
      "[]\n",
      "Why is $ C_V $ used in adiabatic work by an ideal gas ?\n",
      "554 []\n",
      "[]\n",
      "Great ! Please also mark my answer as the correct answer , there is an option for that . Thanks .\n",
      "555 []\n",
      "[]\n",
      "Ca n't upload payload to my apache 2 server . Pentesting exercise\n",
      "556 []\n",
      "[]\n",
      "The affair Season 3 Episode 10 Noah half - French translation\n",
      "557 []\n",
      "['noah', 'french']\n",
      "E @user 568458 Unfortunately , my impression of him on the show was similar to the second , less masculine Daario ( i . e . bland as opposed to the hyper-masculine dandy in the books . ) Euron is incredibly spooky , dark and cruel , possessed of strange magic and relics . His portrayal on the show as is fairly bewildering .\n",
      "558 []\n",
      "['daario']\n",
      "Could a President withdraw nominated Supreme Court judge before confirmation by the senate ?\n",
      "559 []\n",
      "['supreme court']\n",
      "dmckee , I think you should post your comment as an answer so I can mark it accordingly . What you mentioned makes sense .\n",
      "560 []\n",
      "[]\n",
      "In the RotS scene where Obi- Wan and Anakin are stupid enough to walk backwards into the elevator , the droids could have just shot them .\n",
      "561 []\n",
      "['rots', 'obi- wan', 'anakin']\n",
      "Turkish military presence in other countries soil\n",
      "562 []\n",
      "[]\n",
      "Avoided Crossing in QM\n",
      "563 []\n",
      "[]\n",
      "I am unable to get rid of extra charecters with comments in SQL injection payload\n",
      "564 []\n",
      "[]\n",
      "Why do n't cities plant more fruit trees ?\n",
      "565 []\n",
      "[]\n",
      "Help identify a collection of short stories\n",
      "566 []\n",
      "[]\n",
      "@Bebs That's possible too .\n",
      "567 []\n",
      "[]\n",
      "Why is imposing vaccination / immunization so hard to achieve ?\n",
      "568 []\n",
      "[]\n",
      "Few Questions About Tor Hidden Services\n",
      "569 []\n",
      "['tor']\n",
      "It cannot be migrated with an open bounty , but yes , that would have made sense\n",
      "570 []\n",
      "[]\n",
      "@cde should you want to defend your downvote , here http://meta.movies.stackexchange.com/questions/2666/are-spoilers-a-reason-to-get-downvotes\n",
      "571 []\n",
      "[]\n",
      "So , this is a picture from the actual show ? . . . Er , did n't it say on the page where it was from ?\n",
      "572 []\n",
      "[]\n",
      "Looking for a movie where there is a Powerful Board Game played by two brothers ?\n",
      "573 []\n",
      "[]\n",
      "my concern is about securing the content of the output of the api . . request to read the question and the details again\n",
      "574 []\n",
      "[]\n",
      "Please keep it classy\n",
      "575 []\n",
      "[]\n",
      "I ' m voting to close this question as off-topic because it is better suited for worldbuilding . se\n",
      "576 []\n",
      "[]\n",
      "is it because , . As viscosity is zero no viscous force will be there . And viscous force is the reason for pressure drop in tube .\n",
      "577 []\n",
      "[]\n",
      "Who is driving Thomas Jane's hair and wardrobe on the Expanse ?\n",
      "578 []\n",
      "['thomas jane']\n",
      "I ' m afraid this is a deliberate stylistic choice . Also seen in Collide ( 2016 ) and others . Hopefully it's one that will go away .\n",
      "579 []\n",
      "[]\n",
      "What is the momentum between a light and heavy object in motion when the force applied is equal ?\n",
      "580 []\n",
      "[]\n",
      "Can the vice president be fired ?\n",
      "581 []\n",
      "[]\n",
      "@phoog Parliament can do anything it likes . It is sometimes said , by way of emphasis that \" it can make women men , and men women , if it wants \" . The only thing it ca n't do is to pass legislation which binds successor parliament in any way .\n",
      "582 []\n",
      "[]\n",
      "What are the state-of-the art techniques for crowd size estimations ?\n",
      "583 []\n",
      "[]\n",
      "\" Large \" gauge transformation does n't act as do-nothing transformation in QFT : looking for classical analog\n",
      "584 []\n",
      "[]\n",
      "Why did n't the Beast kill Julia ?\n",
      "585 []\n",
      "['julia']\n",
      "How to derive the equation of motion of Born- Infeld ?\n",
      "586 []\n",
      "[]\n",
      "What are the advantages of the French two-round voting system against a one round alternative ?\n",
      "587 []\n",
      "['french']\n",
      "Asymmetry definitions electric $ \\ chi_e $ and magnetic susceptibility $ \\ chi_m $\n",
      "588 []\n",
      "[]\n",
      "How many people are the three billion watching Zaphod Beeblebrox ?\n",
      "589 []\n",
      "['zaphod']\n",
      "Does the \" mole \" like device access Isabella's facial expressions in Her ( 2013 ) ?\n",
      "590 []\n",
      "['isabella']\n",
      "I think that at begining they were 4 – 5 . And in that maze was one guy ( killer ) . At the end of the movie left only a girl and guy with glasses . That killer kiddnaps that girl , cuffs her on the table and he tries to kill . That guy with glasses saves her and they finds way out .\n",
      "591 []\n",
      "[]\n",
      "How did Mon- El know English ?\n",
      "592 []\n",
      "[]\n",
      "Is n't the network called Movies & TV ? ( It clearly is TV , is n't it ? And why would there be an \" academy-award \" tag , if it's off topic ?\n",
      "593 []\n",
      "[]\n",
      "Who legislates military law ?\n",
      "594 []\n",
      "[]\n",
      "What was the tallest structure built in Middle-earth ?\n",
      "595 []\n",
      "[]\n",
      "There you go , answered in the OP .\n",
      "596 []\n",
      "['op']\n",
      "@Demi : I was saying that because I was needing python .\n",
      "597 []\n",
      "[]\n",
      "I used it in NL for 8 years or so and never had any problems . . .\n",
      "598 []\n",
      "['nl']\n",
      "@Gordon Not all , for example gluons and photons do n't interact with the Higgs .\n",
      "599 []\n",
      "['higgs']\n",
      "An old TV show with a man driving around in his cadillac solving problems\n",
      "600 []\n",
      "[]\n",
      "Not sure about this answer . For me the chapter 21 quote included herein conflicts with the \" probably using a Demiguise \" supposition ( which is bolded at the top ) . “ Ah , but the Third Hallow is a true Cloak of Invisibility , Miss Granger ! I mean to say , it is not . . . woven from Demiguise hair \" . I think your second last paragraph gets to the real crux of the matter .\n",
      "601 []\n",
      "['demiguise', 'miss granger', 'demiguise']\n",
      "SSO against many identity providers\n",
      "602 []\n",
      "[]\n",
      "during ssh handshaking server introduce itself - does it improve security to change self presentation ?\n",
      "603 []\n",
      "[]\n",
      "Are there other Mjölnirs ?\n",
      "604 []\n",
      "[]\n",
      "In order to calculate anything , more input data is required ; such as : \" From where do you hit the ball and at what angle ? Do you want to take into account effects due to the spin of the ball ? Should friction be included ? \" Generally ( if you do n't put any restrictions on how you can hit it ) there is no reason that you could not make the ball return ( unless you'd have to hit it so hard that the ball breaks ( which I doubt would be the case ) .\n",
      "605 []\n",
      "[]\n",
      "Why did Clinton wear in all red for an Award ?\n",
      "606 []\n",
      "['clinton']\n",
      "Would the downvoter care to comment ?\n",
      "607 []\n",
      "[]\n",
      "Is Trump tweeting himself on realDonaldTrump ?\n",
      "608 []\n",
      "['trump']\n",
      "@DonatelloSwansino That's good information . You should add it as an answer\n",
      "609 []\n",
      "[]\n",
      "Tango - In the section about the fictional enterprise and the fictional workings of the Star Trek universe there is a section of biographies of the Star Trek characters , including Kirk . My copy of the Making of Star Trek 1968 edition is packed away and unreachable as is any copy of the writers guide I may own .\n",
      "610 []\n",
      "['star trek', 'star trek', 'kirk', 'star trek']\n",
      "What encryption type does Windows Hello use for fingerprint information on Windows 10 latest build ?\n",
      "611 []\n",
      "['windows', 'windows 10']\n",
      "Despite never having seen the movie , im guessing its because they were the four main characters and deserved to be apart from the secondary characters\n",
      "612 []\n",
      "[]\n",
      "Are the Martians saying \" Ut \" or \" Ack \" ?\n",
      "613 []\n",
      "['martians']\n",
      "I actually think that \" Did the Democrats suffer consequences for invoking it ? \" is a better question than the one asked . I think if you ask that . Then after it gets answered youcould ask each one of your other questions assuming you still dont understand how it works .\n",
      "614 []\n",
      "[]\n",
      "In the article linked by Lelouch in his answer below , there is a key point \" The result applies to isolated mechanical systems subject to some constraints , e . g . , all particles must be bound to a finite volume \" . We do not know if the Universe complies to such an assumption .\n",
      "615 []\n",
      "['lelouch']\n",
      "How much official communication was there between the Senate and the Jedi Council ?\n",
      "616 []\n",
      "['jedi council']\n",
      "What old teen movie is this ?\n",
      "617 []\n",
      "[]\n",
      "Black Swan is nowhere near being a horror movie . Some psycho elements - yes .\n",
      "618 []\n",
      "['black swan']\n",
      "Are we currently seeing another ideological realignment happening in today's unstable political climate ?\n",
      "619 []\n",
      "[]\n",
      "Are the Legality of Votes Cast by Non- Citizens Checked After They Have Been Cast ?\n",
      "620 []\n",
      "[]\n",
      "I Periodically shave my head . I can confirm it does hold fuzzy hats well . Actually , keeping your head completely smooth is a lot of work , you have to shave with a real razor ( not an electric one ) daily .\n",
      "621 []\n",
      "[]\n",
      "Good idea , especially for someone who needs this routinely for professional reasons it might be worth buying cheap hardware . I'd like to add two things : 1 ) \" disable wifi physically \" is often nigh impossible . I'd suggest to use a cable , so you never need to connect to a wifi network ( then the device never contains its password ) , or perhaps even remove the wifi chip ( often fairly easy ) . And 2 ) I'd overwrite the stick or sd card after use , not terminate processes or reboot .\n",
      "622 []\n",
      "[]\n",
      "Does using Oauth behind firewall / closed ports requires proxy\n",
      "623 []\n",
      "[]\n",
      "How can a country ( UN or non- UN member ) become a member of World Bank ?\n",
      "624 []\n",
      "['world bank']\n",
      "Javier . . So the net force remains but now involves my hands as well . That makes sense ! oo\n",
      "625 []\n",
      "[]\n",
      "What building in the final scene of Assassins Creed ? ( spoiler )\n",
      "626 []\n",
      "['assassins creed']\n",
      "Probably its the second time coming back from the dead , not just being in the world .\n",
      "627 []\n",
      "[]\n",
      "@CandiedOrange I see a difference here . While exploding bombs or freezing molecules always follow certain chemical and physical laws , human history has witnessed events where perceived equilibrium are broken time after time . Just look at our population and think about what the equilibrium would have been if we were still living in wood and stone age . Or better yet , see how Malthusianism checks out .\n",
      "628 []\n",
      "['malthusianism']\n",
      "Difference between \" C- violation without CP-violation \" and \" C- violation with CP - violation \"\n",
      "629 []\n",
      "['cp', 'cp']\n",
      "Comments are not for extended discussion ; this conversation has been [ moved to chat ] ( http://chat.stackexchange.com/rooms/52466/discussion-on-question-by-ankit-why-is-christian-bale-not-in-the-upcoming-movie ) .\n",
      "630 []\n",
      "[]\n",
      "short story about seeding a planet with humans\n",
      "631 []\n",
      "[]\n",
      "What is the difference between Quasi static and Dynamic simulation ?\n",
      "632 []\n",
      "['quasi']\n",
      "How does one leave a pensieve ?\n",
      "633 []\n",
      "[]\n",
      "Many actors or actresses with aversion to showing skin would n't approve of body doubles . The general public will still think they did it themselves .\n",
      "634 []\n",
      "[]\n",
      "@XiongChiamiov While I can not formally prove it , I find it very hard to believe that feeding SHA 1 with unpredictable input would produce predictable output .\n",
      "635 []\n",
      "[]\n",
      "What is the significance of the torn off button scene in The Next Three Days ?\n",
      "636 []\n",
      "[]\n",
      "Higgs boson and electroweak gauge boson transformations under CP\n",
      "637 []\n",
      "['higgs', 'cp']\n",
      "Relative world carbon footprints by nation ?\n",
      "638 []\n",
      "[]\n",
      "After seeing this [ article ] ( @url ) , I am convinced of the method you give in your answer . However , I have n't still been able to derive the VEV shown in the question . Could you give details ? Also , I ' m not sure why the vacuum ket $ \\ vert 0 \\ rangle $ maps to 1 and why $ \\ vert n \\ rangle $ maps to $ \\ sqrt { n ! } \\ xi ^ n $ .\n",
      "639 []\n",
      "[]\n",
      "What is the difference between Yautja and Super Yautja in Predators ?\n",
      "640 []\n",
      "['yautja', 'yautja', 'predators']\n",
      "What Star Trek TOS characters have been confirmed to be appearing in Discovery ?\n",
      "641 []\n",
      "['star trek tos']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's nothing in law that says a politician ca n't switch parties , but the few times it's happened in history , the politician has been blacklisted by their previous party , and then not exactly welcomed by the new party because they are a turncoat , and could do it again . It is akin to political suicide .\n",
      "642 []\n",
      "[]\n",
      "Nopes , full story was presented in parts , I think 3 parts .\n",
      "643 []\n",
      "[]\n",
      "magnetic potential vs . solid angle\n",
      "644 []\n",
      "[]\n",
      "Why is it necessary to minimize redundancy in the ciphertext of a stream cipher ?\n",
      "645 []\n",
      "[]\n",
      "Identify this post apocalyptic movie with rival cannibal car gangs\n",
      "646 []\n",
      "[]\n",
      "Are Hogwarts students allowed to go to other houses ?\n",
      "647 []\n",
      "['hogwarts']\n",
      "You can add torsion if you want\n",
      "648 []\n",
      "[]\n",
      "I think $ \\ delta \\ omega $ cannot move freely to the right . It's a clifford algebra vulued in this case .\n",
      "649 []\n",
      "[]\n",
      "ID movie : Highschool boy is bullied into a coma\n",
      "650 []\n",
      "[]\n",
      "Oh my god , I did n't know the differential was a thing . . . That helps so much , thanks ! . And @Fedxa , if that's so , then I ' m really confused . I just put a screenshot of the example where I ' m getting that conclusion from .\n",
      "651 []\n",
      "[]\n",
      "Does Rogue One create a plot hole in Return of the Jedi re Hyperspace Radio ?\n",
      "652 []\n",
      "['rogue one']\n",
      "Books are never peer reviewed . I can add some journal references , but they will almost definitely be behind a paywall . I would call it nonpartisan in the sense that is n't an attempt to support any party or candidates , but any analysis of parties is going to vindicate someone .\n",
      "653 []\n",
      "[]\n",
      "@itpastorn yes , but I think you can also take the ones that are described as evidence that there generally * are * holidays , probably many different ones in many different communities , and we just do n't get to hear about them .\n",
      "654 []\n",
      "[]\n",
      "@Wildcard if hope you've done the over 8 years it's required in most of the western world's universities to complete a psycology or psychiatry Bsc + MsC + MIR / PIR Degree for your facts , rather than a internet blog .\n",
      "655 []\n",
      "[]\n",
      "Schroeder's Minkowski Space Integral - Concerns about Wick Rotations\n",
      "656 []\n",
      "['wick rotations']\n",
      "@Gallifreyan , good question .\n",
      "657 []\n",
      "[]\n",
      "Similar to http://security.stackexchange.com/questions/4369/why-is-https-not-the-default-protocolalsocompare http://webmasters.stackexchange.com/questions/1823/https-for-entire-sitevs http://webmasters.stackexchange.com/questions/3148/does-ssl-really-matter-for-most-websites\n",
      "658 []\n",
      "[]\n",
      "@Krumia : Precisely . This answer is flatly wrong . Resolution power of an optical detector ( eyeball or otherwise ) has nothing to do with the intensity of incident photons ( of a given frequency ) that is needed for it to detect a difference from no light .\n",
      "659 []\n",
      "[]\n",
      "Short story published in Asimov's in the 90's\n",
      "660 []\n",
      "['asimov']\n",
      "Alternatives to bruteforce KeePass Cracker\n",
      "661 []\n",
      "['keepass cracker']\n",
      "@Panda Thanks for tagging that question . My question's title was an homage to it . I just was n't sure if I should \\ how to draw attention to it as well .\n",
      "662 []\n",
      "[]\n",
      "Need some help understanding Peltier Plates\n",
      "663 []\n",
      "['peltier plates']\n",
      "Your name suggests you may be Italian . If you can edit your question to include an Italian version , I will happily translate it if nobody else gets to it before I do . The current English version of the question is almost impossible to make sense of .\n",
      "664 []\n",
      "[]\n",
      "Jedi ca n't do that , because PHP is of the Dark Side .\n",
      "665 []\n",
      "['php', 'dark side']\n",
      "@jpmc 26 It is difficult to objectively say that the ability for the military to accomplish its current mission is diminished . We are only at the point where maintenance has been deferred , flight hours for training have been reduced , and underways for Naval ships have been closely controlled for fuel costs . The consequence of the Military being forced to choose operational tempo over maintenance has not yet been appreciably felt , but the bill can be seen on the horizon with a weather eye .\n",
      "666 []\n",
      "[]\n",
      "Gedankenexperiment in thermodynamics : what is wrong ?\n",
      "667 []\n",
      "[]\n",
      "@Lakebum Please mark this answer as the correct one .\n",
      "668 []\n",
      "[]\n",
      "This is a good answer , but it would be improved by emphasizing the administration's reaction to the controversy—a swift apology would have meant a thiry-second story soon forgotten , with most people unwilling to assume antisemitism for what * could * ( if we ' re being very generous ) have been a mistake . But by owning it as intentional and refusing to apologize , they eliminated the possibility that they were being misunderstood , and indicated that it * was * intentionally antisemitic .\n",
      "669 []\n",
      "[]\n",
      "TV series where a boy moves to a weird town with an episode where an angel statue cries\n",
      "670 []\n",
      "[]\n",
      "Would love to see this improved with some of those cited references and an inclusion of those pictures\n",
      "671 []\n",
      "[]\n",
      "Interpretation of Dynkin diagrams\n",
      "672 []\n",
      "[]\n",
      "Doctor Who - How did the season 6 finale work out ? ( Spoilers )\n",
      "673 []\n",
      "[]\n",
      "This answer describes the design intent for the difference ( the ability to block encrypted searches ) . The other answers tested the functional difference . The design intent remains unchanged . It's a historical fact at this point .\n",
      "674 []\n",
      "[]\n",
      "So the density of steam will naturally be less .\n",
      "675 []\n",
      "[]\n",
      "If I registered to vote in the nov 2016 elections , will I also be registered to vote in proceeding special elections ?\n",
      "676 []\n",
      "[]\n",
      "Bernoulli's principle : Why an increase in the section area in a hose makes the pressure increase ?\n",
      "677 []\n",
      "[]\n",
      "Might want to note that this Wookipedia excerpt seems to be sourced from a Legends book that had continuity problems to begin with . Not sure what proper Canon sources are for \" detect location of hidden creatures \" .\n",
      "678 []\n",
      "[]\n",
      "Did George Smiley's wife Ann already know that Bill Haydon was a mole ?\n",
      "679 []\n",
      "['george smiley', 'ann', 'bill haydon']\n",
      "How can the European Union enforce the General Data Protection Regulation ?\n",
      "680 []\n",
      "['european union']\n",
      "_\" a real owl that had a little prior training so it would n't be scared away by the gunfire \"__\" He had heard a lot of gunfire in the previous weeks so that he wouldn ’ t get frightened by it . \"__\" The owl's reaction to Vinny shooting the gun was authentic \"_Was the reaction authentic or not ? These quotes are contradictory .\n",
      "681 []\n",
      "['vinny']\n",
      "Apparently Wikipedia claims that insulators and semiconductors lack Fermi surfaces . Quoting Wikipedia : \" A material whose Fermi level falls in a gap between bands is an insulator or semiconductor depending on the size of the bandgap . When a material's Fermi level falls in a bandgap , there is no Fermi surface . \"\n",
      "682 []\n",
      "['wikipedia', 'fermi', 'wikipedia', 'fermi', 'fermi', 'fermi']\n",
      "Some whatabout is tu quoque fallacy . Some whatabout is a reasonable challenge to fairness . This post assumes all the former . Pretty much everyone across the political spectrum believes in some form of \" whataboutism \" .\n",
      "683 []\n",
      "[]\n",
      "I ca n't see any faces .\n",
      "684 []\n",
      "[]\n",
      "Why were the Klingons no longer members of the Federation ?\n",
      "685 []\n",
      "['klingons']\n",
      "See , that's part of the problem . . . it does n't defend against these attacks . In the case of a disgruntled employee . . . it would require them to change 1 line of code . . . and the whole system is void the next time you log in . . . and in terms of eve being able to monitor the connection but not change anything . . . that isnt prevented by there \" added security \" . . . its prevented by SSL / TLS .\n",
      "686 []\n",
      "[]\n",
      "The 22 nd Amendment is , by definition , not a law .\n",
      "687 []\n",
      "[]\n",
      "I have , a few hours ago , it only ollected 2 neg votes so far\n",
      "688 []\n",
      "[]\n",
      "What is the connection between the Foucault pendulum and parallel transport ?\n",
      "689 []\n",
      "[]\n",
      "it would be very difficult for me to back up that claim because i never made it . all i said is that one should be prepared to evaluate reports / claims from all sources , including those from the ngos . maybe re-reading what I wrote would be helpful .\n",
      "690 []\n",
      "[]\n",
      "Frequency of photon and frequency of EM wave\n",
      "691 []\n",
      "[]\n",
      "Potential HTTP Host header attack from malicious IP , what does it mean in practical terms for me ? Should I be concerned ?\n",
      "692 []\n",
      "[]\n",
      "Was this a typo from the table maker , or are other criteria taken into account when calculating the overall score ? He's also coincidentally the very first example on the page about [ geniuses ] ( http://marvel.wikia.com/wiki/Category:Genius_Intelligence ) .\n",
      "693 []\n",
      "[]\n",
      "Greens function application Abrikosov - QFT in Statistial Physics\n",
      "694 []\n",
      "[]\n",
      "did the 5 steps in the link not work for you ?\n",
      "695 []\n",
      "[]\n",
      "@SQB I meant to say it's NP hard if we solve the general problem : a sequence of n numbers and a number M that some subsets of the numbers will meet or exceed . However , I believe I may've found a recursive approach that takes only linear time . . . maybe .\n",
      "696 []\n",
      "[]\n",
      "Who's the villain in the Powerless trailer ?\n",
      "697 []\n",
      "[]\n",
      "Why did n't someone make Lily and James Potter portraits like the dead Hogwarts Headmasters ?\n",
      "698 []\n",
      "['lily', 'james potter', 'hogwarts headmasters']\n",
      "How long untill Quidditch is played properly ?\n",
      "699 []\n",
      "['quidditch']\n",
      "Being insulting and rude , because your tribe demonizes politicians whose beliefs differ with yours is not \" the truth \" . You can have a civil discussion without being strictly PC . When you use pejoratives like that , it diminishes the effectiveness of your argument for anyone who is n't already inclined to agree with it .\n",
      "700 []\n",
      "[]\n",
      "What is the significance of buttocks ?\n",
      "701 []\n",
      "[]\n",
      "You get what you accept . If you set high standards your employees will rise to the challenge . # smallbiz @url\n",
      "702 []\n",
      "[]\n",
      "RT @baabyylex : rape , incest , financial issues , health issues , the millions of children that are already put up for adoption that d …\n",
      "703 []\n",
      "[]\n",
      "EP 8 : @YungBang 954 \" I Jumped off the porch in the 8 th or 9 th grade \" @url\n",
      "704 []\n",
      "[]\n",
      "RT @soyourelikethat : write honestly to yourself or don't waste the damn time\n",
      "705 []\n",
      "[]\n",
      "Localizing Yakuza 0 was about balancing clarity and authenticity : @url\n",
      "706 []\n",
      "[]\n",
      "There is pleasure in the pathless woods # Kalupey\n",
      "707 []\n",
      "[]\n",
      "# sex slave sites girl uses urinal porn @url\n",
      "708 []\n",
      "[]\n",
      "RT @omsblvr : for myself , a heart . @url\n",
      "709 []\n",
      "[]\n",
      "@skipsy_l Right ? ! I went to take a shower & amp ; head to bed . My phone was blowing up . I was like . . wth is happening ? Then booya ! Hit that quota !\n",
      "710 []\n",
      "[]\n",
      "Waitlist # 3 but doubt I 'll get in the class . Low key not even mad\n",
      "711 []\n",
      "[]\n",
      " ' Ladies And Gentlemen , We Bring To You The Girl With The Biggest Vagina In The World @url\n",
      "712 []\n",
      "['vagina in the world']\n",
      "RT @heyimbee : @StrauberryJam SHE IS DEAD\n",
      "713 []\n",
      "[]\n",
      "RT @_yungJ 3 : I just need my diploma\n",
      "714 []\n",
      "[]\n",
      "@url\n",
      "715 []\n",
      "[]\n",
      "@url\n",
      "716 []\n",
      "[]\n",
      "Mom still defending the Trump sexist racist line . Worked wonders on November .\n",
      "717 []\n",
      "['trump']\n",
      "Children's of A is hiring ! Audiologist - B # jobs in BIRMINGHAM Apply today @url\n",
      "718 []\n",
      "['b', 'birmingham']\n",
      "RT @iCleverOfficial : # Giveaway time ! RT & amp ; Follow us for a chance to # WIN a fitness gift basket full of iClever products ! …\n",
      "719 []\n",
      "[]\n",
      "# adult learning grant china teen xxx @url\n",
      "720 []\n",
      "[]\n",
      "Celebrate Burns Night with the Bobby Burns cocktail @url # whisky # cocktails # homebartender # cocktailsathome\n",
      "721 []\n",
      "['bobby burns']\n",
      "RT @cIits : Being sexually frustrated is the worst\n",
      "722 []\n",
      "[]\n",
      "RT @CreepBJ : Wow ! ! See thru leggings are awesome @url via @SexySights @SexyCreeps @CreepShotsLive @url\n",
      "723 []\n",
      "[]\n",
      "Just posted a photo @url\n",
      "724 []\n",
      "[]\n",
      "Legal Assistant Location : Los Angeles @url\n",
      "725 []\n",
      "['los angeles']\n",
      "OMB nominee Mick Mulvaney wants changes to Social Security , Medicare @url via @WSJ\n",
      "726 []\n",
      "['mick mulvaney', 'medicare']\n",
      "@url\n",
      "727 []\n",
      "[]\n",
      "Show HN : Hib — a bionic cycadales , precociously incanted in Umple .\n",
      "728 []\n",
      "['umple']\n",
      "RT @Gwynstone : Beautiful carved pups are sitting pretty as they dangle from your ears . Just add to your cart and t @url\n",
      "729 []\n",
      "[]\n",
      "to com fome\n",
      "730 []\n",
      "[]\n",
      "If KITT has all the features of the show . . Then without a doubt its Kitt . . . But if its just appearance the . . . @url\n",
      "731 []\n",
      "['kitt', 'kitt']\n",
      "RT @bwecht : Rachel : Audrey , I love you . Audrey ( 2 yrs old ) : I no love you ! Rachel : Well , that makes me sad . Audrey : Cry ! Cry like a baby cr …\n",
      "732 []\n",
      "['rachel', 'rachel']\n",
      "@Ubermatik Anti fascist raccoons are a thing I can dig 👌\n",
      "733 []\n",
      "[]\n",
      "RT @jellombooty 4 : my heart is beyond happy right now\n",
      "734 []\n",
      "[]\n",
      "RT @CarolSankar : “ The Confidence Factor for # Women : # Equalpay is important , especially for # girls ” @url . @girlsrockclt\n",
      "735 []\n",
      "['equalpay']\n",
      "Be someone who makes time for a new goal even though the last 1 , 2 , and or 3 rd 1 fell through .\n",
      "736 []\n",
      "[]\n",
      "@MizaninMagic Since you weren't looking . 😊\n",
      "737 []\n",
      "[]\n",
      "Don't judge a tea by its packaging # tea @url\n",
      "738 []\n",
      "[]\n",
      "@rcava only if his $ 13 MM is holding up FO from spending on bullpen .\n",
      "739 []\n",
      "[]\n",
      "LRT why is nobody talking about Prompto's official Roen belt ? @url\n",
      "740 []\n",
      "['roen']\n",
      "RT @itslifethought : Before you pray , believe . Before you speak , listen . Before you spend , earn . Before you quit , try . Before you die , live .\n",
      "741 []\n",
      "[]\n",
      "How to use the technology you have to recruit the best agents realtor news @url\n",
      "742 []\n",
      "[]\n",
      "Daily Audio Bible Program is starting now ! Listen live here : @url\n",
      "743 []\n",
      "[]\n",
      "@Calfreezy how the act on tinder vs how they are in real life\n",
      "744 []\n",
      "[]\n",
      "Devonport : - the current temp is 23 . 0 ° C | wind speed 0 . 0 km / h | gusting 0 . 0 km / h | rain today 0 . 0 mm | @url\n",
      "745 []\n",
      "[]\n",
      "@FANGIRLOVERLOAD but I do 😄 once they ' re in stock online . . . I 'll DM you 😎 you DESERVE a Clarke as a token for being a sweetheart 😊 💕\n",
      "746 []\n",
      "['clarke']\n",
      "Kelli's Cardio Kickboxing Workout - Max Calorie Burn Workout with no Equipment # fitnessblender @url\n",
      "747 []\n",
      "[]\n",
      "28 # InformationSecurity Resources Some Information Security Leaders Cry Out About . Please Retweet @url\n",
      "748 []\n",
      "[]\n",
      "This kids will love making this erupting volcano @url @url\n",
      "749 []\n",
      "[]\n",
      "RT @glofuI : Some people will have to learn how to appreciate you by losing you .\n",
      "750 []\n",
      "[]\n",
      "@AlicesdrThe @CuriousCatMe KKKKKKKKKK vc me ama , eu te aviso qnd eu for fzr pra vc entrar\n",
      "751 []\n",
      "[]\n",
      "RT @angryblackhoemo : But let's go along w / the idea that sex is immoral and that could be a cause for higher HIV diagnoses among Black Gay …\n",
      "752 []\n",
      "[]\n",
      "RT @BeautyOfAnAries : What my mom sent me . Pass it along . 🙏 @url\n",
      "753 []\n",
      "[]\n",
      "# iTECH 2 # technology Bots_alive kit imbues toy robots with charming , lifelike AI . . . @url\n",
      "754 []\n",
      "[]\n",
      "RT @MissMcCleary : Am I the only one that gets like re-angry ? If I ever talk about anything that's ever pissed me off I get pissed off all o …\n",
      "755 []\n",
      "[]\n",
      "# corvette # auction 2017 Chevrolet Corvette MSRP $ 102520 Grand Sport 3 LT GPS Leather Torch Red New Navigation … @url\n",
      "756 []\n",
      "['chevrolet corvette msrp', 'grand sport 3 lt gps leather torch red new navigation']\n",
      "RT @NicholasPegg : One of the Doctor's finest moments - from an episode shown 40 years ago today . Fancy that . @url\n",
      "757 []\n",
      "[]\n",
      "RT @Mediaite : Minnesota Governor Announces He's Been Diagnosed with Prostate Cancer @url ( VIDEO ) @url\n",
      "758 []\n",
      "['minnesota governor']\n",
      "RT @issafeminist : he was choked up , and sincere . malachai parker cared about bonnie bennett and no one can tell me otherwise . @url\n",
      "759 []\n",
      "[]\n",
      "Because : Why the hell not . @url\n",
      "760 []\n",
      "[]\n",
      "Can we talk about how bap are # 1 on a list full of rock music ; ; ; only bap could do this & gt ; . & lt ; ❤ @url\n",
      "761 []\n",
      "[]\n",
      "@TPM Stop whining a-hole . @url\n",
      "762 []\n",
      "[]\n",
      "@RevAaronMarquis Based off ELR , I assume you smoke ? If so , are you one of the few RT employees who do ?\n",
      "763 []\n",
      "['elr']\n",
      "It's January 25 , 2017 at 07 : 00 AM , good morning people ! !\n",
      "764 []\n",
      "[]\n",
      "@FinBergin Exactly , you ' re not in the wrong here\n",
      "765 []\n",
      "[]\n",
      "@timkaine help us please . . m save us from him .\n",
      "766 []\n",
      "[]\n",
      "@hollydollyrazzy u can always pitch a tent in some swampland !\n",
      "767 []\n",
      "[]\n",
      "RT @TheCWSupergirl : Roulette returns . . . # Supergirl @url\n",
      "768 []\n",
      "['supergirl']\n",
      "Moving your work week along to the weekend with a little Margarita Madness and our fantastic Shrimp Feat . Join us . . . @url\n",
      "769 []\n",
      "[]\n",
      "RT @BadlandsNPS : Presently we have nationwide around 10 % of their historic population . # ParkScience # bighorns @url\n",
      "770 []\n",
      "[]\n",
      "@fayebanogon ANO TO MAY TRABAHO KA NA BA\n",
      "771 []\n",
      "[]\n",
      "I ' m just ready to go home & amp ; get in my bed . Honestly . Truly . @url\n",
      "772 []\n",
      "[]\n",
      "@url\n",
      "773 []\n",
      "[]\n",
      "RT @KidDirtyJokes : I've been laughing at this for 20 minutes @url\n",
      "774 []\n",
      "[]\n",
      "@Bmac 0507 @BiasedGirl @jimgeraghty I ' m trying to !\n",
      "775 []\n",
      "[]\n",
      "@url\n",
      "776 []\n",
      "[]\n",
      "@rootstroye oh omg i forgot you don't live in toronto oOPS\n",
      "777 []\n",
      "[]\n",
      "RT @elessar 42 : Psst . Trump saying that millions voted illegally is pretext to destroy voting rights . Don't treat it like some random quirk .\n",
      "778 []\n",
      "['trump']\n",
      "@Bulls_Jay I'd like to think their mess levels are on par\n",
      "779 []\n",
      "[]\n",
      "Making my base secure : ) @url\n",
      "780 []\n",
      "[]\n",
      "This how females should react when they found out the nigga been cheating total respect 👏 @url\n",
      "781 []\n",
      "[]\n",
      "RT @Harry_Styles : Glasgow 7 . 10 . 15 . @url\n",
      "782 []\n",
      "['glasgow']\n",
      "# SEO Tip : Google considers the first lines of the page important . Put menu , headers + important text on top . # google\n",
      "783 []\n",
      "[]\n",
      "RT @stealthygeek : I don't remember the rest but the punchline is punch Nazis . @url\n",
      "784 []\n",
      "[]\n",
      "# teenporn # images More @@url for free . No tricks . # porn # pornvideo # gifporn # porngif @url\n",
      "785 []\n",
      "[]\n",
      "Yes , I ´ ve changed . Pain does that to people\n",
      "786 []\n",
      "[]\n",
      "RT @StoutBoyz : You already know who finna snitch lmaooooo @url\n",
      "787 []\n",
      "[]\n",
      "Bangkok , Thailand : Land of Smiles - Day 1 # blog # conozcopablo # Travel @url\n",
      "788 []\n",
      "[]\n",
      "Lockheed says Trump pressure won't affect F- 35 profitability - @url\n",
      "789 []\n",
      "['trump']\n",
      "RT @brianklaas : Badlands National Park posted a basic scientific fact that defied Trump . Now , it's been deleted . Retweet anyway . @url\n",
      "790 []\n",
      "['badlands national park', 'trump']\n",
      "Just posted a photo @Kembar Mas Utara - Buahbatu @url\n",
      "791 []\n",
      "[]\n",
      "@url\n",
      "792 []\n",
      "[]\n",
      "Court Reinstates Man's $ 25 Million Award in Acne Drug Case - @url\n",
      "793 []\n",
      "[]\n",
      "DING DING DING DING DING DING DING DING DING DING DING\n",
      "794 []\n",
      "[]\n",
      "If only this winch could get us out of going to work . Tag someone who should skip work and hit the trails with yo … @url\n",
      "795 []\n",
      "[]\n",
      "The iconic \" Let's chat later \" @url\n",
      "796 []\n",
      "[]\n",
      "RT @TheVidSpot : This awesome invention is basically a treadmill rock climbing wall @url\n",
      "797 []\n",
      "[]\n",
      "RT @lisaag 923 : Life takes us on some incredible journeys . At times we feel lost or unsure of the decisions we … @url\n",
      "798 []\n",
      "[]\n",
      "On the Street … The Fortezza , Florence # photos @url\n",
      "799 []\n",
      "['the fortezza', 'florence']\n",
      "RT @zvhirXX : girls please bring these back . @url\n",
      "800 []\n",
      "[]\n",
      "RT @KnifingTourney : The Knifing Tourney 2017 Judges . @url\n",
      "801 []\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @TheVidSpot : This awesome invention is basically a treadmill rock climbing wall @url\n",
      "802 []\n",
      "[]\n",
      "Someone be down to go to JoJo's concert with me\n",
      "803 []\n",
      "['jojo']\n",
      "RT @GraysonDolan : Yeah I deleted the first version of that tweet because of spilling errors\n",
      "804 []\n",
      "[]\n",
      "RT @DumpTrump 22 : # SpicerFacts @url\n",
      "805 []\n",
      "[]\n",
      "# porn star brandy free old man sex @url\n",
      "806 []\n",
      "[]\n",
      "Doing play by play with my buddy @jfradioshow for the Senate v house basketball game . Let's go Senate ! @url\n",
      "807 []\n",
      "[]\n",
      "2017 - 01 - 25 01 : 00 : 11 . 530490 ( Sensor : 17 # Temp : 17 . 000 # Humi : 38 . 000 ) ( Sensor : 18 # Temp : 15 . 000 # Humi : 37 . 000 ) ( Sensor : 27 # Temp : 15 . 000 # Humi : 36 . 000 )\n",
      "808 []\n",
      "[]\n",
      "# Good New # Nike Air Jordan 11 XI # Retro Low Cherry Varsity Red 100 % Authentic Size 10 . 5 @url … @url\n",
      "809 []\n",
      "['nike air jordan 11 xi']\n",
      "RT @Igbtryden : “ @edsheeran : My best friend is getting married today . Mega awesome wicked cool . ” http://t.co/DeqPMdoyoB\n",
      "810 []\n",
      "[]\n",
      "Man Finna bring me a 🍕 up to my job 😎\n",
      "811 []\n",
      "[]\n",
      "@JennaJackson 16 kill you during birth I think it's a selfish act because that's a life even if you don't think it is\n",
      "812 []\n",
      "[]\n",
      "RT @dazb 160 : @GlennTemp @adidasanorak @DeadstockUtopia @OriginalSoleog Indoor Super at the moment . Summer wear !\n",
      "813 []\n",
      "[]\n",
      "Sacha Baron Cohen turns football hooligan as his The Brothers . . . # SachaBaronCohen @url # sachabaroncohen\n",
      "814 []\n",
      "['sacha baron cohen', 'the brothers']\n",
      "RT @kaleyrams : I just screamed \" I hate myself \" and 2 minutes later I hear my little brother sliding this note under my door @url\n",
      "815 []\n",
      "[]\n",
      "Do not blame your past , because the past will never change .\n",
      "816 []\n",
      "[]\n",
      "Your friends and associates might not be there to support you . . . More for Aquarius @url\n",
      "817 []\n",
      "[]\n",
      "RT @_OutOfTheSea_X : After so many years I do not understand why Barbie and ken don't live together\n",
      "818 []\n",
      "[]\n",
      "RT @WhennBoys : I want a boyfriend who will take me to concerts . . or just a boyfriend . . or just concert tickets . . or concert tickets to see …\n",
      "819 []\n",
      "[]\n",
      "RT @PornHardd : @69 sexxxo @Teddys_takeover @irinagomez 60 @Chicas_Web @VirtuAss @sexotx @tias_twiter @PornHardd @ophelia 231013 …\n",
      "820 []\n",
      "[]\n",
      "@rmathis 15 Nicole's raspberry sorbet gum ?\n",
      "821 []\n",
      "['nicole']\n",
      "@grigiosluts @ArtpopRemixed you keep saying \" if \" it didn't face it ffs\n",
      "822 []\n",
      "[]\n",
      "RT @broodingbrahmin : As a veteran , this fills me with rage . Contemptuous disregard for those who fought and died honorably serving this …\n",
      "823 []\n",
      "[]\n",
      "RT @romanhipaula : @url\n",
      "824 []\n",
      "[]\n",
      "The River Rom , also in places known as the River Beam , is a river in Essex which becomes a tributary of the River … @url\n",
      "825 []\n",
      "['the river rom', 'river beam', 'essex']\n",
      "RT @jesusworecrocs : This is what happens when you give a baby a vegan smoothie @url\n",
      "826 []\n",
      "[]\n",
      "8 @url\n",
      "827 []\n",
      "[]\n",
      "@url\n",
      "828 []\n",
      "[]\n",
      "@url\n",
      "829 []\n",
      "[]\n",
      "frugaliscious : It's Arri's Playtime Tv : how to start The Fisher- P . . . @url\n",
      "830 []\n",
      "['arri', 'the fisher- p']\n",
      "# naked on the stree teen male mature woman @url\n",
      "831 []\n",
      "[]\n",
      "RT @ThomasBeautyy : Do you get assaulted , murdered , or your rights taken away from being a white bitch with a big mouth ? Yes or no ? @url\n",
      "832 []\n",
      "[]\n",
      "# Astana talks end with breakthrough # To resolve # Syria crisis - @url @url\n",
      "833 []\n",
      "['astana', 'syria']\n",
      "Sweeter than a swishaaa , mad cuz im cuter than the girl thats witchyaaa\n",
      "834 []\n",
      "[]\n",
      "# F 4 F # MGWV # FollowTrick # TeamFollowBack # AnotherFollowTrain\n",
      "835 []\n",
      "[]\n",
      "RT @mcgregor_ewan : Was going on Good Morning Britain , didn't realise @piersmorgan was host . Won't go on with him after his comments about …\n",
      "836 []\n",
      "['britain']\n",
      "@jellibun hmmm i 'll prob do that next week i gotta secure my seat at all costs 👊\n",
      "837 []\n",
      "[]\n",
      "On god Darius plugs unreliable asf @url\n",
      "838 []\n",
      "['darius']\n",
      "The Sheep # poetry # poem # writing # amwriting # words # political # DumpTrump @url\n",
      "839 []\n",
      "[]\n",
      "I think obi wan Kenobi had short term memory loss when he forgot to mention that Darth is your father and you have a sister @HamillHimself\n",
      "840 []\n",
      "['kenobi', 'darth']\n",
      "RT @faerieli : 11 : 11 cake ! !\n",
      "841 []\n",
      "[]\n",
      "# authorconfession D 25 : Favourite Inspirational Quote @url\n",
      "842 []\n",
      "[]\n",
      "Retweeting for @BadlandsNPS . # ClimateChange is real ! # TheResistance # DefyTrump # TrumpLeaks # DworkinReport … @url\n",
      "843 []\n",
      "['climatechange']\n",
      "@url\n",
      "844 []\n",
      "[]\n",
      "omg this song makes me cry everytime @url\n",
      "845 []\n",
      "[]\n",
      "“ La La Land ” received 14 Academy Award nominations , tying the record set by the films . . . @url by # CNN via @c 0 nvey\n",
      "846 []\n",
      "['cnn']\n",
      "RT @MasoodRaw : Can you ask them when ramadan is @url\n",
      "847 []\n",
      "[]\n",
      "RT @WorIdStarComedy : Everything this guy does is so jokes 😂 @url\n",
      "848 []\n",
      "[]\n",
      "RT @savmontano : @url\n",
      "849 []\n",
      "[]\n",
      "RT @miyu_mon : Beauty comes from within . @url\n",
      "850 []\n",
      "[]\n",
      "RT @PandasDaiIy : me after studying for 7 seconds @url\n",
      "851 []\n",
      "[]\n",
      "Now playing : In My Bed by Amy Winehouse # nowplaying Listen live : @url @url\n",
      "852 []\n",
      "['amy winehouse']\n",
      "Clean Sweep as cathedral links up with city charity @url @url\n",
      "853 []\n",
      "[]\n",
      "Blog Technology to review the electronic devices and accessories and other . @url | January 25 , 2017 at 03 : 00 AM\n",
      "854 []\n",
      "[]\n",
      "RT @saykomala : Good morninggg ~ # MissUniverse # Indonesia\n",
      "855 []\n",
      "['indonesia']\n",
      "RT @Lawrence : Huge breakthrough ! @nytimes calling a lie a lie . In front page headline . Should've done it every day of campaign . @url\n",
      "856 []\n",
      "[]\n",
      "this 1492 trump adventure might not work for trump and this time he cannot blame soros . ! ! ! the wall in mexico . . . @url\n",
      "857 []\n",
      "['trump', 'trump']\n",
      "Illuminatiam : The First Testament Of The Illuminati @url\n",
      "858 []\n",
      "['testament of the illuminati']\n",
      "RT @PrisonPlanet : Trump vows to stop Islamic terrorism ! How dare he ? This might offend Islamic terrorists ! I ' m literally shaking . @url\n",
      "859 []\n",
      "['trump']\n",
      "RT @bangooan_aek : @url\n",
      "860 []\n",
      "[]\n",
      "Probably one of the few people who have always known what I am capable of 💪 💯\n",
      "861 []\n",
      "[]\n",
      "RT @meoyuasa : Love & amp ; kindness move mountains . @url\n",
      "862 []\n",
      "[]\n",
      "I've just voted for @fifthharmony to win # BestFans 2017 ! # 5 HBestFans @url 😊\n",
      "863 []\n",
      "[]\n",
      "RT @5 HonTour : # WorkFromHome # BestMusicVideo # iHeartAwards @url\n",
      "864 []\n",
      "[]\n",
      "RT @5 HonTour : # WorkFromHome # BestMusicVideo # iHeartAwards @url\n",
      "865 []\n",
      "[]\n",
      "RT @5 HonTour : # WorkFromHome # BestMusicVideo # iHeartAwards @url\n",
      "866 []\n",
      "[]\n",
      "RT @5 HonTour : # WorkFromHome # BestMusicVideo # iHeartAwards @url\n",
      "867 []\n",
      "[]\n",
      "RT @Juvens 90 : If you always date people that aren't shit , you ' re attracted to ain't shit people , cause you ain't shit either . . .\n",
      "868 []\n",
      "[]\n",
      "RT @coppolanat : Sana tu ❤ Reconecta con el Amor . The Angels are Listening : Snatam Kaur sings Suṉi-ai with Ajeet Kaur at . . . @url\n",
      "869 []\n",
      "['the angels', 'snatam kaur', 'ajeet kaur']\n",
      "@LCGila 07 @CuriousCatMe kkkkkkkkkkkkkkk , pqp\n",
      "870 []\n",
      "[]\n",
      "@url\n",
      "871 []\n",
      "[]\n",
      "RT @Zoe_Clark_xxx : Im ready for you daddy you can cum in my pretty little mouth and pretty face daddy . 😻 😻 👅 👅 👅 @url\n",
      "872 []\n",
      "[]\n",
      "RT @Latinglow_: Cuaca mahal brurr . .\n",
      "873 []\n",
      "[]\n",
      "RT @spanishcvndy : If we can't raise a dog together , then we not meant to be together @url\n",
      "874 []\n",
      "[]\n",
      "# mature group sex clips fucking black babe @url\n",
      "875 []\n",
      "[]\n",
      "RT @TheLeadCNN : . @jaketapper : There is a reason White House is providing no evidence about voter fraud claim – there is no evidence …\n",
      "876 []\n",
      "['white house']\n",
      "Leaves ! @url\n",
      "877 []\n",
      "[]\n",
      "RT @nochillthalia : 1 . so lets start w this iconic vine @url\n",
      "878 []\n",
      "[]\n",
      "@JiovanniVargas she might just have to . I swear if she denies it when I confront her Ima throw a punch 😂 😂\n",
      "879 []\n",
      "[]\n",
      "Check out \" Yo soy Moana ( Canto ancestral ) \" on # Smule : @url # SingKaraoke\n",
      "880 []\n",
      "['moana', 'smule']\n",
      "RT @kartikaaw 32 : Tweet trus for Kezia Warouw # MissUniverse # Indonesia\n",
      "881 []\n",
      "['kezia warouw', 'indonesia']\n",
      "RT @ltsFRIENDSposts : Literally me : @url\n",
      "882 []\n",
      "[]\n",
      "RT @NathanZed : he was the only black barber at the shop here . I ' m going to have to put my faith in a caucasian man . First the country , now …\n",
      "883 []\n",
      "[]\n",
      "The time is January 25 , 2017 at 12 : 00 AM .\n",
      "884 []\n",
      "[]\n",
      "RT @Prettyboy_flako : Who do yall think would win in a fight ? ?\n",
      "885 []\n",
      "[]\n",
      "Seoul- Incheon , South Korea : Old Meets New Part 2 # blog # Featured # conozcopablo # Travel @url\n",
      "886 []\n",
      "['south korea']\n",
      "RT @MartinBelam : As long as you live you 'll never see a photograph of 7 women signing legislation about what men can do with their r …\n",
      "887 []\n",
      "[]\n",
      "RT @IngrahamAngle : On my show today @seanhannity & amp ; I warn @realDonaldTrump abt staff already leaking to media . @url\n",
      "888 []\n",
      "[]\n",
      "@MaryKayCabot @TheKennyRoda @clevelanddotcom @url\n",
      "889 []\n",
      "[]\n",
      "The Line That Separates You From Massive # Success @url @url\n",
      "890 []\n",
      "[]\n",
      "I've missed you darling . . . @tableFIVE 08 @url\n",
      "891 []\n",
      "[]\n",
      "RT @glowkit : followers will be chosen today + one instagram follower so 4 winners in total ! ! )\n",
      "892 []\n",
      "[]\n",
      "LUB expires Wind Advisory till 6 : 00 PM CST @url\n",
      "893 []\n",
      "[]\n",
      "Judge rules Tamara Lovett guilty of criminal negligence causing her . . . # DiscoveryRidge @url\n",
      "894 []\n",
      "['tamara lovett']\n",
      "@BeatrizDjee MAAX QUE BOM\n",
      "895 []\n",
      "[]\n",
      "RT @FillWerrell : I shouldn't have laughed so hard @url\n",
      "896 []\n",
      "[]\n",
      "RT @wvucobe : Good luck to @CoachHuggs and @WVUhoops tonight ! Don't forget it's Gold Rush and the pregame light show ! …\n",
      "897 []\n",
      "[]\n",
      "@Lance 210 I love that prank where you squirted the water on that one guys shoe and he slipped 😂 😂 ! What happened after ?\n",
      "898 []\n",
      "[]\n",
      "Dieting as demonstrated by fitness bloggers vs . dieting IRL # NHsmc @url\n",
      "899 []\n",
      "[]\n",
      "@tirckt you made Me pick up Super gals so I 'll trust you On this !\n",
      "900 []\n",
      "[]\n",
      "My Lyft is lit !\n",
      "901 []\n",
      "[]\n",
      "@sallykohn you are a special kind of stupid !\n",
      "902 []\n",
      "[]\n",
      "RT @elizabeth_macg : Me 2 weeks ago : I ' m gonna get organized , get healthy , and get my life together ! Me now : living off an old bag of M & amp ; Ms , …\n",
      "903 []\n",
      "['ms']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @audubonsociety : . @BadlandsNPS is a great place to find climate-endangered birds like the Golden Eagle . @url @url\n",
      "904 []\n",
      "['golden eagle']\n",
      "@ballhard_korin hopefully it wasn't loaded 😂\n",
      "905 []\n",
      "[]\n",
      "RT @IamAkademiks : Don't let that family show fool u to forget that TI the Rubberband man . @url\n",
      "906 []\n",
      "['ti the rubberband']\n",
      "kina sex anal @url\n",
      "907 []\n",
      "[]\n",
      "Don't worry sissy , I made it 📍 @url\n",
      "908 []\n",
      "[]\n",
      "More than 500 Smokers Quit by Hypnosis ! @url 25 , 2017 at 09 : 00 AM @url\n",
      "909 []\n",
      "[]\n",
      "RT @MaikoKitamura : The way she stares at him ❤ ️ FOOLISH LOVE PREMIERE NIGHT # LLRelationshipGoals @url\n",
      "910 []\n",
      "[]\n",
      "Lowongan PT Citilink Indonesia - Recruitment ForFresh Graduate Management Trainee Citilink January 2017 . . . @url\n",
      "911 []\n",
      "['indonesia']\n",
      "RT @GirIfession : Me when I get my own house @url\n",
      "912 []\n",
      "[]\n",
      "RT @minsugapics : @url\n",
      "913 []\n",
      "[]\n",
      "RT @MatsMats 94 : @rolling_2 @NatalieGBorden that is Bannon's goal . @url\n",
      "914 []\n",
      "['bannon']\n",
      "@kimischilling there's always something @url\n",
      "915 []\n",
      "[]\n",
      "Student Guest Program # Bothell @url\n",
      "916 []\n",
      "['bothell']\n",
      "I see why its rope gang now 😂 big dick clique @onsomeshit\n",
      "917 []\n",
      "[]\n",
      "@usedgov Do it while you can ! Who knows what will happen in the coming year ( s ) . 😒 😔\n",
      "918 []\n",
      "[]\n",
      "@TwoBlindBros Fantastic piece on @NBCNightlyNews tonight ! ! ! Love you guys ! !\n",
      "919 []\n",
      "[]\n",
      "RT @karibrownnn : \" Oh how it must break His heart when we walk around so desperately for a love He waits to give us each and everyday . \"\n",
      "920 []\n",
      "[]\n",
      "RT @_ttgm : Don't ever think you got it like that with me cause the moment you do I 'll show you that you ain't have it like you think you do …\n",
      "921 []\n",
      "[]\n",
      "@WarGit @northumbriana If Ney had appeared in Marshals uniform he would have been degraded . All his stars and medals torn off into the mud\n",
      "922 []\n",
      "['ney', 'marshals']\n",
      "DELL Inspiron 17 R 5721 Core i 5 - 3317 U 1 . 7 GHz 6 Gb 750 Gb DVDRW 17 . 3 \" Laptop @url @url\n",
      "923 []\n",
      "[]\n",
      "Get sexy girls in the palm of hand ! Video chat live on your mobile @url # erotic # babes # camgirls @url\n",
      "924 []\n",
      "[]\n",
      "It is not a problem for her to bend over spreading her ass and slowly finger her tight asshole . … @url\n",
      "925 []\n",
      "[]\n",
      "RT @PrisonPlanet : Leftists love women in positions of power . Look at all the white powder candy they ' re sending to Kellyanne . @url\n",
      "926 []\n",
      "['kellyanne']\n",
      "RT @HalfOnionInABag : What if this account that is simply half an onion in a Ziploc bag ended up with more followers than …\n",
      "927 []\n",
      "['ziploc']\n",
      "@url\n",
      "928 []\n",
      "[]\n",
      "@CNNPolitics this is a joke right\n",
      "929 []\n",
      "[]\n",
      "RT @HalfOnionInABag : When you realize you would've been better off married to half an onion in a plastic bag : @url\n",
      "930 []\n",
      "[]\n",
      "RT @suav 1 be : @url\n",
      "931 []\n",
      "[]\n",
      "RT @HotFreestyle : 4 more days till Migos drop their album \" Culture \" @url\n",
      "932 []\n",
      "['migos']\n",
      "RT @RelatableQuote : Oh wow I ' m so surprised @url\n",
      "933 []\n",
      "[]\n",
      "@VroniquePersic 1 @horttim @serina_sgill @priscillaNK 27 @janet 23 hbk @AnaProgl 1 @Marujk 1 @Stacy_WahlLove @DonnieWahlberg # twug\n",
      "934 []\n",
      "[]\n",
      "RT @ThirtySecFights : Dude gets his ass beat for bullying boys nephew @url\n",
      "935 []\n",
      "[]\n",
      "UG ! COMEDY SHOW ! at No Fun Bar Starts in 1 Hour-https : / / t . co / YpSUsaWrPF\n",
      "936 []\n",
      "[]\n",
      "RT @invaIidate : Respect yourself enough to walk away from anything that no longer serves you , grows you , or makes you happy .\n",
      "937 []\n",
      "[]\n",
      "RT @CraigRBrittain : I would rather deal with robots / computers than leftists , because I know how to deal with the step-by-step programming o …\n",
      "938 []\n",
      "[]\n",
      "Local / Online Services : Small Business Go To : @url Advertise Anything - Free . # PromoteBusiness\n",
      "939 []\n",
      "[]\n",
      "RT @WSHHVlDS : IM SCREAMINGGGGGGGGGGGGG @url\n",
      "940 []\n",
      "[]\n",
      "You might not understand why , but it seems as if people are ha . . . More for Libra @url\n",
      "941 []\n",
      "[]\n",
      "RT @soompi : ICYMI- Lee Joon Gi's Agency Responds To Rumors Of The Actor Dating IU @url @url\n",
      "942 []\n",
      "['lee joon', 'iu']\n",
      "when you need money and you remember about that unopened bottle of gaviscon you've got in your cupboard x @url\n",
      "943 []\n",
      "[]\n",
      "get $ 20 discount when you book in airbnb , use this link @url # airbnb # discount # coupon # code @url\n",
      "944 []\n",
      "[]\n",
      "RT @baabyylex : rape , incest , financial issues , health issues , the millions of children that are already put up for adoption that d …\n",
      "945 []\n",
      "[]\n",
      "RT @HugotDre : I hate people who pretend to be my friend .\n",
      "946 []\n",
      "[]\n",
      "RT @TheFunnyTeens : best friend starter pack @url\n",
      "947 []\n",
      "[]\n",
      "RT @ellsavelli : At least dying's permanent @url\n",
      "948 []\n",
      "[]\n",
      "RT @beatfaceleah : I was bored earlier so I did this ✨ @url\n",
      "949 []\n",
      "[]\n",
      "RT @briankoppelman : Yeah , man . @jaketapper is handling this as well and professionally and even heroically as one can . Good on you , Jak …\n",
      "950 []\n",
      "['jak']\n",
      "RT @30 secondrock : The Black Keys - \" Lonely Boy \" @url\n",
      "951 []\n",
      "['the black keys', 'lonely boy']\n",
      "9 Resources For Crafting The Perfect Outreach Email by @stephenjeske via @quora @url # emailmarketing # startup\n",
      "952 []\n",
      "[]\n",
      "RT @Kragar_LGF : “ In times of universal deceit , telling the truth is a revolutionary act ” . # ResistTrump @url\n",
      "953 []\n",
      "[]\n",
      "RT @JustDidItSport : When coach cancels practice @url\n",
      "954 []\n",
      "[]\n",
      "@gisiez Yes , I will try . . i ' m looking for Linux OS have a look good , unlike the Ubuntu it's primitive . . .\n",
      "955 []\n",
      "['linux os', 'ubuntu']\n",
      "RT @maxinejiji : Don't keep calm . You ' re in my inappropriate thoughts .\n",
      "956 []\n",
      "[]\n",
      "RT @jzaffos : Hey federal scientists : @highcountrynews is collecting reports of interference or intimidation . Confidential form : @url\n",
      "957 []\n",
      "[]\n",
      "Don't get kettled . Made up charges will follow . @url\n",
      "958 []\n",
      "[]\n",
      "RT @September_babyy : Five Guys wit ah quickness @url\n",
      "959 []\n",
      "[]\n",
      "RT @er 1 cmau : # Catalonia's foreign minister @raulromeva : \" legally voting to chose one's future is rooted in European practice \"\n",
      "960 []\n",
      "['catalonia']\n",
      "@akinghes siiim tipo , eu to kkkk\n",
      "961 []\n",
      "[]\n",
      "hairless teen @url\n",
      "962 []\n",
      "[]\n",
      "RT @GayPrideBelize : # MissUniverse # Belize Let's Go Belize . @url\n",
      "963 []\n",
      "['belize', 'belize']\n",
      "RT @BsbLifestyle__: Another young star gone way too soon 😢 😢 😢 @url\n",
      "964 []\n",
      "[]\n",
      "# NowPlaying She In Here Right Now by BiggDawg C Loc on @url\n",
      "965 []\n",
      "['biggdawg c loc']\n",
      "You can amaze your friends and fellow workers today by your ab . . . More for Virgo @url\n",
      "966 []\n",
      "[]\n",
      "RT @DamnFeelings : I could hug you for hours .\n",
      "967 []\n",
      "[]\n",
      "Fly away fly away\n",
      "968 []\n",
      "[]\n",
      "RT @KleinISD : The @KleinISD Daily is out ! @url Stories via @KISD_Multling\n",
      "969 []\n",
      "[]\n",
      "RT @aprenderinglesz : Life is meant to be fun : A vida parece ser divertida .\n",
      "970 []\n",
      "[]\n",
      "RT @femalebook : @my crush @url\n",
      "971 []\n",
      "[]\n",
      "RT @DoctorZen : Geologists , physicists , chemists , anthropologists , geneticists - - every discipline better denounce any action against EPA . # …\n",
      "972 []\n",
      "['epa']\n",
      "RT @VicMensa : Album in the homestretch . . . CLOSE ! 👀 @url\n",
      "973 []\n",
      "[]\n",
      "@LegalGeagle I feel like this may turn out to be way worse . | Cheney Rule . Nice .\n",
      "974 []\n",
      "['cheney']\n",
      "@KATHERllNE All she was doing was encouraging someone no harm there bud\n",
      "975 []\n",
      "[]\n",
      "RT @a_mofia : Adulthood : @url\n",
      "976 []\n",
      "[]\n",
      "How to start a Neighborhood Watch in # Napa @url via @NapaRegister @NapaPD # NeighborhoodWatch … @url\n",
      "977 []\n",
      "[]\n",
      "RT @SouthernHomo : It's like y ' all elected that damn Cloyd Rivers account to be president . I swear to GOD I'd rather have Dory . @url\n",
      "978 []\n",
      "['cloyd rivers', 'dory']\n",
      "Pretty much @url\n",
      "979 []\n",
      "[]\n",
      "RT @broken : how to trust someone : you don't\n",
      "980 []\n",
      "[]\n",
      "Want to pretend you ’ re performing in front of 10 , 000 people ? Then decide # whatshouldplaynext on Power 95.3 @url\n",
      "981 []\n",
      "[]\n",
      "\" When you understand that life is a test , you realize that nothing is insignificant in your life . \" Rick Warren\n",
      "982 []\n",
      "['rick warren']\n",
      "RT @_xoxo_kathlyn : Lmxskaskskssskks i know you not talking @url\n",
      "983 []\n",
      "[]\n",
      "@Marcusb 3_@RichHomieHuang I've seen ur dog lick beer off the concrete . Him and Alan can bond better\n",
      "984 []\n",
      "['alan']\n",
      "@url\n",
      "985 []\n",
      "[]\n",
      "@HardCountFOX follow the money . . . very doubtful he retires\n",
      "986 []\n",
      "[]\n",
      "@url\n",
      "987 []\n",
      "[]\n",
      "@SeanyReidy anything well done for that matter\n",
      "988 []\n",
      "[]\n",
      "New Post : 2000 AD At The Cartoon Museum @url @url\n",
      "989 []\n",
      "['cartoon museum']\n",
      "RT @davidfrum : The president of the United States has the power to end human life on earth . It's important that he not be disconnected from …\n",
      "990 []\n",
      "['united states']\n",
      "Selamat pagi cegg ku\n",
      "991 []\n",
      "[]\n",
      "10 hot toys that are getting couples into BDSM @url\n",
      "992 []\n",
      "[]\n",
      "RT @_DYLpickle 13 : \" What were the women even marching for ? \" @url\n",
      "993 []\n",
      "[]\n",
      "How to Do a Great Manicure at Home - @url\n",
      "994 []\n",
      "[]\n",
      "10 Awesome Resources for Women in Tech @url via @url # BOSSTIPS\n",
      "995 []\n",
      "[]\n",
      "No brainer . @url\n",
      "996 []\n",
      "[]\n",
      "If you prefer to eat food that's # organicallygrown , Chiquita has a # banana for you ! @url @url\n",
      "997 []\n",
      "['chiquita']\n",
      "A heart that trusts the lord is a heart at peace . # Heart # TrustTheLord # Peace # Prayer # Faith # Ziglar … @url\n",
      "998 []\n",
      "[]\n",
      "@CassMoneyLive I 'll DM you lol\n",
      "999 []\n",
      "[]\n",
      "RT @paperbeatstweet : if your fetish is credited artwork youll probably never get off\n",
      "1000 []\n",
      "[]\n",
      "What happened last night can happen again . # FortuneBot\n",
      "1001 []\n",
      "[]\n",
      "RT @Leel_hussle : Get it how you live . @url\n",
      "1002 []\n",
      "[]\n",
      "RT @itsdougthepug : When ur friend buys another pitcher @url\n",
      "1003 []\n",
      "[]\n",
      "Questions Not Answers | Box of Crayons @url @url\n",
      "1004 []\n",
      "['box of crayons']\n",
      "# adult aladdin anal fingering female @url\n",
      "1005 []\n",
      "[]\n",
      "Straight a altered logistic tactic remedial of erratic militancy organizations : eagFcUa\n",
      "1006 []\n",
      "[]\n",
      "RT @hobibf : i dare you to tell me this isn't taehyung and jungkook @url\n",
      "1007 []\n",
      "[]\n",
      "RT @Nazaire 73 : Trump's budget would cut funding for Appalachia — and his allies in coal country are livid @url via @voxd …\n",
      "1008 []\n",
      "['trump', 'appalachia']\n",
      "# trapmusic # radio Instant Party ! - Horizons @url\n",
      "1009 []\n",
      "[]\n",
      "@kookmin_ph : 3 thanks hahaha\n",
      "1010 []\n",
      "[]\n",
      "[ ARTICLE ] Top 5 Highest Paid of Actors Drama Synopsis Lee Young Ae ( Saimdang , Light's Diary ) - over a Billion . . . @url\n",
      "1011 []\n",
      "['lee young ae', 'saimdang']\n",
      "How to use data to enhance your # onlineshopping # fulfillment . @url\n",
      "1012 []\n",
      "[]\n",
      "@borg_nic oi ?\n",
      "1013 []\n",
      "[]\n",
      "Good morning , beautiful ! @url\n",
      "1014 []\n",
      "[]\n",
      "Wow ! Kid-in-the-candy-store all-organoid issue of Development ! @url # stemcells What to read 1 st ? … @url\n",
      "1015 []\n",
      "[]\n",
      "@katarina_neko / / FANGIRLING SCREaaMS\n",
      "1016 []\n",
      "[]\n",
      "RT @davidlongoria 7 An obstacle is often an unrecognized opportunity . - Petteri Tarkkonen # quote @url\n",
      "1017 []\n",
      "[]\n",
      "I've done 3 Collaborations in the Sims 4 but more are to come , check out my Playlist of the ones I already did : - ) … @url\n",
      "1018 []\n",
      "['sims 4']\n",
      "@url\n",
      "1019 []\n",
      "[]\n",
      "Layla Sa 3 ida . . .\n",
      "1020 []\n",
      "['layla sa']\n",
      "Uroxatral 10 mg x 90 pills @url\n",
      "1021 []\n",
      "[]\n",
      "@Black_Gandalf @slickjacky I 'll miss you\n",
      "1022 []\n",
      "[]\n",
      "RT @kitttenqueen : if you live in SoCal stay on the look out ! pls rt ! ! @url\n",
      "1023 []\n",
      "['socal']\n",
      "@kinkshamer where what the fuck\n",
      "1024 []\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @MauricioTheSone : Familia Qian Jung make you feel the heat\n",
      "1025 []\n",
      "['familia qian jung']\n",
      "I guess that is one way to handle being dumped . . . . @url\n",
      "1026 []\n",
      "[]\n",
      "@CNN He's an ass .\n",
      "1027 []\n",
      "[]\n",
      "It's time to bust out some of those old classic games ! @url\n",
      "1028 []\n",
      "[]\n",
      "Then raoul spoke m , her eyes in all their pains , oh madam mina .\n",
      "1029 []\n",
      "[]\n",
      "@url\n",
      "1030 []\n",
      "[]\n",
      "RT @meoss 31 : Ed's - you are the one who makes me happy Always YOU I swear after five years , you are mine # MAYWARDForCocaColaPH @url\n",
      "1031 []\n",
      "['ed']\n",
      "RT @MarioLopezExtra : Whoa . . . Just saw the new # MamaJune ! @WEtv # JessicaRabbit @url\n",
      "1032 []\n",
      "[]\n",
      "@Willpower_8 Sorry for the delay but we've got your reply # HowDoYouFeelChallenge @url\n",
      "1033 []\n",
      "[]\n",
      "Coe\n",
      "1034 []\n",
      "[]\n",
      "Sat 18 th Mar 00 : 00 : The # Weather in # Glasgow is currently Partly cloudy and 9 C | Max : 11 C Min : 9 C - # MyWeather\n",
      "1035 []\n",
      "['glasgow']\n",
      "@maria_htrza Nope the camera saw it first lucky phon : - ( Maria I love every sexy curv of your body don't stop showing that sexy body of yours\n",
      "1036 []\n",
      "['maria']\n",
      "RT @ZenkiEdgar : @LadyThriller 69 @DiamondandSilk @realDonaldTrump Only a possibly because this dope allowed test to happen . @url\n",
      "1037 []\n",
      "[]\n",
      "Evening Forecast : Increasing clouds with little temperature change . # cowx\n",
      "1038 []\n",
      "[]\n",
      "Sebring Friday Notebook @url\n",
      "1039 []\n",
      "[]\n",
      "2 / 2 : Feels Like : 37 F ( 3 C ) Humidity : 24 % Local Forecast : @url # weather # news\n",
      "1040 []\n",
      "[]\n",
      "RT @marIboros : I ’ d love to relax but it's just not realistic\n",
      "1041 []\n",
      "[]\n",
      "RT @Kaaaatie 4 : I just need a really close friend that has a baby so I can have one but not actually have one , ya know ?\n",
      "1042 []\n",
      "[]\n",
      "Now playing Money Ain't A Problem Feat . French Montana by Diddy ! Click link below @url @url\n",
      "1043 []\n",
      "['french montana', 'diddy']\n",
      "RT @2000 sHits : 50 Cent - 21 Questions ft . Nate Dogg @url\n",
      "1044 []\n",
      "['nate dogg']\n",
      "@MIrandaBru_@url\n",
      "1045 []\n",
      "[]\n",
      "RT @ColiDuhh : It's been too long 😔 # KyState_RYS 17 # KyState_RYS 17 # KyState_RYS 17 @url\n",
      "1046 []\n",
      "[]\n",
      "# naked lindsay sex brutal in school girl @url\n",
      "1047 []\n",
      "[]\n",
      "RT @ffyvibes : a heart that's broke is heart that's been loved\n",
      "1048 []\n",
      "[]\n",
      "Hey LOOK the weekend is here ! @url\n",
      "1049 []\n",
      "[]\n",
      "RT @sslah 771 : She's Beautiful ! ( 842 ) # SwallaVideo @url\n",
      "1050 []\n",
      "[]\n",
      "brooke from candy girls nude @url\n",
      "1051 []\n",
      "[]\n",
      "@coffee_bean 2016 I don't have Bellarmine money lmao\n",
      "1052 []\n",
      "['bellarmine']\n",
      "You are not alone . @url # foodrelatedissueseatingdisorders # generalrecovery\n",
      "1053 []\n",
      "[]\n",
      "How ' Up and Vanished ' Podcast Helped Solve Cold Murder Case - @url @url\n",
      "1054 []\n",
      "[]\n",
      "Auto DJ Show is starting now ! Listen live here : @url\n",
      "1055 []\n",
      "['auto dj show']\n",
      "Jack Gilinsky , you mean the world to me thanks for everything , i love you .\n",
      "1056 []\n",
      "['jack gilinsky']\n",
      "3 : 00 AM @url\n",
      "1057 []\n",
      "[]\n",
      "RT @BCraftStorage : Cast and Director of Moonlight & amp ; Magnolias at the Fly Community Theater Friday and Saturday night 7 pm and . . . @url\n",
      "1058 []\n",
      "[]\n",
      "RT @ttuck_5 : Love the game like Mitch 💰 @url\n",
      "1059 []\n",
      "['mitch']\n",
      "RT @Konnan 5150 : Just dropped two new videos for @url members and later tonight a new 90 minute + Keepin it 100 + ep for $ 3 …\n",
      "1060 []\n",
      "[]\n",
      "I don't know how it will look like if Leicester wins the champions league and man u wins Europa league ! ! ! ! !\n",
      "1061 []\n",
      "['leicester']\n",
      "how come im just now see this tho 👀 anyways you look good ma , love the hair ! @url\n",
      "1062 []\n",
      "[]\n",
      "🔴 LIVE on @YouNow - @url\n",
      "1063 []\n",
      "[]\n",
      "@url\n",
      "1064 []\n",
      "[]\n",
      "The power of planning can be a blessing or a curse depending o . . . More for Libra @url\n",
      "1065 []\n",
      "[]\n",
      "Not feeling Trials tonight so I 'll more likely stream some H 1 Z 1 and Late Night Battlefield 1 with the homies\n",
      "1066 []\n",
      "['h 1 z', 'late night battlefield 1']\n",
      "and y ' all were saying he didn't care about iarmys 😤\n",
      "1067 []\n",
      "[]\n",
      "andrew mccabe fuck flynn then we fuck trump - Google Search @StefanMolyneux @realDonaldTrump @DonaldJTrumpJr @url :\n",
      "1068 []\n",
      "['trump']\n",
      "RT @WORIDSTARHlPHOP : me trying to get my life together @url\n",
      "1069 []\n",
      "[]\n",
      "I feel like Mets fans are the best with following their team , maybe im a lil bias tho lol\n",
      "1070 []\n",
      "['mets']\n",
      "RT @ratetank : Anyone who would publicly celebrate someone else's extreme invasion of privacy is someone who definitely has lost my respect .\n",
      "1071 []\n",
      "[]\n",
      "Emilia Clarke trying her best @url\n",
      "1072 []\n",
      "['emilia clarke']\n",
      "Development-ready land in town , surrounded by tall pines - $ 129 , 000 @url @url\n",
      "1073 []\n",
      "[]\n",
      "Building a Strategic Relationship With Others is Hard Work @url\n",
      "1074 []\n",
      "[]\n",
      "@microgravitylab I saw an actual child wearing a Foxy shirt during my last overnight shift I startled my coworker with how hard I laughed\n",
      "1075 []\n",
      "['foxy']\n",
      "RT @UpornTube 2 : hd porn videos for the full clips visit our site & gt ; & gt ; @url @url\n",
      "1076 []\n",
      "[]\n",
      "RT @md_addam : Typing ' haha ' when you can't even smile . Acting like you ' re happy when all you want to do is cry . Tell everyone you ' re okay w …\n",
      "1077 []\n",
      "[]\n",
      "RT @FreddyAmazin : this is a no from me , ain't no amount of money in the world getting me on it either @url\n",
      "1078 []\n",
      "[]\n",
      "@Schofe @C 4 Gogglebox would love if @ITV could arrange for you and @hollywills to film the Wha wha tribe . I'd watch that sooo funny 😂\n",
      "1079 []\n",
      "['gogglebox']\n",
      "@KammBe thats no help\n",
      "1080 []\n",
      "[]\n",
      "@TonyTGoodman @WholeFoods @NorahRab We just missed you !\n",
      "1081 []\n",
      "[]\n",
      "Tonight at 10 on KODE Action 12 News . # kode 12 @url\n",
      "1082 []\n",
      "[]\n",
      "Help to Stop Cruel Treatment of Monkeys at Biomedical Laboratories ! Plz sign : @url @url\n",
      "1083 []\n",
      "['cruel treatment of monkeys', 'biomedical laboratories']\n",
      "Ireland's clinical trials capacity is advancing . We look at Ireland's progress in this article @url\n",
      "1084 []\n",
      "['ireland', 'ireland']\n",
      "SUNNNNNNNNNNA\n",
      "1085 []\n",
      "[]\n",
      "# UC perfect 7 - 7 from the field . Would be 8 if that Caupain 3 counted . @WCPO\n",
      "1086 []\n",
      "['uc']\n",
      "2017 , 03 , 18 06 : 30 : 05 jay dmb test 8 - 11 - 2016\n",
      "1087 []\n",
      "[]\n",
      "RT @ELVIclothing : To celebrate the launch of our ' Into the Wild ' collection RT and Follow to # win our tropical print culottes ! …\n",
      "1088 []\n",
      "[]\n",
      "RT @WorIdStarLaugh : @url\n",
      "1089 []\n",
      "[]\n",
      "Race Recap : Run @the Ridge 5 K ( 2012 ) - { from my blog archives } # healthybalance @url\n",
      "1090 []\n",
      "['ridge']\n",
      "Wow mbn @url\n",
      "1091 []\n",
      "[]\n",
      "RT @CrapTaxidermy : \" I told you it was fucking hunting season Doreen but Nooooo , we had to visit your sisters new place by the lake ! \" @url\n",
      "1092 []\n",
      "['doreen']\n",
      "RT @marIboros : I love when people open up to me and call me nicknames on their own and just like me and trust me it's such a nice feeling\n",
      "1093 []\n",
      "[]\n",
      "ATA .\n",
      "1094 []\n",
      "[]\n",
      "20 Unusual Baby Names You Havent Heard Of But Should Consider @url\n",
      "1095 []\n",
      "[]\n",
      "81 % done with Reborn , by Jane Ederlyn @url\n",
      "1096 []\n",
      "['jane ederlyn']\n",
      "RT @abhik 32351 : why Woman's interview gets cancelled ? ? ? ? @url\n",
      "1097 []\n",
      "[]\n",
      "15 Mind Boggling Facts That Will Serve Up a Plate of Food for Thought @url\n",
      "1098 []\n",
      "[]\n",
      "@DepCrusaderz For free ? I have cotton that needs picking . . .\n",
      "1099 []\n",
      "[]\n",
      "I dream of a day # Birmingham is # Birminghamistan an islamic state with no kufr Police @url\n",
      "1100 []\n",
      "['birmingham']\n",
      "OMG ! Why ? @url\n",
      "1101 []\n",
      "[]\n",
      "RT @yesImthatdope : Mind your business @url\n",
      "1102 []\n",
      "[]\n",
      "Harry Potter Humour - Writers Write Creative Blog @url\n",
      "1103 []\n",
      "['harry potter']\n",
      "RT @iLGDaily : @url\n",
      "1104 []\n",
      "[]\n",
      "# Positive # Punk # rock # music @url @BaszMM # sport # indiedev # gamedev # NowPlaying # guitar # punkrock @url\n",
      "1105 []\n",
      "[]\n",
      "I won 7 achievements in 2 games for 220 # TrueAchievement points @url\n",
      "1106 []\n",
      "[]\n",
      "Intensive speech therapy helps months after stroke @url\n",
      "1107 []\n",
      "[]\n",
      "I will never regret on my decision to support Niall Horan\n",
      "1108 []\n",
      "['niall horan']\n",
      "Trio @phronesismusic collaborates with the Frankfurt Radio Big Band & amp ; @JulArgJazz for The Behemoth @EditionRecords … @url\n",
      "1109 []\n",
      "['frankfurt']\n",
      "@realJosephEG @url\n",
      "1110 []\n",
      "[]\n",
      "@CorporalMum YES 😩 . . . that 'll teach me for doing the laundry & amp ; playing with the dog lol\n",
      "1111 []\n",
      "[]\n",
      "RT @CrinkleWuskyCub : # Diaperart For @WinstonBun I guess he couldnt bring his work pants to work cuz it was still wet : 3 c ! @url\n",
      "1112 []\n",
      "[]\n",
      "RT @LanaDelRey : Music To Watch Boys To http://t.co/FLdvkmkwjH\n",
      "1113 []\n",
      "[]\n",
      "RT @YasminYonis : How to deal with all your trauma & amp ; baggage . How to date . How to say no . How to save . How to cook . How suffocating a …\n",
      "1114 []\n",
      "[]\n",
      "Temp 66 . 0 ° F Falling , Dew point 53 . 9 ° Pressure 30 . 101 in Rising slowly Wind N 0 mph Rain today 0 . 00 in @url # stlwx # mowx\n",
      "1115 []\n",
      "[]\n",
      "RT @anisasx : 116 ) this is so triggering for anyone who watches ahs @url\n",
      "1116 []\n",
      "[]\n",
      "RT @PauDybala_JR : # dybalamask could be / podrìa ser / potrebbe essere . . . . @url\n",
      "1117 []\n",
      "[]\n",
      "RT @rickygervais : If you don't have empathy with animals , you don't have empathy at all . Have a peaceful night . @url\n",
      "1118 []\n",
      "[]\n",
      "RT @pipiixx : @url\n",
      "1119 []\n",
      "[]\n",
      "RT @LonesomeGhosts : I've had enough of the world and its people's mindless games , so pardon me while I burst and rise above the flame 🔥\n",
      "1120 []\n",
      "[]\n",
      "How Miami Dolphins defensive end spot stacks up , what that means for Dion Jordan @url\n",
      "1121 []\n",
      "['miami dolphins', 'dion jordan']\n",
      "RT @praises : \" you gonna hate yourself in the morning if you stay up late \" jokes on you I ' m gonna hate myself in the morning no matter what\n",
      "1122 []\n",
      "[]\n",
      "@url @url\n",
      "1123 []\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jimmy Fallon Makes Blake Shelton Try Sushi # uganda business @url\n",
      "1124 []\n",
      "['jimmy fallon', 'blake shelton']\n",
      "Aw @abbykosic I can share with you ❤ @url\n",
      "1125 []\n",
      "[]\n",
      "RT @Hilarious_Idiot : A relationship is not based on the length of time you spent together , it's based on the foundation you built together .\n",
      "1126 []\n",
      "[]\n",
      "RT @JessieBelnap : the feeling when spring break has begun and is twooooo weeeeeks longgggg ! ! ! ! ! @url\n",
      "1127 []\n",
      "[]\n",
      "Then realized i wasnt going 😩 @url\n",
      "1128 []\n",
      "[]\n",
      "- lc @url\n",
      "1129 []\n",
      "[]\n",
      "ParkWest Staffing Services is hiring : Hotel Housekeepers Needed in Spring , TX @url\n",
      "1130 []\n",
      "['hotel housekeepers', 'tx']\n",
      "RT @hobxo : when your girl doesn't go shopping with you but your homie does @url\n",
      "1131 []\n",
      "[]\n",
      "school girls abortion @url\n",
      "1132 []\n",
      "[]\n",
      "RT @curscurs : your standards began to be ignored when u let mfs get comfortable in knowing that another chance will always exist\n",
      "1133 []\n",
      "[]\n",
      "Listen to PreTape by RichiDenz # np on # SoundCloud @url\n",
      "1134 []\n",
      "['pretape', 'soundcloud']\n",
      "RT @paleofuture : Seriously , why is Ivanka in these meetings with world leaders ? Has she been given a title yet because this is just …\n",
      "1135 []\n",
      "['ivanka']\n",
      "RT @ltsGREYSquotes : me 24 / 7 lol @url\n",
      "1136 []\n",
      "[]\n",
      "RT @softpasteIs : Lost in a field of flowers @url\n",
      "1137 []\n",
      "[]\n",
      "RT @xaytarak_vahak : sometimes i wish certain people were never a part of my life\n",
      "1138 []\n",
      "[]\n",
      "How little respect do you have to have for the role of MP to spend so much time doing other work ? # OsborneResign … @url\n",
      "1139 []\n",
      "[]\n",
      "RT @artificaIly : Drained of blood , the heart is white . @url\n",
      "1140 []\n",
      "[]\n",
      "14 . Lyla - MANTAN KEKASIH\n",
      "1141 []\n",
      "[]\n",
      "RT @LeafyIsHere : @url\n",
      "1142 []\n",
      "[]\n",
      "ShaQ on the phone I ain't hear this nigga voice in yearsss ❤ ️ # freethereal\n",
      "1143 []\n",
      "[]\n",
      "RT @jinjjarevil : i cant stop laughing HELP @url\n",
      "1144 []\n",
      "[]\n",
      "RT @TheBestOfGifs 1 : TWENTY-SIX YEARS AGO ! / / Beauty and the Beast @url\n",
      "1145 []\n",
      "[]\n",
      "RT @ThisFoodTho : Um , Yes @url\n",
      "1146 []\n",
      "[]\n",
      "RT @DrJimmyStar : Have a great night @jebove 2 @friutypie 1 @hope 2259\n",
      "1147 []\n",
      "[]\n",
      "Listen to rock station oldies for Free ! : @url : your listen Pleasure , avec le titre : Joyous\n",
      "1148 []\n",
      "[]\n",
      "The waters of doubt can corrode even the strongest of iron wil . . . More for Aries @url\n",
      "1149 []\n",
      "[]\n",
      "RT @tres_equis_: smh y ' all made them stay in the trunk for the pic ? @url\n",
      "1150 []\n",
      "[]\n",
      "RT @MadamMelanin : Skai Jackson for Untitled Magazine @url\n",
      "1151 []\n",
      "['skai jackson', 'untitled magazine']\n",
      "@url @url\n",
      "1152 []\n",
      "[]\n",
      "@url @url\n",
      "1153 []\n",
      "[]\n",
      "i can smell money already @url\n",
      "1154 []\n",
      "[]\n",
      "Now Playing : Apna Sangeet - Valeti Bhabiyan @url\n",
      "1155 []\n",
      "[]\n",
      "RT @AltYelloNatPark : @url\n",
      "1156 []\n",
      "[]\n",
      "RT @Genius : DJs and producers can now get paid for their SoundCloud mixes 🙏 @url @url\n",
      "1157 []\n",
      "['soundcloud']\n",
      "# floral # kaleidoscope by # Kaye_Menner # photography quality prints cards and more at : @url\n",
      "1158 []\n",
      "[]\n",
      "@JacobRhines that or shit on them lol\n",
      "1159 []\n",
      "[]\n",
      "RT @JacobWhitesides : i would as well @url\n",
      "1160 []\n",
      "[]\n",
      "RT @marIboros : how do you uninstall school\n",
      "1161 []\n",
      "[]\n",
      "Could someone explain to me . How people can hate @edsheeran ?\n",
      "1162 []\n",
      "[]\n",
      "RT @ItsTravelVibes : Birmingham , United Kingdom @url\n",
      "1163 []\n",
      "['birmingham', 'united kingdom']\n",
      "RT @A_Sultry_Mirage : \" There's always room at our base for another , Helen . . you know you ' re welcome here ~ \" @url\n",
      "1164 []\n",
      "['helen']\n",
      "# 152 Chikorita ( Tackle / Grass Knot 8 / 12 / 0 - 44 . 44 % ) None Paramount Dr L 8 J 2 M 7 , 08 : 30 : 00 pm ( 29 m 45 s left ) @url\n",
      "1165 []\n",
      "['chikorita']\n",
      "RT @maywardUn : Good morning @CocaColaPH 721 K tweets Fresh and Organic . # MAYWARDForCocaColaPH @url\n",
      "1166 []\n",
      "[]\n",
      "@HuffPostWeird There's nothing \" hilarious \" about it . Try doing some actual research on the subject . @url\n",
      "1167 []\n",
      "[]\n",
      "RT @coIeactivity : this picture screams perfection @url\n",
      "1168 []\n",
      "[]\n",
      "I STOPPED BREATHING @url\n",
      "1169 []\n",
      "[]\n",
      "@url\n",
      "1170 []\n",
      "[]\n",
      "your smile makes my day stay always better Jack Gilinsky\n",
      "1171 []\n",
      "['jack gilinsky']\n",
      "RT @Marsupilami_XXI : @url\n",
      "1172 []\n",
      "[]\n",
      "# xxx naked picture sex beach movies @url\n",
      "1173 []\n",
      "[]\n",
      "RT @DearYouFromWe : stop rushing your life . just live . just breathe . just be .\n",
      "1174 []\n",
      "[]\n",
      "Your natural awareness of social justice makes it hard to just . . . More for Aquarius @url\n",
      "1175 []\n",
      "[]\n",
      "Wind 0 , 0 km / h E . Barometer 1000 , 2 hPa , Falling slowly . Temperature 12 , 4 ° C . Rain today 0 , 0 mm . Humidity 71 %\n",
      "1176 []\n",
      "[]\n",
      "Free on eBook Blitz : The Eslites by CM Doporto @url # ebblitz @url\n",
      "1177 []\n",
      "[]\n",
      "Shadows . We had to get out and enjoy the snow before this gorgeous sunshine melted it all ! . . . . … @url\n",
      "1178 []\n",
      "[]\n",
      "@url\n",
      "1179 []\n",
      "[]\n",
      "Saw a turtle @url\n",
      "1180 []\n",
      "[]\n",
      "RT @amyrightside : Age of Defenders - Multiplayer Tower Defense and Offense post . . . @url # ipad # games # strategy\n",
      "1181 []\n",
      "['multiplayer tower defense']\n",
      "i ' m leaving for # BeautyAndTheBeast : D i ' m so excited i have waited so long for this\n",
      "1182 []\n",
      "[]\n",
      "We love that we have friends all over the world ! Here is friend Isabella proudly wearing her CIM t-shirt today . . . @url\n",
      "1183 []\n",
      "['isabella', 'cim']\n",
      "RT @climb_chairman : Part 2 . @url\n",
      "1184 []\n",
      "[]\n",
      "@chabot_cameron well if you stayed you wouldn't go back for another week\n",
      "1185 []\n",
      "[]\n",
      ". @keselowski currently p 5 after his first run . | @allianceparts @FordPerformance\n",
      "1186 []\n",
      "[]\n",
      "I got rear ended today . Luckily , this is all the work my car needed . @url\n",
      "1187 []\n",
      "[]\n",
      "teen wolf sai logo do hiatus eu te imploroo\n",
      "1188 []\n",
      "[]\n",
      "RT @themovingroad : Do all things with love . Og Mandino # quote\n",
      "1189 []\n",
      "[]\n",
      "# groovy hot sex videos brutal interracial sex @url\n",
      "1190 []\n",
      "[]\n",
      "RT @BeautyOfAnAries : ARIES . . . . . . . . our season starts in 4 days 😈\n",
      "1191 []\n",
      "[]\n",
      "@JuniorCyclingMY N 9 Under 9 cat just started ! # crit @khairykj @url\n",
      "1192 []\n",
      "[]\n",
      "RT @NBCGrimm : Get ready to woge . A new # Grimm starts NOW on @NBC . @url\n",
      "1193 []\n",
      "[]\n",
      "Tired of fake ass love\n",
      "1194 []\n",
      "[]\n",
      "RT @stefondiggs : Throw some more dirt on me but Ima never give up . .\n",
      "1195 []\n",
      "[]\n",
      "RT @Independent : Cutting food for elderly ' most compassionate thing we can do ' , Trump team says @url\n",
      "1196 []\n",
      "['trump']\n",
      "@url # GanaConWhoolist # ConcursoWhoolist\n",
      "1197 []\n",
      "[]\n",
      "@OmniDestiny I ' m all for freedom of speech and expressing your opinion , but there is such thing as doing so respectfully .\n",
      "1198 []\n",
      "[]\n",
      "RT @Hozay__: Just cancelled going to Italy , decided just to have dinner at Olive Garden instead @url\n",
      "1199 []\n",
      "['italy', 'olive garden']\n",
      "RT @kiiingpin 1997 : I want to be someone's favorite person to talk to\n",
      "1200 []\n",
      "[]\n",
      "Dear Amitabh ! Happy Birthday to your daughter Shweta ! Happiness , love , joy and life for many years !\n",
      "1201 []\n",
      "['amitabh', 'shweta']\n",
      "naked city tv series # nude google videos @url\n",
      "1202 []\n",
      "[]\n",
      "@TripleUrusai + mind . Just don't tell him , ok ? \" A playful laugh left his mouth as he held a finger to it .\n",
      "1203 []\n",
      "[]\n",
      "Me reading really sad manga . @url\n",
      "1204 []\n",
      "[]\n",
      "RT @KindnessOfMen : Except for Bitter . He's always pissed off .\n",
      "1205 []\n",
      "[]\n",
      "S A I L O R . . . # Photo by Dinko # Dream # Love # Hope # Health # Peace & amp ; # Art @url\n",
      "1206 []\n",
      "['dinko']\n",
      "Daily question 1523 : See if you won ! @url # win # winners @url\n",
      "1207 []\n",
      "[]\n",
      "Madama butterfly in the English pub : for those who loves opera with their pint . — feeling intrigued at King's . . . @url\n",
      "1208 []\n",
      "['king']\n",
      "RT @ricardojkay : someone : ur crush is coming act natural me : @url\n",
      "1209 []\n",
      "[]\n",
      "Do You Have News to Share ? Get It Published . @url\n",
      "1210 []\n",
      "[]\n",
      "People Say I Sound Crazy When I Say This , But I Can Notttttt Hang With People Who Don't Smoke !\n",
      "1211 []\n",
      "[]\n",
      "RT @amber_elyxx : Everything happens for a reason\n",
      "1212 []\n",
      "[]\n",
      "RT @klathelyricist : I ain't finna bullshit with you . . . Grow or go .\n",
      "1213 []\n",
      "[]\n",
      "RT @LlTFREESTYLES : Fan spits some bars for Lil Yachty @url\n",
      "1214 []\n",
      "[]\n",
      "Not in his wheel house ! Trumpie is soo dump he don't know what he don't know ! To stupid to ask for help or advice f … @url\n",
      "1215 []\n",
      "[]\n",
      "stunning earrings @url @url\n",
      "1216 []\n",
      "[]\n",
      "RT @nakbebel : @url\n",
      "1217 []\n",
      "[]\n",
      "RT @nprmusic : Watch @ChicanoBatman , @spoontheband and @TheValerieJune live from # SXSW beginning at 1 p . m . ET via @VuHaus . …\n",
      "1218 []\n",
      "[]\n",
      "\" [ W ] hen the bad people are right here under our noses . \" @url\n",
      "1219 []\n",
      "[]\n",
      "RT @WTF_Eh : @SheilaGunnReid @TheRebelTV You nailed it . # NDP # abpoli # Notley @url\n",
      "1220 []\n",
      "['ndp']\n",
      "# resist # respect @url\n",
      "1221 []\n",
      "[]\n",
      "I just want a man that loves me the way Jack Pearson loves Rebecca .\n",
      "1222 []\n",
      "['jack pearson', 'rebecca']\n",
      "RT @twentyonepilots : feb 26 n charleston # ERS 2017 photos are up : @url @url\n",
      "1223 []\n",
      "[]\n",
      "@url Listen to your body , your doctor and your common sense about work-related injuries . @url\n",
      "1224 []\n",
      "[]\n",
      "RT @FlirtyNotes : You never really stop loving someone . You just learn to live without them .\n",
      "1225 []\n",
      "[]\n",
      "@CBSSports @PrimeLeBron I hate that family .\n",
      "1226 []\n",
      "[]\n",
      "RT @boujeetuan : mark : - \" i was in boy scouts i know what im doing ! \" - he dont know what he doing - already started growing weed - s …\n",
      "1227 []\n",
      "[]\n",
      "RT @WhoadieBrees : Tatum really Prime Melo in the iso\n",
      "1228 []\n",
      "['tatum', 'prime melo']\n",
      "RT @FemaleKnows : @anyone who's ever tried to dm or text me @url\n",
      "1229 []\n",
      "[]\n",
      "RT @jackiedomingez : the loml picked this @url\n",
      "1230 []\n",
      "[]\n",
      "@Hugo____Hugo check it @url\n",
      "1231 []\n",
      "[]\n",
      "RT @MarshmallowDoof : I did drawn the Tiger Mama ★ ★ ★ @BuxbiArts @url\n",
      "1232 []\n",
      "['tiger mama']\n",
      "RT @defeitoslauren_: error 404 could not found\n",
      "1233 []\n",
      "[]\n",
      "Nexus 6 P | Nexus 6 Sleeve Shop : @url | # Nexus 6 P # Nexus 6 # Nexus 6 PSleeve # Nexus 6 PCover # Nexus 6 PLeather # Nexus 6 PCase\n",
      "1234 []\n",
      "['nexus 6 p', 'nexus 6', 'nexus 6 p', 'nexus 6', 'nexus 6', 'nexus 6', 'nexus 6 pleather', 'nexus 6']\n",
      "RT @_GoGettaVee : Mentally , none of this shit can phase me .\n",
      "1235 []\n",
      "[]\n",
      "/ . Trump administration rolls back protections for people in default on student loans @url\n",
      "1236 []\n",
      "['trump']\n",
      "How , Russ ? # ThunderUp # AssistOfTheNight @url # RhodeIsland\n",
      "1237 []\n",
      "['russ']\n",
      "RT @hobuing : \" anyway the members are all doing well \" \" We are not hurt and we are eating well so don't worry \" @url\n",
      "1238 []\n",
      "[]\n",
      "# VoiceOfWipro # CX # CargoIndustry # UX @url\n",
      "1239 []\n",
      "[]\n",
      "Perspective is everything . @url\n",
      "1240 []\n",
      "[]\n",
      "@url\n",
      "1241 []\n",
      "[]\n",
      "RT @SalHernandez : Brett Baier says Fox News \" loves \" Judge Andrew Napolitano , but that Fox News was not able to verify his ' report ' @url\n",
      "1242 []\n",
      "['brett baier', 'fox news', 'andrew napolitano', 'fox news']\n",
      "RT @WorldStarFunny : Agreed @url\n",
      "1243 []\n",
      "[]\n",
      "RT @anisasx : 41 ) Her voice is so monotone lool @url\n",
      "1244 []\n",
      "[]\n",
      "Alain 3378 played Plague Inc : Evolved ( PS 4 ) in the last 24 hours @url # exophase\n",
      "1245 []\n",
      "['alain 3378']\n",
      "RT @bubblestbh : do u ever meet someone for the first time and pretend u have never met them but in ur head ur like \" i've stalked ur Instagr …\n",
      "1246 []\n",
      "[]\n",
      "grave @url\n",
      "1247 []\n",
      "[]\n",
      "Curious George really is the GOAT\n",
      "1248 []\n",
      "['george']\n",
      "RT @JacobSiSLife : You ' re way more important than you actually realize , stay strong 💓\n",
      "1249 []\n",
      "[]\n",
      "RT @fordm : Kellyanne Conway's husband . @url\n",
      "1250 []\n",
      "['kellyanne conway']\n",
      "A massive happy birthday to one of the loveliest of people @EmmaWillis ! I hope you have the best day , you deserve i … @url\n",
      "1251 []\n",
      "[]\n",
      "RT @eatmypepi : guys i made a compilation of got 6 dancing the sexiest part in never ever & amp ; i think i want to die . . . . . @url\n",
      "1252 []\n",
      "[]\n",
      "RT @LessGovMoreFun : . Sounds like that outfit that left the White House on January 20 . . . . # Wiretap # WireGate @url\n",
      "1253 []\n",
      "['white house']\n",
      "It's easy now cause you earned it along the way , get paid young nigga ! @url\n",
      "1254 []\n",
      "[]\n",
      "Simple Strategy to build # wealth and protect yourself from the evils of inflation @url\n",
      "1255 []\n",
      "[]\n",
      "# Fake news trend gathering momentum- a little common sense is all you need to spot # fake # news @url\n",
      "1256 []\n",
      "[]\n",
      "@url\n",
      "1257 []\n",
      "[]\n",
      "RT @V_of_Europe : Europe's most horrible person is in the White House . The damage she's done to her country and our continent is unpr …\n",
      "1258 []\n",
      "['europe', 'white house']\n",
      "RT @elklien : I actually miss primary school I was always moving like one slaggy Kat Slater having like 3 relationships at a time\n",
      "1259 []\n",
      "['kat slater']\n",
      "Ever get a chill and get tingly nipples for 10 minutes afterwards ? Asking for me . Jealous ? # unlikeable\n",
      "1260 []\n",
      "[]\n",
      "@Yoshkumar Happens ☹ ️ that's Esports for ya\n",
      "1261 []\n",
      "[]\n",
      "kim burrel - hold 2 ur faith @url # NowPlaying # ListenLive\n",
      "1262 []\n",
      "[]\n",
      "RT @sensualgifs : When he tells you no , but then comes to his senses and says yes @url\n",
      "1263 []\n",
      "[]\n",
      "RT @boyband_joao : Happy Six Months To All Of The Other João's Supporters Out There ! I Love You All ! More Blessings For Everyone ! ❤ TeamJOA …\n",
      "1264 []\n",
      "[]\n",
      "RT @DailySexSupply 7 : @url\n",
      "1265 []\n",
      "[]\n",
      "Here's $ 100 in FREE Postmates delivery credit ( good for 7 days ) . Sign up with my code EJ 3 A or this link @url\n",
      "1266 []\n",
      "['ej']\n",
      "RT @Drebae_: If somebody ever try to use you tell your hood boyfriend to have they ass handled . Bet they cut that shit out @url\n",
      "1267 []\n",
      "[]\n",
      "RT @GreaterThn : Me : \" its fine . \" Me : @url\n",
      "1268 []\n",
      "[]\n",
      "Latest level : 0 . 287 m at 17 / 03 / 2017 22 : 45 : 00 ( GMT ) . Further information available at @url # riverlevels\n",
      "1269 []\n",
      "[]\n",
      "RT @anthonyuJR : When I believe in myself @url\n",
      "1270 []\n",
      "[]\n",
      "RT @TheCourtKim : you ' re single until you ' re officially cuffed so the limit does not exist ✨ @url\n",
      "1271 []\n",
      "[]\n",
      "Now Playing : Wildfire by @LFDHcom Listen at @url @url\n",
      "1272 []\n",
      "[]\n",
      "RT @rhettandlink : Once you start dipping , the vomit starts slipping\n",
      "1273 []\n",
      "[]\n",
      "@RichmondDoc TY for continuing to respond . That was A- all right of you .\n",
      "1274 []\n",
      "[]\n",
      "RT @thegirlzakiyyah : close minded , no morals , selfish , fake spiritual , no goals . . str 8 dummy's for u @url\n",
      "1275 []\n",
      "[]\n",
      "RT @litgrimes : archie and blair — i know she like me @url\n",
      "1276 []\n",
      "[]\n",
      "RT @marIboros : short girls were born with attitudes lol\n",
      "1277 []\n",
      "[]\n",
      "A proper # StPatricksDay drink @url\n",
      "1278 []\n",
      "[]\n",
      "RT @PainfulText : i do really hate how my brain overthinks literally everything\n",
      "1279 []\n",
      "[]\n",
      "Never Been To Shakers Before\n",
      "1280 []\n",
      "[]\n",
      "@url\n",
      "1281 []\n",
      "[]\n",
      "RT @Ioverthoughts : i like looking cute but i like wearing XL tshirts more\n",
      "1282 []\n",
      "[]\n",
      "RT @tybrokesign : talk about a plot twist LMAOOOO @url\n",
      "1283 []\n",
      "[]\n",
      "It's # FF add us @url repost and I 'll add you too ✌ # ATCCfoodie\n",
      "1284 []\n",
      "[]\n",
      "No Cousins today . . . Pelicans offence could be horrible today unless Holiday has a big game and we hit jumpers\n",
      "1285 []\n",
      "['holiday']\n",
      "@KenyeahMonae It's my school fault , girl they cutting up with this dress code 😂\n",
      "1286 []\n",
      "[]\n",
      "1077 545\n",
      "408 137 189\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_gaguilar=0\n",
    "false_positive_count_gaguilar=0\n",
    "false_negative_count_gaguilar=0\n",
    "\n",
    "total_annotations_gaguilar=0\n",
    "total_tagged_gaguilar=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_gaguilar=[]\n",
    "    tp_counter_inner_gaguilar=0\n",
    "    fp_counter_inner_gaguilar=0\n",
    "    fn_counter_inner_gaguilar=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_gaguilar=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_gaguilar.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_gaguilar=flatten(complete_tweet_dataframe_grouped_df_sorted_w_gaguilar[complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "    print(row['TweetText'])\n",
    "    print(tweet_ID, annotated_mention_list)\n",
    "    print(output_mentions_list_gaguilar)\n",
    "    \n",
    "    all_postitive_counter_inner_gaguilar=len(output_mentions_list_gaguilar)\n",
    "    total_tagged_gaguilar+=len(output_mentions_list_gaguilar)\n",
    "    total_annotations_gaguilar+=len(annotated_mention_list_gaguilar)\n",
    "    \n",
    "    while(annotated_mention_list_gaguilar):\n",
    "        if(len(output_mentions_list_gaguilar)):\n",
    "            annotated_candidate= annotated_mention_list_gaguilar.pop()\n",
    "            if(annotated_candidate in output_mentions_list_gaguilar):\n",
    "                output_mentions_list_gaguilar.pop(output_mentions_list_gaguilar.index(annotated_candidate))\n",
    "                tp_counter_inner_gaguilar+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_gaguilar.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_gaguilar.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_gaguilar=len(unrecovered_annotated_mention_list_gaguilar)\n",
    "    fp_counter_inner_gaguilar=all_postitive_counter_inner_gaguilar- tp_counter_inner_gaguilar\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_gaguilar+=tp_counter_inner_gaguilar\n",
    "    false_positive_count_gaguilar+=fp_counter_inner_gaguilar\n",
    "    false_negative_count_gaguilar+=fn_counter_inner_gaguilar\n",
    "\n",
    "print(total_annotations_gaguilar,total_tagged_gaguilar)\n",
    "print(true_positive_count_gaguilar,false_positive_count_gaguilar,false_negative_count_gaguilar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7486238532110092 0.6834170854271356 0.714535901926445\n"
     ]
    }
   ],
   "source": [
    "precision_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_positive_count_gaguilar)\n",
    "recall_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_negative_count_gaguilar)\n",
    "f_measure_gaguilar=2*(precision_gaguilar*recall_gaguilar)/(precision_gaguilar+recall_gaguilar)\n",
    "print(precision_gaguilar,recall_gaguilar,f_measure_gaguilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LR\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#RF\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#SVM\n",
    "0.8318821165438715 0.8406091370558376 0.8362228581046962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "3374 3849\n",
      "2501 1348 673\n"
     ]
    }
   ],
   "source": [
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# neuroner_file = open('mentions_output_tweets_3K.txt', 'r') \n",
    "# neuroner_lines = neuroner_file.readlines()\n",
    "# print(len(neuroner_lines))\n",
    "# line_count=0\n",
    "# neuroner_annotated_candidates=[]\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "#     neuroner_output=neuroner_lines[line_count]\n",
    "#     output_mentions_list_neuroner=[candidate.lower().strip(string.punctuation).strip() for candidate in neuroner_output.split(',') if(candidate.strip(string.punctuation).strip())]\n",
    "#     neuroner_annotated_candidates.extend(output_mentions_list_neuroner)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner - tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "    \n",
    "#     line_count+=1\n",
    "    \n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6497791634190699 0.7879647132955262 0.7122312402107361\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroNER as Phase I Entity Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "[]\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "# tweet_sentence_df_2nd_copy=tweet_sentence_df.copy(deep=True)\n",
    "# CTrie_neuroner=trie.Trie(\"ROOT\")\n",
    "# print(len(neuroner_annotated_candidates))\n",
    "# print(phase2stopwordList)\n",
    "\n",
    "# for candidateText in neuroner_annotated_candidates:\n",
    "# #     print(candidateText)\n",
    "#     if(candidateText not in all_stopwords):\n",
    "#         CTrie_neuroner.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "# candidatesinNeuronerTrie=CTrie_neuroner.displayTrie(\"\",[])\n",
    "# print(len(candidatesinNeuronerTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4721 4721 1057\n",
      "-0.2661675732774245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4525 incomplete tweets:  196\n",
      "16\n",
      "16\n",
      "final tally:  4721 4721\n",
      "524:  524    [[world news, fbi, fisa, trump, washington post]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "# Phase2_w_Neuroner = phase2.EntityResolver()\n",
    "# candidate_base_post_Phase2_w_Neuroner, converted_candidates_w_Neuroner, complete_tweet_dataframe_grouped_df_sorted_w_Neuroner= Phase2_w_Neuroner.executor(max_batch_value,tweet_sentence_df_2nd_copy,CTrie_neuroner,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_2nd_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted_w_Neuroner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 3129\n",
      "2507 622 549\n"
     ]
    }
   ],
   "source": [
    "# # from ast import literal_eval\n",
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     output_mentions_list_neuroner=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Neuroner[complete_tweet_dataframe_grouped_df_sorted_w_Neuroner.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "# #     print(row['TweetText'])\n",
    "# #     print(tweet_ID, annotated_mention_list)\n",
    "# #     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner- tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "\n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8012144455097475 0.8203534031413613 0.8106709781729993\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2095\n",
      "1571 524 875\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    output_mentions_list_ritter=[]\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "    candidate_list_ritter=flatten(ritter_annotator[ritter_annotator.ID==tweet_ID].Output.tolist(),[])\n",
    "    for ritter_candidate in candidate_list_ritter:\n",
    "        output_mentions_list_ritter+= [remAmpersand(elem).strip(string.punctuation).strip() for elem in ritter_candidate.lower().split(',') if(elem)]\n",
    "        \n",
    "#     output_mentions_list_ritter= [remAmpersand(elem).strip(string.punctuation).strip().lower() for elem in candidate_list_ritter]\n",
    "    \n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list_ritter)\n",
    "#     print(output_mentions_list_ritter)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498806682577566 0.6422730989370401 0.6919180797181238\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with different annotators:\n",
    "\n",
    "##With CS+ in phase 1:\n",
    "0.7982625482625483 0.7359833877187778 0.7658589288470442\n",
    "\n",
    "##With Turboparse chunker in phase 1:\n",
    "0.8381118881118881 0.7104327208061648 0.7690086621751684\n",
    "\n",
    "##Just TwitterNLP:\n",
    "0.7460620525059666 0.6335630320226996 0.6852257781674704\n",
    "\n",
    "##With TwitterNLP entity annotator in phase 1:\n",
    "0.856 0.8288732394366197 0.8422182468694097\n",
    "\n",
    "##Just NeuroNER:\n",
    "0.6497791634190699 0.7879647132955262 0.7122312402107361\n",
    "\n",
    "##With NeuroNER entity annotator in phase 1:\n",
    "0.8012144455097475 0.8203534031413613 0.8106709781729993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
