{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats\n",
    "\n",
    "# import phase2_Trie_baseline_reintroduction_effectiveness as phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=word_tokenize(\"Very well explained take on Carter/ Russia/ FISA/ Trump's sitch.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "        elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/venezuela.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "#tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/roevwade.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/wnut17test.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "# print(len(tweets_unpartitoned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105\n"
     ]
    }
   ],
   "source": [
    "# tweets_1=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_2=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_3=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned= pd.concat([tweets_1,tweets_2,tweets_3])\n",
    "print(len(tweets_unpartitoned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with TurboParser NP Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1=time.time()\n",
    "df_holder=[]\n",
    "batch_number=0\n",
    "# tweetList=[]\n",
    "sentenceList=[]\n",
    "sentID=0\n",
    "sentID_to_tweet_ID={}\n",
    "mentionList=[]\n",
    "\n",
    "for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "    index=row.Index\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    hashtags=str(row.HashTags)\n",
    "\n",
    "    user=str(row.User)\n",
    "    tweetText=str(row.TweetText)\n",
    "    annot_raw=\"\"\n",
    "    stanford_candidates=\"\"\n",
    "    ritter_candidates = \"\"\n",
    "    calai_candidates=\"\"\n",
    "\n",
    "    ne_List_final=[]\n",
    "    userMention_List_final=[]\n",
    "    tweetSentenceList=splitSentence(tweetText)\n",
    "    sentenceList.extend(tweetSentenceList)\n",
    "    \n",
    "    for sentence in tweetSentenceList:\n",
    "        sentID_to_tweet_ID[sentID]=int(index)\n",
    "        sentID+=1\n",
    "    \n",
    "    mentions=[]\n",
    "    \n",
    "#     if(len(tweetSentenceList)!=len(str(row.mentions_other_BIO).split(';'))):\n",
    "#         print('index: ',index)\n",
    "    \n",
    "    for sentence_level in str(row.mentions_other).split(';'):\n",
    "        if(sentence_level):\n",
    "            for mention in sentence_level.split(','):\n",
    "                if(mention):\n",
    "                    mentions.append(mention.strip())\n",
    "    \n",
    "#     if(len(tweetSentenceList)!= len(mentions)):\n",
    "#         print('tally: ',len(tweetSentenceList), len(mentions))\n",
    "#         print(tweetSentenceList)\n",
    "#         print(row.mentions_other)\n",
    "#     print(mentions)\n",
    "\n",
    "    mentionList.append(mentions)\n",
    "#     tweetList.append(tweetText)\n",
    "    \n",
    "    for sen_index in range(len(tweetSentenceList)):\n",
    "        sentence=tweetSentenceList[sen_index]\n",
    "        annotation=[]\n",
    "        tweetWordList=getWords(sentence)\n",
    "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
    "#         phase1Candidates\n",
    "        dict1 = {'tweetID':str(index), 'sentID':str(sen_index), 'hashtags':hashtags, 'user':user, 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList, 'start_time':now,'entry_batch':batch_number,'annotation':annotation,'stanford_candidates':stanford_candidates,'ritter_candidates':ritter_candidates,'calai_candidates':calai_candidates}\n",
    "        df_holder.append(dict1)\n",
    "\n",
    "#     for candidate in ne_List_final:\n",
    "#         #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
    "#         candidateText=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
    "#         candidateText=(candidateText.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "#         candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#         # if(index==9423):\n",
    "#         #     print(candidateText)\n",
    "#         combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
    "#         if not ((candidateText in combined)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
    "#             self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),candidate.features,batch_number)\n",
    "\n",
    "#     NE_list_phase1+=ne_List_final\n",
    "\n",
    "#     UserMention_list+=userMention_List_final\n",
    "\n",
    "tweet_sentence_df= pd.DataFrame(df_holder,columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','tweetwordList', 'start_time','entry_batch','annotation','stanford_candidates','ritter_candidates','calai_candidates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = ['Guangdong Public University of Foreign Studies is located in Guangzhou.',\n",
    "#              'Lucy is in Kolkata with diamonds.','Bernie Sanders says his fight is for the working class.','elizabeth warren chaired the CBFC',\n",
    "#              'coronavirus is scary!','U.S. is struggling'\n",
    "#             ]\n",
    "\n",
    "# print(len(mentionList),len(tweetList),len(sentID_to_tweet_ID.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conll=[]\n",
    "try:\n",
    "# #     result_stanford = tweebo_api.parse_stanford(text_data)\n",
    "# #     result_conll = tweebo_api.parse_conll(text_data)\n",
    "    result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[:1000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[1000:2000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[2000:3000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[3000:])\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse done!\n",
      "2548\n"
     ]
    }
   ],
   "source": [
    "print('parse done!')\n",
    "print(len(sentenceList))\n",
    "\n",
    "# print(len(tweetList))\n",
    "# print(len(result_conll))\n",
    "\n",
    "# print(sentID_to_tweet_ID[15])\n",
    "# print(result_conll[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just printing the twokenized sentences\n",
    "# sentId=0\n",
    "# df_holder=[]\n",
    "# df_columns=['tweet_id','sentence_id','word']\n",
    "# for sentence in sentenceList:\n",
    "# #     print(sentence)\n",
    "#     sentence_tokens= flatten([split_apostrophe(elem) for elem in getWordsII(sentence)],[])\n",
    "# #     print(sentence_tokens)\n",
    "# #     result=result_conll[sentId]\n",
    "#     for token in sentence_tokens:\n",
    "# #     for result_line in result.split('\\n'):\n",
    "# #         tabs = result_line.split('\\t')\n",
    "#         df_dict={'tweet_id':str(sentID_to_tweet_ID[sentId]),'sentence_id':str(sentId), 'word':token}\n",
    "#         df_holder.append(df_dict)\n",
    "#     sentId+=1\n",
    "\n",
    "# df_out = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print('pre-encoding dataframe: ', len(df_out))\n",
    "\n",
    "# #align mentions with tweets and generate BIO encoding:\n",
    "# encoded_df_columns=['Sentence #','Word','Tag']\n",
    "# encoded_df_holder=[]\n",
    "\n",
    "# # file_text=''\n",
    "# for index, mentions in enumerate(mentionList):\n",
    "#     tweet_sentID_list= df_out[df_out['tweet_id']==str(index)].sentence_id.tolist()\n",
    "#     tweet_word_list= df_out[df_out['tweet_id']==str(index)].word.tolist()\n",
    "#     print('tweet ID:', index,mentions)\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, mentions)\n",
    "    \n",
    "# #     print('tallying list lengths: ',len(tweet_sentID_list),len(tweet_word_list),len(tweet_encoding_list))\n",
    "    \n",
    "#     for encoded_list_index, sentID in enumerate(tweet_sentID_list):\n",
    "#         encoded_df_dict={'Sentence #':tweet_sentID_list[encoded_list_index], 'Word':tweet_word_list[encoded_list_index], 'Tag':tweet_encoding_list[encoded_list_index]}\n",
    "#         encoded_df_holder.append(encoded_df_dict)\n",
    "# #         file_text+=tweet_word_list[encoded_list_index]+'\\t'+tweet_encoding_list[encoded_list_index]+'\\n'\n",
    "# #     file_text+='\\n'\n",
    "\n",
    "# encoded_df_out=pd.DataFrame(encoded_df_holder,columns=encoded_df_columns)\n",
    "# print('post-encoding dataframe: ', len(encoded_df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/tweets_3k_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/venezuela_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billnye_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/pikapika_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/ripcity_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/roevwade_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "# print(encoded_df_out.tail(40))\n",
    "\n",
    "# import re\n",
    "# mystr='Macron.'\n",
    "# print(split_apostrophe(mystr))\n",
    "# print(mystr.split('-'))\n",
    "# re.split(\"(-)\",mystr)\n",
    "# if((mystr.count('.')==1)&(mystr.endswith('.'))):\n",
    "#     temp=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",mystr)))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "# conll_nounPhrase_chunking(conll_results)\n",
    "\n",
    "def getConnectedComponents(visited, adjList):\n",
    "    cc=[]\n",
    "    cc_positions=[]\n",
    "    nodeList=list(visited.keys())\n",
    "#     print('**',visited,adjList)\n",
    "    for ind in range(len(nodeList)):\n",
    "        node=nodeList[ind]\n",
    "#         print('==>',node)\n",
    "        if not(visited[node][0]):\n",
    "            if(ind>0):\n",
    "                last.sort(key = int)\n",
    "                if('^' in posStr):\n",
    "#                     print('::',last)\n",
    "                    candidateStringInner=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "                    cc.append(candidateStringInner)\n",
    "                    cc_positions.append(last)\n",
    "            last=[]\n",
    "            posStr=''\n",
    "            bfs=[node]\n",
    "            while(bfs):\n",
    "                curr=bfs.pop(0)\n",
    "                visited[curr][0]=True\n",
    "                last.append(curr)\n",
    "                posStr+=visited[curr][2]\n",
    "                for neighbour in adjList[curr]:\n",
    "                    if(not visited[neighbour][0]):\n",
    "                        bfs.append(neighbour)\n",
    "        ind+=1\n",
    "    last.sort(key = int)\n",
    "    if('^' in posStr):\n",
    "        candidateString=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "        cc.append(candidateString)\n",
    "        cc_positions.append(last)\n",
    "#     print('connected components:')\n",
    "#     print(cc)\n",
    "    return cc, cc_positions\n",
    "\n",
    "def conll_nounPhrase_chunking(tabbed_entries):\n",
    "    spans=[]\n",
    "    span=[]\n",
    "    for tabbed_entry in tabbed_entries:\n",
    "        entry=[]\n",
    "        if((tabbed_entry[3]==proper_noun_tag)|(tabbed_entry[3]==common_noun_tag)):\n",
    "            entry=tabbed_entry\n",
    "        if(tabbed_entry[3]==prep_tag):\n",
    "            head=int(tabbed_entry[6])-1\n",
    "            if(head>0):\n",
    "                head_entry=tabbed_entries[head]\n",
    "                if((head_entry[3]==proper_noun_tag)|(head_entry[3]==common_noun_tag)):\n",
    "                    entry=tabbed_entry\n",
    "        if(entry):\n",
    "            if(int(entry[0])>1):\n",
    "                if(span):\n",
    "                    if((int(entry[0])-int(span[-1][0]))>1):\n",
    "                        spans.append(span)\n",
    "                        span=[entry]\n",
    "                    else:\n",
    "                        span.append(entry)\n",
    "                else:\n",
    "                    span=[entry]\n",
    "            else:\n",
    "                span=[entry]\n",
    "    if(spans):\n",
    "        if(spans[-1][0]!=span[0]):\n",
    "            spans.append(span)\n",
    "    else:\n",
    "        if(span):\n",
    "            spans.append(span)\n",
    "    \n",
    "    final_spans=[]\n",
    "    final_spans_positions=[]\n",
    "    for span in spans:\n",
    "        minIndex=int(span[0][0])\n",
    "        maxIndex=int(span[-1][0])\n",
    "        visited={}\n",
    "        adjList={}\n",
    "        for entry in span:\n",
    "            visited[entry[0]]=[False,entry[1],entry[3]]\n",
    "            if(entry[0] not in adjList.keys()):\n",
    "                adjList[entry[0]]=[]\n",
    "            dependency=entry[6]\n",
    "            if((int(dependency)>=minIndex)&(int(dependency)<=maxIndex)):\n",
    "                adjList[entry[0]].append(dependency)\n",
    "                if(dependency not in adjList.keys()):\n",
    "                    adjList[dependency]=[]\n",
    "                adjList[dependency].append(entry[0])\n",
    "        retTup=getConnectedComponents(visited,adjList)\n",
    "        final_spans.extend(retTup[0])\n",
    "        final_spans_positions.extend(retTup[1])\n",
    "    return final_spans,final_spans_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(annotated_mention_list,output_mentions_list):\n",
    "\n",
    "    # print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "    print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "    \n",
    "    precision=(tp_counter_inner)/(tp_counter_inner+fp_counter_inner)\n",
    "    recall=(tp_counter_inner)/(tp_counter_inner+fn_counter_inner)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized.txt\", \"w\")\n",
    "# file_text1=''\n",
    "\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized_POSTAG.txt\", \"w\")\n",
    "# file_text=''\n",
    "\n",
    "# candidates=[]\n",
    "# # sentId=0\n",
    "# tweetId=0\n",
    "# CTrie=trie.Trie(\"ROOT\")\n",
    "# # for sentence in sentenceList:\n",
    "# for tweet in tweetList:\n",
    "# #     result=result_conll[sentId]\n",
    "#     result=result_conll[tweetId]\n",
    "#     tweet_word_list=[]\n",
    "#     conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "#     for result_line in conll_results:\n",
    "#         file_text+=result_line[1]+'\\t'+result_line[3]+'\\n'\n",
    "#         tweet_word_list+=result_line[1]\n",
    "# #         file_text1+=result_line[1]+'\\tO'+'\\n'\n",
    "# #         print('result_line: ', result_line)\n",
    "#     file_text+='\\n'\n",
    "#     tweetId+=1\n",
    "#     tweetMentions=mentionList[tweetId]\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, tweetMentions)\n",
    "#     print(len(tweet_word_list),len(tweet_encoding_list))\n",
    "#     for ind, word in enumerate(tweet_word_list):\n",
    "#         file_text1+=word+'\\t'+tweet_encoding_list[ind]+'\\n'\n",
    "#     file_text1+='\\n'\n",
    "# #     sentId+=1\n",
    "#     tweetId+=1\n",
    "\n",
    "# f.write(file_text)\n",
    "# f.close()\n",
    "\n",
    "# f1.write(file_text1)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548 1105 2548\n"
     ]
    }
   ],
   "source": [
    "candidates=[]\n",
    "sentId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "for sentence in sentenceList:\n",
    "    result=result_conll[sentId]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    sentence_candidates,sentence_candidates_positions=conll_nounPhrase_chunking(conll_results)\n",
    "#     print(sentence)\n",
    "#     print(conll_results)\n",
    "#     print(sentence_candidates)\n",
    "\n",
    "    candidate_ind=0\n",
    "    phase1Out=''\n",
    "#     for candidate in ne_List_allCheck:\n",
    "#         position = '*'+'*'.join(str(v) for v in candidate.position)\n",
    "#         position=position+'*'\n",
    "#         candidate.set_sen_index(sen_index)\n",
    "#         phase1Out+=(((candidate.phraseText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "\n",
    "    for candidateText in sentence_candidates:\n",
    "#         print(candidateText)\n",
    "        candidateText=candidateText.lower()\n",
    "        position = '*'+'*'.join(str(v) for v in sentence_candidates_positions[candidate_ind])\n",
    "        position=position+'*'\n",
    "#         print(candidateText,sentence_candidates_positions[candidate_ind])\n",
    "        CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "        candidate_ind+=1\n",
    "        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "        \n",
    "#     candidates.append(sentence_candidates)\n",
    "    candidates.append(phase1Out)\n",
    "\n",
    "    sentId+=1\n",
    "#     print('===========')\n",
    "print(len(sentenceList),len(tweets_unpartitoned),len(candidates))\n",
    "\n",
    "tweet_sentence_df['phase1Candidates']=candidates\n",
    "time2=time.time()\n",
    "# print(tweet_sentence_df['phase1Candidates'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "candidates=CTrie.displayTrie(\"\",[])\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  2548 2548 274\n",
      "-0.3811952061026746\n",
      "For entities:  (158, 6)\n",
      "For non-entities:  (113, 6)\n",
      "For ambiguous:  (3, 6)\n",
      "For entities:  (158, 6)\n",
      "For non-entities:  (113, 6)\n",
      "For ambiguous:  (3, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n",
      "completed tweets:  2514 incomplete tweets:  34\n",
      "16\n",
      "16\n",
      "final tally:  2548 2548\n",
      "524:  524    [[]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2 = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted= Phase2.executor(max_batch_value,tweet_sentence_df,CTrie,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df)\n",
    "time3=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_unpartitoned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Venezuela, Venezuela, Venezuela]</td>\n",
       "      <td>[RheallN, RheallN, RheallN]</td>\n",
       "      <td>[This is a good thread on the backstory that h...</td>\n",
       "      <td>[point in #venezuela::*15*16*17*||, trump admi...</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [trump administration], []]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(us, b)], [(trump administration, g)], [(pea...</td>\n",
       "      <td>[[], [trump administration], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[France, MayDay, France, MayDay, France, MayDa...</td>\n",
       "      <td>[Abreus, Abreus, Abreus, Abreus, Abreus]</td>\n",
       "      <td>[Wow., Absolute chaos today in #France on #May...</td>\n",
       "      <td>[, chaos today in #france on #mayday::*2*3*4*5...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(police, b)], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[France, MayDay, France, MayDay, France, MayDa...</td>\n",
       "      <td>[Clare_Scotland, Clare_Scotland, Clare_Scotlan...</td>\n",
       "      <td>[Wow., Absolute chaos today in #France on #May...</td>\n",
       "      <td>[, chaos today in #france on #mayday::*2*3*4*5...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(police, b)], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>[Venezuela, TrishRegan, Venezuela, TrishRegan,...</td>\n",
       "      <td>[MAGALOGAN2379, MAGALOGAN2379, MAGALOGAN2379, ...</td>\n",
       "      <td>[TONIGHT - hear directly from ⁦@realDonaldTrum...</td>\n",
       "      <td>[⁦::*6*||venezuela::*10*||, , tonigh::*3*||pre...</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "      <td>[[], [], [pres of venezuela], [venezuela], [],...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(pres of venezuela, g)], [(venezuela...</td>\n",
       "      <td>[[], [], [pres of venezuela], [venezuela], [],...</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[Venezuela, Venezuela, Venezuela, Venezuela, V...</td>\n",
       "      <td>[andreaskapp, andreaskapp, andreaskapp, andrea...</td>\n",
       "      <td>[Watch @anyaparampil's great interview demolis...</td>\n",
       "      <td>[trump::*7*||push for regime change in #venezu...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[trump], [trump, neocon nutjobs pompeo, bolto...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(trump, g)], [(trump, g), (neocon nutjobs po...</td>\n",
       "      <td>[[trump], [trump, neocon nutjobs pompeo, bolto...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID                           index         entry_batch  \\\n",
       "0        0                 [nan, nan, nan]           [0, 0, 0]   \n",
       "1        1       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "2        2       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "3        3  [nan, nan, nan, nan, nan, nan]  [0, 0, 0, 0, 0, 0]   \n",
       "4        4       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "\n",
       "               sentID                                           hashtags  \\\n",
       "0           [0, 1, 2]                  [Venezuela, Venezuela, Venezuela]   \n",
       "1     [0, 1, 2, 3, 4]  [France, MayDay, France, MayDay, France, MayDa...   \n",
       "2     [0, 1, 2, 3, 4]  [France, MayDay, France, MayDay, France, MayDa...   \n",
       "3  [0, 1, 2, 3, 4, 5]  [Venezuela, TrishRegan, Venezuela, TrishRegan,...   \n",
       "4     [0, 1, 2, 3, 4]  [Venezuela, Venezuela, Venezuela, Venezuela, V...   \n",
       "\n",
       "                                                user  \\\n",
       "0                        [RheallN, RheallN, RheallN]   \n",
       "1           [Abreus, Abreus, Abreus, Abreus, Abreus]   \n",
       "2  [Clare_Scotland, Clare_Scotland, Clare_Scotlan...   \n",
       "3  [MAGALOGAN2379, MAGALOGAN2379, MAGALOGAN2379, ...   \n",
       "4  [andreaskapp, andreaskapp, andreaskapp, andrea...   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [This is a good thread on the backstory that h...   \n",
       "1  [Wow., Absolute chaos today in #France on #May...   \n",
       "2  [Wow., Absolute chaos today in #France on #May...   \n",
       "3  [TONIGHT - hear directly from ⁦@realDonaldTrum...   \n",
       "4  [Watch @anyaparampil's great interview demolis...   \n",
       "\n",
       "                                    phase1Candidates  \\\n",
       "0  [point in #venezuela::*15*16*17*||, trump admi...   \n",
       "1  [, chaos today in #france on #mayday::*2*3*4*5...   \n",
       "2  [, chaos today in #france on #mayday::*2*3*4*5...   \n",
       "3  [⁦::*6*||venezuela::*10*||, , tonigh::*3*||pre...   \n",
       "4  [trump::*7*||push for regime change in #venezu...   \n",
       "\n",
       "                 annotation       stanford_candidates  \\\n",
       "0              [[], [], []]              [[], [], []]   \n",
       "1      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "2      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "3  [[], [], [], [], [], []]  [[], [], [], [], [], []]   \n",
       "4      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "\n",
       "                                     output_mentions  \\\n",
       "0                   [[], [trump administration], []]   \n",
       "1                               [[], [], [], [], []]   \n",
       "2                               [[], [], [], [], []]   \n",
       "3  [[], [], [pres of venezuela], [venezuela], [],...   \n",
       "4  [[trump], [trump, neocon nutjobs pompeo, bolto...   \n",
       "\n",
       "                           completeness             current_minus_entry  \\\n",
       "0                    [True, True, True]                 [0.0, 0.0, 0.0]   \n",
       "1        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "2        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "3  [True, True, True, True, True, True]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "4        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                               candidates_with_label  \\\n",
       "0  [[(us, b)], [(trump administration, g)], [(pea...   \n",
       "1                    [[], [], [(police, b)], [], []]   \n",
       "2                    [[], [], [(police, b)], [], []]   \n",
       "3  [[], [], [(pres of venezuela, g)], [(venezuela...   \n",
       "4  [[(trump, g)], [(trump, g), (neocon nutjobs po...   \n",
       "\n",
       "                                only_good_candidates      ambiguous_candidates  \n",
       "0                   [[], [trump administration], []]              [[], [], []]  \n",
       "1                               [[], [], [], [], []]      [[], [], [], [], []]  \n",
       "2                               [[], [], [], [], []]      [[], [], [], [], []]  \n",
       "3  [[], [], [pres of venezuela], [venezuela], [],...  [[], [], [], [], [], []]  \n",
       "4  [[trump], [trump, neocon nutjobs pompeo, bolto...      [[], [], [], [], []]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['trump administration', 'globalism']\n",
      "[]\n",
      "2 ['macron']\n",
      "[]\n",
      "3 ['macron']\n",
      "['pres of venezuela', 'venezuela']\n",
      "4 ['pres of venezuela', 'venezuela']\n",
      "['trump', 'trump', 'neocon nutjobs pompeo', 'bolton', 'abrams', 'trump']\n",
      "5 ['trump', 'trump', 'pompeo', 'bolton', 'abrams', 'trump']\n",
      "[]\n",
      "6 []\n",
      "['president', 'dictator raul castro', 'castro']\n",
      "7 ['dictator raul castro', 'minister of defense', 'castro', 'cuban']\n",
      "['pres of venezuela', 'venezuela']\n",
      "8 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "9 []\n",
      "[]\n",
      "10 []\n",
      "[]\n",
      "11 ['los teques']\n",
      "[]\n",
      "12 ['neokaxtrizmo']\n",
      "[]\n",
      "13 []\n",
      "['trump', 'cuba', 'canada', 'maduro', 'bolton', 'abrams', 'maduro', 'venezuela']\n",
      "14 ['trump', 'cuba', 'canada', 'maduro', 'bolton', 'abrams', 'erdogan', 'putin', 'maduro', 'venezuela']\n",
      "[]\n",
      "15 []\n",
      "['united states']\n",
      "16 ['secretary of state mike pompeo', 'united states']\n",
      "['venezuela', 'president', 'nicolás maduro', 'maduro']\n",
      "17 ['venezuela', 'constitution', 'nicolás maduro', 'maduro']\n",
      "[]\n",
      "18 []\n",
      "[]\n",
      "19 []\n",
      "[]\n",
      "20 []\n",
      "[]\n",
      "21 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "22 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "23 []\n",
      "[]\n",
      "24 []\n",
      "[]\n",
      "25 []\n",
      "[]\n",
      "26 ['vpn']\n",
      "['venezuela', 'maduro']\n",
      "27 ['venezuela', 'maduro']\n",
      "[]\n",
      "28 ['socialism']\n",
      "['russians']\n",
      "29 ['russians']\n",
      "[]\n",
      "30 []\n",
      "[]\n",
      "31 []\n",
      "['u.s. secretary of state']\n",
      "32 ['u.s. secretary of state', 'russian foreign minister']\n",
      "[]\n",
      "33 []\n",
      "['US', 'venezuela']\n",
      "34 ['us', 'venezuela']\n",
      "['maduro']\n",
      "35 ['socialism', 'socialism', 'maduro']\n",
      "[]\n",
      "36 []\n",
      "[]\n",
      "37 []\n",
      "[]\n",
      "38 []\n",
      "[]\n",
      "39 []\n",
      "[]\n",
      "40 []\n",
      "[]\n",
      "41 []\n",
      "['guaido']\n",
      "42 ['guaido']\n",
      "[]\n",
      "43 []\n",
      "[]\n",
      "44 []\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "45 ['ret. brigadier general', 'u.s. military', 'u.s. special forces', 'juan guaido']\n",
      "['u.s', 'venezuela']\n",
      "46 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "47 []\n",
      "['juan guaido']\n",
      "48 ['juan guaido']\n",
      "[]\n",
      "49 []\n",
      "[]\n",
      "50 []\n",
      "['usmc']\n",
      "51 ['usmc']\n",
      "['u.s', 'venezuela']\n",
      "52 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "53 []\n",
      "[]\n",
      "54 ['macron']\n",
      "[]\n",
      "55 ['spanish']\n",
      "['america']\n",
      "56 ['america', 'usa']\n",
      "[]\n",
      "57 []\n",
      "['el diario de dmfusion']\n",
      "58 ['el diario de dmfusion']\n",
      "[]\n",
      "59 []\n",
      "[]\n",
      "60 []\n",
      "[]\n",
      "61 []\n",
      "['gnb in', 'caracas']\n",
      "62 ['gnb', 'caracas']\n",
      "['altamira', 'caracas', 'gnb']\n",
      "63 ['altamira', 'caracas', 'gnb']\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "64 ['ret. brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "['trump administration']\n",
      "65 ['trump administration', 'globalism']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "66 ['venezuela', 'maduro', 'venezuela']\n",
      "['lima group', 'president', 'venezuelans']\n",
      "67 ['lima group', 'interim president', 'venezuelans']\n",
      "[]\n",
      "68 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "69 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "70 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "71 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "72 []\n",
      "['venezuela', 'maduro']\n",
      "73 ['venezuela', 'maduro']\n",
      "['msnbc']\n",
      "74 ['msnbc']\n",
      "['altamira', 'caracas']\n",
      "75 ['altamira']\n",
      "[]\n",
      "76 []\n",
      "[]\n",
      "77 []\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "78 ['trish regan primetime', 'u.s']\n",
      "['usmc']\n",
      "79 ['usmc']\n",
      "[]\n",
      "80 []\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "81 ['ret. brigadier general', 'u.s', 'u.s special forces', 'juan guaido']\n",
      "[]\n",
      "82 []\n",
      "['guaido', 'maduro']\n",
      "83 ['guaido', 'maduro']\n",
      "['trump administration']\n",
      "84 ['trump administration', 'globalism']\n",
      "['venezuela', 'latin', 'america', 'bernie sanders', 'united states']\n",
      "85 ['venezuela', 'latin america', 'democrat socialists', 'bernie sanders', 'united states']\n",
      "[]\n",
      "86 []\n",
      "[]\n",
      "87 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "88 ['pres of venezuela', 'venezuela']\n",
      "['president', 'president']\n",
      "89 ['hillary']\n",
      "['gnb in caracas']\n",
      "90 ['gnb', 'caracas']\n",
      "['adm', 'iran', 'cuba']\n",
      "91 ['adm. faller', 'iran', 'tehran', 'cuba']\n",
      "[]\n",
      "92 []\n",
      "[]\n",
      "93 []\n",
      "[]\n",
      "94 []\n",
      "['u.s', 'venezuela']\n",
      "95 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "96 []\n",
      "['adm', 'venezuela']\n",
      "97 ['adm. faller', 'venezuela']\n",
      "[]\n",
      "98 []\n",
      "[]\n",
      "99 []\n",
      "[]\n",
      "100 ['measles', 'bill', 'iraqi']\n",
      "['pres of venezuela', 'venezuela']\n",
      "101 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "102 []\n",
      "[]\n",
      "103 []\n",
      "[]\n",
      "104 []\n",
      "[]\n",
      "105 ['socialism']\n",
      "[]\n",
      "106 []\n",
      "[]\n",
      "107 []\n",
      "[]\n",
      "108 []\n",
      "['altamira interchange', 'la carlota']\n",
      "109 ['rubén brito', 'altamira interchange', 'la carlota', 'national guard']\n",
      "['caracas', 'catalonia']\n",
      "110 ['spanish government', 'caracas', 'catalonia']\n",
      "['gnb in', 'caracas']\n",
      "111 ['gnb', 'caracas']\n",
      "['gnb in', 'carabobo']\n",
      "112 ['gnb', 'valencia', 'carabobo']\n",
      "[]\n",
      "113 ['socialism']\n",
      "[]\n",
      "114 []\n",
      "['usmc']\n",
      "115 ['usmc']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "116 ['american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "[]\n",
      "117 []\n",
      "[]\n",
      "118 []\n",
      "['gnb in', 'caracas']\n",
      "119 ['gnb', 'caracas']\n",
      "[]\n",
      "120 []\n",
      "['usmc']\n",
      "121 ['usmc']\n",
      "[]\n",
      "122 []\n",
      "[]\n",
      "123 ['crabcon']\n",
      "['la carlota', 'air base in caracas']\n",
      "124 ['la carlota', 'caracas']\n",
      "['adm', 'china', 'venezuela']\n",
      "125 ['adm. faller', 'china', 'venezuela', 'venezuelan', 'venezuelan']\n",
      "[]\n",
      "126 []\n",
      "['venezuelans', 'venezuela']\n",
      "127 ['greek', 'venezuelans', 'venezuela']\n",
      "[]\n",
      "128 []\n",
      "['maduro', 'american', 'maduro', 'america']\n",
      "129 ['maduro', 'american', 'coup', 'maduro', 'america']\n",
      "['venezuela', 'venezuela', 'venezuelans']\n",
      "130 ['venezuela', 'venezuela', 'venezuelans']\n",
      "[]\n",
      "131 ['constitutionality', '']\n",
      "[]\n",
      "132 ['macron']\n",
      "['gru']\n",
      "133 ['gru']\n",
      "['venezuela']\n",
      "134 ['venezuela']\n",
      "['maduro', 'juan guaidó', 'US']\n",
      "135 ['juan guaidó', 'us', 'maduro']\n",
      "['pepe mujica', 'maduro']\n",
      "136 ['pepe mujica', 'maduro']\n",
      "['guardian', 'corbynista kryptonite']\n",
      "137 [\"guardian's soy boy\", 'corbynista kryptonite']\n",
      "[]\n",
      "138 []\n",
      "[]\n",
      "139 []\n",
      "['adm', 'venezuela']\n",
      "140 ['adm. faller', 'venezuela']\n",
      "['ron paul', 'guaido', 'washington']\n",
      "141 ['ron paul', 'guaido', 'washington', 'venezuelan']\n",
      "['maduro death squads', 'faes']\n",
      "142 ['maduro death squads', 'faes']\n",
      "[]\n",
      "143 []\n",
      "[]\n",
      "144 []\n",
      "[]\n",
      "145 ['bolivar']\n",
      "[]\n",
      "146 ['macron']\n",
      "[]\n",
      "147 []\n",
      "['president']\n",
      "148 []\n",
      "[]\n",
      "149 ['socialism']\n",
      "[]\n",
      "150 ['socialism']\n",
      "[]\n",
      "151 []\n",
      "['venezuela', 'venezuela', 'venezuelans']\n",
      "152 ['venezuela', 'venezuela', 'venezuelans']\n",
      "['colorado/ florida', 'u.s']\n",
      "153 ['colorado', 'florida', 'u.s']\n",
      "[]\n",
      "154 []\n",
      "[]\n",
      "155 []\n",
      "['america', 'mike pompeo']\n",
      "156 ['mike pompeo']\n",
      "[]\n",
      "157 ['macron']\n",
      "[]\n",
      "158 []\n",
      "['president', 'united states']\n",
      "159 ['president of the united states']\n",
      "[]\n",
      "160 ['venezuela']\n",
      "[]\n",
      "161 []\n",
      "[]\n",
      "162 []\n",
      "[]\n",
      "163 []\n",
      "[]\n",
      "164 []\n",
      "[]\n",
      "165 []\n",
      "['maduro']\n",
      "166 ['venezuelan army', 'maduro']\n",
      "[]\n",
      "167 []\n",
      "[]\n",
      "168 ['macron']\n",
      "['trish regan', 'venezuela']\n",
      "169 ['trish regan', 'venezuela']\n",
      "[]\n",
      "170 []\n",
      "[]\n",
      "171 []\n",
      "[]\n",
      "172 []\n",
      "[]\n",
      "173 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "174 ['pres of venezuela', 'venezuela']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "175 ['trish regan primetime', 'u.s']\n",
      "[]\n",
      "176 []\n",
      "[]\n",
      "177 ['macron']\n",
      "[]\n",
      "178 []\n",
      "[]\n",
      "179 ['spanish']\n",
      "['maduro', 'US', 'uk', 'US', 'uk']\n",
      "180 ['maduro', 'us', 'uk', 'us', 'us', 'uk']\n",
      "[]\n",
      "181 []\n",
      "['guaido']\n",
      "182 ['guaido']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "183 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['pepe mujica']\n",
      "184 ['pepe mujica']\n",
      "['guaido', 'maduro', 'trump', 'US']\n",
      "185 ['guaido', 'maduro', 'us', 'trump', 'us']\n",
      "[]\n",
      "186 []\n",
      "['russians']\n",
      "187 ['russians']\n",
      "[]\n",
      "188 ['socialism']\n",
      "['president', 'obama']\n",
      "189 ['us', 'obama', 'hillary']\n",
      "[]\n",
      "190 ['macron']\n",
      "[]\n",
      "191 ['macron']\n",
      "[]\n",
      "192 []\n",
      "[]\n",
      "193 []\n",
      "[]\n",
      "194 []\n",
      "['guaido', 'gnb', 'la carlota']\n",
      "195 ['guaido', 'gnb', 'la carlota']\n",
      "[]\n",
      "196 []\n",
      "[]\n",
      "197 []\n",
      "[]\n",
      "198 ['macron']\n",
      "[]\n",
      "199 []\n",
      "['guardian', 'corbynista kryptonite']\n",
      "200 [\"guardian's soy boy\", 'corbynista kryptonite']\n",
      "[]\n",
      "201 ['democracy']\n",
      "['u.s', 'venezuela']\n",
      "202 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "203 []\n",
      "[]\n",
      "204 []\n",
      "['fort russ news', 'maduro']\n",
      "205 ['fort russ news', 'venezuelan army', 'maduro']\n",
      "[]\n",
      "206 []\n",
      "['utopia']\n",
      "207 ['leftists', 'socialist utopia', 'socialism']\n",
      "['u.s. secretary of state']\n",
      "208 ['u.s. secretary of state', 'russian foreign minister']\n",
      "['ron paul', 'guaido', 'washington']\n",
      "209 ['ron paul', 'guaido', 'washington', 'venezuelan']\n",
      "['trump administration', 'u.s']\n",
      "210 ['trump administration', 'u.s']\n",
      "[]\n",
      "211 ['socialism']\n",
      "[]\n",
      "212 []\n",
      "['venezuelans', 'venezuelans']\n",
      "213 ['venezuelans', 'venezuelans']\n",
      "['russians']\n",
      "214 ['russians']\n",
      "[]\n",
      "215 []\n",
      "[]\n",
      "216 []\n",
      "['gun🇺🇸🇺🇸🇺🇸']\n",
      "217 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "218 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "219 ['macron']\n",
      "[]\n",
      "220 []\n",
      "['adm']\n",
      "221 ['adm. faller']\n",
      "[]\n",
      "222 []\n",
      "[]\n",
      "223 []\n",
      "[]\n",
      "224 []\n",
      "['guaido']\n",
      "225 ['guaido']\n",
      "[]\n",
      "226 []\n",
      "['maduro', 'twitter']\n",
      "227 ['maduro', 'france', 'twitter']\n",
      "[]\n",
      "228 ['socialism']\n",
      "['msm', 'bbc']\n",
      "229 ['msm', 'bbc']\n",
      "['adm', 'venezuela']\n",
      "230 ['adm. faller', 'venezuela']\n",
      "[]\n",
      "231 []\n",
      "['adm', 'venezuela']\n",
      "232 ['adm. faller', 'venezuela']\n",
      "[]\n",
      "233 []\n",
      "[]\n",
      "234 []\n",
      "[]\n",
      "235 []\n",
      "[]\n",
      "236 ['macron']\n",
      "[]\n",
      "237 []\n",
      "[]\n",
      "238 []\n",
      "[]\n",
      "239 []\n",
      "[]\n",
      "240 []\n",
      "[]\n",
      "241 ['day now', 'faller']\n",
      "['united states']\n",
      "242 ['united states']\n",
      "[]\n",
      "243 []\n",
      "[]\n",
      "244 ['venezuelan']\n",
      "[]\n",
      "245 []\n",
      "['venezuelans']\n",
      "246 ['venezuelans']\n",
      "['US']\n",
      "247 ['us', 'isis', 'us']\n",
      "['pres of venezuela', 'venezuela']\n",
      "248 ['pres of venezuela', 'venezuela']\n",
      "['msnbc']\n",
      "249 ['msnbc']\n",
      "[]\n",
      "250 []\n",
      "['venezuela']\n",
      "251 ['venezuela', 'socialist']\n",
      "[]\n",
      "252 []\n",
      "['trump administration']\n",
      "253 ['trump administration', 'globalism']\n",
      "[]\n",
      "254 []\n",
      "['libya', 'yemen', 'gaza', 'syria', 'honduras', 'iran', 'somalia']\n",
      "255 ['usa foreign policy', 'libya', 'yemen', 'gaza', 'syria', 'honduras', 'iran', 'somalia']\n",
      "['pres of venezuela', 'venezuela']\n",
      "256 ['pres of venezuela', 'venezuela']\n",
      "['US', 'bernie sanders']\n",
      "257 ['us', 'bernie sanders', 'congress']\n",
      "['venezuela', 'latin', 'america', 'bernie sanders', 'united states']\n",
      "258 ['venezuela', 'latin america', 'democrat socialists', 'bernie sanders', 'united states']\n",
      "['venezuela']\n",
      "259 ['venezuela', 'socialist']\n",
      "['president', 'venezuela', 'president']\n",
      "260 ['president of venezuela', 'constitution']\n",
      "['trump administration']\n",
      "261 ['trump administration', 'globalism']\n",
      "['u.s']\n",
      "262 ['u.s']\n",
      "[]\n",
      "263 ['socialism']\n",
      "['maduro', 'chavez', 'mestizos', 'chavez', 'guaido', 'maduro']\n",
      "264 ['maduro', 'chavez', 'mestizos', 'chavez', 'guaido', 'maduro']\n",
      "[]\n",
      "265 []\n",
      "[]\n",
      "266 []\n",
      "[]\n",
      "267 ['us']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "268 ['trish regan primetime', 'u.s']\n",
      "['venezuelans']\n",
      "269 ['venezuelans']\n",
      "[]\n",
      "270 []\n",
      "[]\n",
      "271 []\n",
      "[]\n",
      "272 []\n",
      "[]\n",
      "273 []\n",
      "['gnb in', 'caracas']\n",
      "274 ['gnb', 'caracas']\n",
      "['pres of venezuela', 'venezuela']\n",
      "275 ['pres of venezuela', 'venezuela']\n",
      "['msnbc']\n",
      "276 ['msnbc']\n",
      "['president', 'obama']\n",
      "277 ['us', 'obama', 'hillary']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "278 ['trish regan primetime', 'u.s']\n",
      "['trump', 'doc duvalier']\n",
      "279 ['trump', 'dictator baby doc duvalier']\n",
      "['venezuela', 'maduro']\n",
      "280 ['venezuela', 'maduro']\n",
      "['venezuelans']\n",
      "281 ['venezuelans']\n",
      "[]\n",
      "282 []\n",
      "['venezuelans']\n",
      "283 ['venezuelans']\n",
      "[]\n",
      "284 []\n",
      "[]\n",
      "285 []\n",
      "[]\n",
      "286 []\n",
      "['caracas']\n",
      "287 ['caracas']\n",
      "[]\n",
      "288 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "289 ['pres of venezuela', 'venezuela']\n",
      "['la carlota air base', 'caracas']\n",
      "290 ['la carlota air base', 'caracas']\n",
      "[]\n",
      "291 []\n",
      "['russians']\n",
      "292 ['russians']\n",
      "[]\n",
      "293 []\n",
      "[]\n",
      "294 []\n",
      "[]\n",
      "295 []\n",
      "['moa', 'guaidó', 'snookered', 'white house']\n",
      "296 ['guaidó', 'white house']\n",
      "['president', 'united states']\n",
      "297 ['president of the united states']\n",
      "[]\n",
      "298 []\n",
      "[]\n",
      "299 []\n",
      "['beatriz becerra on twitter']\n",
      "300 ['beatriz becerra', 'twitter']\n",
      "[]\n",
      "301 []\n",
      "['sb', 'venezuelans', 'maduro', 'venezuelans']\n",
      "302 ['sb 168', 'venezuelans', 'maduro', 'venezuelans', 'florida']\n",
      "['mossos']\n",
      "303 ['mossos']\n",
      "['u.s', 'guaido', 'maduro', 'u.s']\n",
      "304 ['u.s', 'guaido', 'maduro', 'u.s']\n",
      "['venezuela', 'freeland']\n",
      "305 ['venezuela', 'freeland']\n",
      "['pres of venezuela', 'venezuela']\n",
      "306 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "307 []\n",
      "[]\n",
      "308 ['socialism']\n",
      "[]\n",
      "309 ['dubarryfilms daily']\n",
      "[]\n",
      "310 []\n",
      "[]\n",
      "311 []\n",
      "['american']\n",
      "312 ['south american']\n",
      "['pres of venezuela', 'venezuela']\n",
      "313 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "314 []\n",
      "['adm']\n",
      "315 ['adm. faller', 'russia']\n",
      "[]\n",
      "316 []\n",
      "[]\n",
      "317 ['hirono']\n",
      "[]\n",
      "318 []\n",
      "['venezuelans']\n",
      "319 ['venezuelans']\n",
      "['united states']\n",
      "320 ['united states']\n",
      "[]\n",
      "321 []\n",
      "['pompeo']\n",
      "322 ['pompeo']\n",
      "['pres of venezuela', 'venezuela']\n",
      "323 ['', 'pres of venezuela', 'venezuela']\n",
      "['president', 'nicolas maduro', 'president']\n",
      "324 ['nicolas maduro', 'democracy']\n",
      "[]\n",
      "325 []\n",
      "[]\n",
      "326 ['macron']\n",
      "['usmc']\n",
      "327 ['usmc']\n",
      "[]\n",
      "328 []\n",
      "['u.s', 'venezuela']\n",
      "329 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "330 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "331 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "332 ['macron']\n",
      "[]\n",
      "333 []\n",
      "['young voices', 'venezuela']\n",
      "334 ['young voices', 'daniel dimartino', 'venezuela', 'fox news']\n",
      "[]\n",
      "335 ['macron']\n",
      "[\"maduro's side\"]\n",
      "336 ['maduro']\n",
      "[]\n",
      "337 ['socialism']\n",
      "['smith']\n",
      "338 ['smith']\n",
      "[]\n",
      "339 []\n",
      "['president', 'juan guaido']\n",
      "340 ['juan guaido']\n",
      "[]\n",
      "341 ['starbucks commies']\n",
      "['lavrov']\n",
      "342 ['foreign minister lavrov']\n",
      "['altamira interchange', 'la carlota']\n",
      "343 ['rubén brito', 'altamira interchange', 'la carlota', 'national guard troops']\n",
      "[]\n",
      "344 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "345 ['', 'pres of venezuela', 'venezuela']\n",
      "[]\n",
      "346 []\n",
      "[]\n",
      "347 []\n",
      "[]\n",
      "348 []\n",
      "[]\n",
      "349 ['socialism']\n",
      "[]\n",
      "350 []\n",
      "['venezuela', 'president', 'nicolás maduro', 'maduro']\n",
      "351 ['venezuela', 'constitution', 'nicolás maduro', 'maduro']\n",
      "[]\n",
      "352 []\n",
      "[]\n",
      "353 []\n",
      "[]\n",
      "354 []\n",
      "[]\n",
      "355 []\n",
      "['venezuela', 'paris']\n",
      "356 ['venezuela', 'paris']\n",
      "['adm', 'iran', 'cuba']\n",
      "357 ['adm. faller', 'iran', 'tehran', 'cuba']\n",
      "[]\n",
      "358 []\n",
      "[]\n",
      "359 []\n",
      "[]\n",
      "360 []\n",
      "['reuters', 'pentagon']\n",
      "361 ['russia', 'reuters', 'pentagon']\n",
      "['venezuelans']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 ['venezuelans']\n",
      "['united states']\n",
      "363 ['united states']\n",
      "[]\n",
      "364 ['macron']\n",
      "[]\n",
      "365 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "366 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "367 []\n",
      "[]\n",
      "368 []\n",
      "[]\n",
      "369 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "370 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "371 ['florida']\n",
      "['usmc']\n",
      "372 ['usmc']\n",
      "[]\n",
      "373 ['venezuela']\n",
      "[]\n",
      "374 []\n",
      "[]\n",
      "375 []\n",
      "[]\n",
      "376 []\n",
      "[]\n",
      "377 []\n",
      "['caracas']\n",
      "378 ['caracas']\n",
      "['venezuela']\n",
      "379 ['venezuela', 'socialism']\n",
      "['president', 'juan guaido']\n",
      "380 ['juan guaido']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "381 ['trish regan primetime', 'u.s']\n",
      "['maduro', 'chavez', 'president']\n",
      "382 ['maduro', 'chavez']\n",
      "['trump administration']\n",
      "383 ['trump administration', 'globalism']\n",
      "[]\n",
      "384 []\n",
      "[]\n",
      "385 []\n",
      "[]\n",
      "386 []\n",
      "[]\n",
      "387 []\n",
      "[]\n",
      "388 ['macron']\n",
      "['president', 'united states']\n",
      "389 ['president of the united states']\n",
      "['saudi arabia', 'US', 'uk']\n",
      "390 ['saudi arabia', 'us', 'uk']\n",
      "[]\n",
      "391 ['macron']\n",
      "['president', 'president']\n",
      "392 []\n",
      "['venezuela', 'maduro']\n",
      "393 ['venezuela', 'maduro']\n",
      "[]\n",
      "394 ['macron']\n",
      "[]\n",
      "395 []\n",
      "['trump', 'mike pence', 'john bolton', 'elliott abrams', 'ted cruz', 'marco rubio', 'juan guaido', 'venezuelans', 'american']\n",
      "396 ['donald trump', 'mike pence', 'john bolton', 'elliott abrams', 'ted cruz', 'marco rubio', 'juan guaido', 'venezuelans', 'american fossil fuel industry']\n",
      "['usmc']\n",
      "397 ['usmc']\n",
      "['colombia']\n",
      "398 ['colombia']\n",
      "['pres of venezuela', 'venezuela']\n",
      "399 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "400 []\n",
      "[]\n",
      "401 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "402 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "403 []\n",
      "['maduro']\n",
      "404 ['maduro']\n",
      "[]\n",
      "405 []\n",
      "['america', 'mike pompeo']\n",
      "406 ['america', 'mike pompeo']\n",
      "['ar', 'washington']\n",
      "407 ['ar-57', 'kent', 'washington', 'p90']\n",
      "[]\n",
      "408 []\n",
      "[]\n",
      "409 []\n",
      "[]\n",
      "410 []\n",
      "['venezuela', 'broward']\n",
      "411 ['venezuela', 'scott', 'broward', 'dws']\n",
      "['maduro']\n",
      "412 ['maduro']\n",
      "['trump', 'colombia']\n",
      "413 ['trump', 'colombia']\n",
      "['president', 'obama']\n",
      "414 ['us', 'obama', 'hillary']\n",
      "[]\n",
      "415 []\n",
      "['pepe mujica']\n",
      "416 ['pepe mujica']\n",
      "[]\n",
      "417 []\n",
      "['pepe mujica', 'maduro']\n",
      "418 ['pepe mujica', 'left', 'maduro']\n",
      "[]\n",
      "419 []\n",
      "[]\n",
      "420 []\n",
      "['guardian', 'corbynista kryptonite']\n",
      "421 [\"guardian's soy boy\", 'socialist', 'corbynista kryptonite']\n",
      "[]\n",
      "422 []\n",
      "[]\n",
      "423 []\n",
      "[]\n",
      "424 []\n",
      "['maduro death squads', 'faes']\n",
      "425 ['maduro death squads', 'faes']\n",
      "['uk', 'US']\n",
      "426 ['uk', 'us', 'venezuelan', 'us']\n",
      "[]\n",
      "427 []\n",
      "[]\n",
      "428 ['spanish']\n",
      "['maduro', 'venezuelan national assembly', 'nicolas maduro', 'maduro']\n",
      "429 ['maduro', 'venezuelan national assembly', 'nicolas maduro', 'maduro']\n",
      "['utc']\n",
      "430 []\n",
      "[]\n",
      "431 []\n",
      "[]\n",
      "432 ['western', 'us']\n",
      "[]\n",
      "433 []\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "434 ['ret. brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "['venezuela']\n",
      "435 ['venezuela', 'socialism']\n",
      "['president', 'united states']\n",
      "436 ['president of the united states']\n",
      "['adm', 'venezuela']\n",
      "437 ['adm. faller', 'venezuela']\n",
      "[]\n",
      "438 ['operation freedom']\n",
      "['president', 'united states']\n",
      "439 ['president of the united states']\n",
      "[]\n",
      "440 []\n",
      "['united states']\n",
      "441 ['united states']\n",
      "[]\n",
      "442 []\n",
      "['venezuela']\n",
      "443 ['venezuela']\n",
      "['guardian', 'washington']\n",
      "444 ['guardian', 'washington']\n",
      "[]\n",
      "445 []\n",
      "[]\n",
      "446 ['day now', 'faller']\n",
      "['aplicacion del']\n",
      "447 []\n",
      "[]\n",
      "448 []\n",
      "['federal ndp']\n",
      "449 ['trudeau', 'federal ndp']\n",
      "['altamira', 'altamira', 'guaido']\n",
      "450 ['altamira', 'altamira', 'vz', 'guaido']\n",
      "['adm']\n",
      "451 ['adm. faller']\n",
      "[]\n",
      "452 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "453 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "454 []\n",
      "[]\n",
      "455 []\n",
      "['guardian', 'corbynista kryptonite']\n",
      "456 [\"guardian's soy boy\", 'socialist', 'corbynista kryptonite']\n",
      "['ar', 'washington']\n",
      "457 ['ar - 57', 'kent', 'washington', 'p90']\n",
      "[]\n",
      "458 ['spanish']\n",
      "['united states']\n",
      "459 ['united states']\n",
      "[]\n",
      "460 []\n",
      "[]\n",
      "461 []\n",
      "['latin', 'america']\n",
      "462 ['latin america']\n",
      "['cia', 'boeing', 'n881yv', 'bogota', 'february']\n",
      "463 ['cia', '21 air boeing 767-241 n881yv', 'bogota', '21air']\n",
      "['guaido', 'maduro']\n",
      "464 ['guaido', 'maduro']\n",
      "['pres of venezuela', 'venezuela']\n",
      "465 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "466 ['australian', 'socialism']\n",
      "[]\n",
      "467 []\n",
      "[]\n",
      "468 ['macron']\n",
      "[]\n",
      "469 []\n",
      "[]\n",
      "470 []\n",
      "[]\n",
      "471 []\n",
      "[]\n",
      "472 []\n",
      "['venezuelans']\n",
      "473 ['venezuelans']\n",
      "[]\n",
      "474 ['democratic', 'socialism']\n",
      "[]\n",
      "475 []\n",
      "['san diego']\n",
      "476 ['navy', 'san diego']\n",
      "['US']\n",
      "477 ['us', 'isis', 'us']\n",
      "[]\n",
      "478 []\n",
      "['msm', 'bbc']\n",
      "479 ['msm', 'bbc']\n",
      "[]\n",
      "480 []\n",
      "[]\n",
      "481 []\n",
      "['president', 'united states']\n",
      "482 ['president of the united states']\n",
      "[]\n",
      "483 []\n",
      "[]\n",
      "484 ['socialism']\n",
      "[]\n",
      "485 []\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "486 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['gnb in', 'caracas']\n",
      "487 ['gnb', 'caracas']\n",
      "[]\n",
      "488 []\n",
      "['usmc']\n",
      "489 ['usmc']\n",
      "[]\n",
      "490 []\n",
      "[]\n",
      "491 ['socialism']\n",
      "[]\n",
      "492 ['macron']\n",
      "['guaido', 'gnb', 'la carlota']\n",
      "493 ['pro-guaido', 'gnb', 'la carlota']\n",
      "[]\n",
      "494 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "495 ['pres of venezuela']\n",
      "['gnb in', 'caracas']\n",
      "496 ['gnb', 'caracas']\n",
      "[]\n",
      "497 []\n",
      "['president', 'obama']\n",
      "498 ['us', 'obama', 'hillary']\n",
      "[]\n",
      "499 []\n",
      "['pepe mujica']\n",
      "500 ['pepe mujica']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "501 ['trish regan primetime', 'u.s']\n",
      "[]\n",
      "502 []\n",
      "[]\n",
      "503 []\n",
      "[]\n",
      "504 ['macron']\n",
      "['guaido', 'maduro']\n",
      "505 ['guaido', 'maduro']\n",
      "[]\n",
      "506 ['btr', 'bolivar']\n",
      "['venezuelans']\n",
      "507 ['venezuelans']\n",
      "[]\n",
      "508 []\n",
      "[]\n",
      "509 []\n",
      "[]\n",
      "510 []\n",
      "['pat robertson', 'president nicolas maduro', 'christian']\n",
      "511 ['pat robertson', 'us', 'president nicolas maduro', 'christian', 'zionist']\n",
      "[]\n",
      "512 []\n",
      "[]\n",
      "513 []\n",
      "[]\n",
      "514 []\n",
      "['president', 'dictator raul castro', 'castro']\n",
      "515 ['dictator raul castro', 'minister of defense', 'castro', 'cuban']\n",
      "[]\n",
      "516 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "517 ['pres of venezuela']\n",
      "[]\n",
      "518 []\n",
      "[]\n",
      "519 ['cuban']\n",
      "[]\n",
      "520 []\n",
      "[]\n",
      "521 ['spanish']\n",
      "[]\n",
      "522 []\n",
      "['maduro death squads', 'faes']\n",
      "523 ['maduro death squads', 'faes']\n",
      "[]\n",
      "524 []\n",
      "[]\n",
      "525 []\n",
      "[]\n",
      "526 []\n",
      "[]\n",
      "527 []\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "528 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "[]\n",
      "529 ['spanish']\n",
      "[]\n",
      "530 []\n",
      "[]\n",
      "531 []\n",
      "['venezuelans']\n",
      "532 ['venezuelans']\n",
      "[]\n",
      "533 []\n",
      "[]\n",
      "534 ['starbucks commies']\n",
      "[]\n",
      "535 []\n",
      "['trump administration']\n",
      "536 ['trump administration', 'globalism']\n",
      "[]\n",
      "537 ['socialism']\n",
      "['pres of venezuela', 'venezuela']\n",
      "538 ['pres of venezuela']\n",
      "[]\n",
      "539 []\n",
      "[]\n",
      "540 []\n",
      "['u.s. war']\n",
      "541 ['senators', 'u.s. war']\n",
      "['pres of venezuela', 'venezuela']\n",
      "542 ['pres of venezuela']\n",
      "[]\n",
      "543 []\n",
      "[]\n",
      "544 []\n",
      "['gnb in', 'caracas']\n",
      "545 ['gnb', 'caracas']\n",
      "[]\n",
      "546 ['us']\n",
      "['altamira', 'caracas']\n",
      "547 ['altamira']\n",
      "[]\n",
      "548 []\n",
      "[]\n",
      "549 []\n",
      "['guaido']\n",
      "550 ['guaido']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "551 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "552 []\n",
      "['trump administration']\n",
      "553 ['trump administration', 'globalism']\n",
      "[]\n",
      "554 []\n",
      "[]\n",
      "555 ['macron']\n",
      "['trump administration']\n",
      "556 ['trump administration', 'globalism']\n",
      "[]\n",
      "557 []\n",
      "['utc']\n",
      "558 ['utc', 'vet']\n",
      "[]\n",
      "559 []\n",
      "['aplicacion del']\n",
      "560 []\n",
      "[]\n",
      "561 []\n",
      "[]\n",
      "562 []\n",
      "[]\n",
      "563 ['macron']\n",
      "['maduro', 'chavez', 'president']\n",
      "564 ['maduro', 'chavez']\n",
      "[]\n",
      "565 []\n",
      "['u.s', 'reuters']\n",
      "566 ['u.s', 'reuters']\n",
      "['venezuela']\n",
      "567 ['venezuela']\n",
      "[]\n",
      "568 ['leftist']\n",
      "['usmc']\n",
      "569 ['usmc']\n",
      "[]\n",
      "570 []\n",
      "[]\n",
      "571 []\n",
      "[]\n",
      "572 []\n",
      "[]\n",
      "573 []\n",
      "[]\n",
      "574 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "575 ['venezuela', 'maduro', 'venezuela']\n",
      "['u.s', 'venezuela']\n",
      "576 ['russia', 'u.s', 'venezuela']\n",
      "[]\n",
      "577 []\n",
      "['venezuela']\n",
      "578 []\n",
      "['cia']\n",
      "579 ['us', 'cia', 'guarimbas']\n",
      "[]\n",
      "580 []\n",
      "[]\n",
      "581 []\n",
      "[]\n",
      "582 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "583 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "584 ['macron']\n",
      "[]\n",
      "585 []\n",
      "[]\n",
      "586 []\n",
      "['venezuela']\n",
      "587 ['venezuela']\n",
      "[]\n",
      "588 ['macron']\n",
      "[]\n",
      "589 []\n",
      "['venezuela', 'maduro']\n",
      "590 ['venezuela', 'maduro']\n",
      "[]\n",
      "591 ['day now', 'faller']\n",
      "[]\n",
      "592 ['macron']\n",
      "[]\n",
      "593 []\n",
      "['venezuela']\n",
      "594 ['venezuela']\n",
      "[]\n",
      "595 []\n",
      "[]\n",
      "596 []\n",
      "[]\n",
      "597 []\n",
      "[]\n",
      "598 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "599 ['venezuela', 'maduro', 'venezuela']\n",
      "['rory carroll', 'guardian', 'caracas', 'bolivarian revolution']\n",
      "600 ['rory carroll', 'guardian', 'caracas', 'bolivarian revolution']\n",
      "['cia']\n",
      "601 ['cia']\n",
      "[]\n",
      "602 ['los teques']\n",
      "[]\n",
      "603 []\n",
      "[]\n",
      "604 ['los teques']\n",
      "[]\n",
      "605 []\n",
      "['caracas']\n",
      "606 ['venezuelan national guard', 'caracas']\n",
      "[]\n",
      "607 []\n",
      "[]\n",
      "608 []\n",
      "[]\n",
      "609 []\n",
      "['youtube', 'bing', 'guaido', 'caracas']\n",
      "610 ['youtube', 'bing', 'google', 'guaido', 'caracas']\n",
      "['big time']\n",
      "611 ['kinder', 'socialism']\n",
      "[]\n",
      "612 []\n",
      "[]\n",
      "613 []\n",
      "[]\n",
      "614 ['macron']\n",
      "['venezuela', 'president', 'nicolás maduro', 'maduro']\n",
      "615 ['venezuela', 'constitution', 'nicolás maduro', 'maduro']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "616 ['venezuela', 'maduro', 'venezuela']\n",
      "['uss theodore roosevelt']\n",
      "617 ['uss theodore roosevelt']\n",
      "[]\n",
      "618 []\n",
      "[]\n",
      "619 []\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "620 ['ret. brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "['president']\n",
      "621 []\n",
      "['venezuela']\n",
      "622 ['venezuela']\n",
      "['venezuela']\n",
      "623 ['venezuela', 'socialist']\n",
      "[]\n",
      "624 ['dictator', 'los teques']\n",
      "[]\n",
      "625 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "626 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "627 []\n",
      "[]\n",
      "628 ['macron']\n",
      "[]\n",
      "629 []\n",
      "[]\n",
      "630 []\n",
      "[]\n",
      "631 []\n",
      "[]\n",
      "632 []\n",
      "[]\n",
      "633 []\n",
      "['bay of pigs', 'allen dulles']\n",
      "634 ['bay of pigs', 'allen dulles']\n",
      "[]\n",
      "635 []\n",
      "['venezuela', 'president', 'nicolás maduro', 'maduro']\n",
      "636 ['venezuela', 'constitution', 'nicolás maduro', 'maduro']\n",
      "['pompeo']\n",
      "637 ['pompeo']\n",
      "['gnb in', 'caracas']\n",
      "638 ['gnb', 'caracas']\n",
      "['russians']\n",
      "639 ['russians']\n",
      "[]\n",
      "640 []\n",
      "[]\n",
      "641 []\n",
      "['venezuelans']\n",
      "642 []\n",
      "[]\n",
      "643 ['macron']\n",
      "['pres of venezuela', 'venezuela']\n",
      "644 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "645 []\n",
      "[]\n",
      "646 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "647 ['pres of venezuela']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "648 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "649 []\n",
      "['gnb in', 'caracas']\n",
      "650 ['gnb', 'caracas']\n",
      "['US', 'venezuelans']\n",
      "651 ['venezuelans']\n",
      "[]\n",
      "652 ['national guard']\n",
      "[]\n",
      "653 []\n",
      "[]\n",
      "654 []\n",
      "['defense secretary shanahan', 'bolton', 'weds', 'u.s', 'cuba']\n",
      "655 ['defense secretary shanahan', 'europe', 'bolton', 'nsc', 'weds', 'u.s', 'russia', 'cuba']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "656 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "[]\n",
      "657 []\n",
      "[]\n",
      "658 ['macron']\n",
      "['pres of venezuela', 'venezuela']\n",
      "659 ['pres of venezuela']\n",
      "['maduro', 'twitter']\n",
      "660 ['maduro', 'france', 'twitter']\n",
      "['venezuela', 'cia']\n",
      "661 ['venezuela', 'cia']\n",
      "['venezuela', 'president', 'armed forces']\n",
      "662 ['venezuela', 'national armed forces commander in chief', 'constitution']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "663 ['venezuela', 'maduro', 'venezuela']\n",
      "['US']\n",
      "664 ['us']\n",
      "[]\n",
      "665 ['national guards']\n",
      "[]\n",
      "666 []\n",
      "['guaido']\n",
      "667 ['guaido']\n",
      "[]\n",
      "668 ['starbucks commies']\n",
      "[]\n",
      "669 []\n",
      "[]\n",
      "670 []\n",
      "[]\n",
      "671 []\n",
      "['guaido']\n",
      "672 ['guaido']\n",
      "[]\n",
      "673 ['los teques']\n",
      "['pres of venezuela', 'venezuela']\n",
      "674 ['pres of venezuela']\n",
      "[]\n",
      "675 ['information and press department']\n",
      "[]\n",
      "676 []\n",
      "['u.s. war']\n",
      "677 ['representative', 'senators', 'u.s. war']\n",
      "['gnb in', 'caracas']\n",
      "678 ['gnb', 'caracas']\n",
      "[]\n",
      "679 []\n",
      "[]\n",
      "680 []\n",
      "['lima group', 'president', 'venezuelans']\n",
      "681 ['lima group', 'interim president', 'venezuelans']\n",
      "[]\n",
      "682 ['macron']\n",
      "[]\n",
      "683 ['macron']\n",
      "['maduro', 'american', 'maduro', 'america']\n",
      "684 ['maduro', 'american', 'coup', 'maduro', 'america']\n",
      "['armed forces']\n",
      "685 ['armed forces']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "686 ['trish regan primetime', 'u.s']\n",
      "['adm']\n",
      "687 ['adm. faller']\n",
      "[]\n",
      "688 []\n",
      "['venezuela', 'maduro']\n",
      "689 ['venezuela', 'maduro']\n",
      "[]\n",
      "690 []\n",
      "[]\n",
      "691 []\n",
      "[]\n",
      "692 []\n",
      "[]\n",
      "693 ['starbucks commies']\n",
      "[]\n",
      "694 []\n",
      "[]\n",
      "695 []\n",
      "[]\n",
      "696 ['socialism']\n",
      "[]\n",
      "697 ['macron']\n",
      "['ron paul', 'guaido', 'washington']\n",
      "698 ['ron paul', 'guaido', 'washington', 'venezuelan']\n",
      "['pres of venezuela', 'venezuela']\n",
      "699 ['pres of venezuela']\n",
      "['venezuelans', 'saudi arabia']\n",
      "700 ['venezuelans', 'saudi arabia']\n",
      "[]\n",
      "701 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "702 ['pres of venezuela']\n",
      "['venezuela', 'maduro']\n",
      "703 ['venezuela', 'maduro']\n",
      "[]\n",
      "704 ['socialism']\n",
      "[]\n",
      "705 []\n",
      "[]\n",
      "706 ['venezuelan']\n",
      "[]\n",
      "707 []\n",
      "[]\n",
      "708 []\n",
      "['gnb in', 'caracas']\n",
      "709 ['gnb', 'caracas']\n",
      "[]\n",
      "710 ['macron']\n",
      "[]\n",
      "711 ['macron']\n",
      "['ted', 'barr']\n",
      "712 ['ted', 'barr']\n",
      "[]\n",
      "713 ['macron']\n",
      "['utopia']\n",
      "714 ['leftists', 'socialist utopia', 'socialism']\n",
      "['guaido']\n",
      "715 ['guaido']\n",
      "[]\n",
      "716 []\n",
      "[]\n",
      "717 []\n",
      "[]\n",
      "718 []\n",
      "['united states']\n",
      "719 ['united states']\n",
      "[]\n",
      "720 []\n",
      "[]\n",
      "721 ['macron']\n",
      "['smith']\n",
      "722 ['smith']\n",
      "[]\n",
      "723 []\n",
      "[]\n",
      "724 []\n",
      "['caracas', 'punta cana']\n",
      "725 ['tc - tsr', 'caracas', 'punta cana']\n",
      "[]\n",
      "726 ['macron']\n",
      "[]\n",
      "727 []\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 []\n",
      "['US', 'venezuela']\n",
      "729 ['us', 'venezuela']\n",
      "[]\n",
      "730 []\n",
      "[]\n",
      "731 ['national guard', 'la guardia']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "732 ['venezuela', 'maduro', 'venezuela']\n",
      "['venezuela', 'twitter', 'correo del orinoco', 'va', 'twitter']\n",
      "733 [\"venezuela's culture minister\", 'twitter', 'correo del orinoco', 'va', 'twitter']\n",
      "[]\n",
      "734 ['socialism']\n",
      "[]\n",
      "735 ['starbucks commies']\n",
      "['ron paul', 'washington']\n",
      "736 ['ron paul', 'washington', 'venezuelan']\n",
      "[]\n",
      "737 []\n",
      "['east coast']\n",
      "738 ['east coast', 'west coast']\n",
      "[]\n",
      "739 ['national guard']\n",
      "['venezuela', 'paris']\n",
      "740 ['venezuela', 'paris']\n",
      "['president', 'obama']\n",
      "741 ['us', 'obama', 'hillary']\n",
      "['president', 'obama']\n",
      "742 ['us', 'obama', 'hillary']\n",
      "['gnb in', 'caracas']\n",
      "743 ['gnb', 'caracas']\n",
      "['president', 'oas']\n",
      "744 ['constitution', 'oas']\n",
      "[]\n",
      "745 []\n",
      "['venezuela', 'maduro']\n",
      "746 ['venezuela', 'maduro']\n",
      "[]\n",
      "747 ['socialism']\n",
      "[]\n",
      "748 ['dictator', 'los teques']\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "749 ['ret. brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "[]\n",
      "750 ['dictator', 'los teques']\n",
      "['venezuela']\n",
      "751 ['paramilitaries', 'guerrillas', 'military', 'venezuela']\n",
      "['paris']\n",
      "752 ['place d’italie', 'paris']\n",
      "[]\n",
      "753 []\n",
      "[]\n",
      "754 ['macron']\n",
      "['trump', 'libya', 'muslim brotherhood']\n",
      "755 ['trump', 'libya', 'muslim brotherhood', 'russia', 'isis', 'baghdadi']\n",
      "['venezuela', 'maduro']\n",
      "756 ['venezuela', 'maduro']\n",
      "[]\n",
      "757 []\n",
      "['usmc']\n",
      "758 ['usmc']\n",
      "[]\n",
      "759 []\n",
      "[]\n",
      "760 []\n",
      "['bolton', 'trump', 'US']\n",
      "761 ['bolton', 'trump', 'us']\n",
      "[]\n",
      "762 ['blackwater']\n",
      "[]\n",
      "763 []\n",
      "[]\n",
      "764 []\n",
      "[]\n",
      "765 ['spanish']\n",
      "['pres of venezuela', 'venezuela']\n",
      "766 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "767 []\n",
      "[]\n",
      "768 []\n",
      "[]\n",
      "769 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "770 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "771 []\n",
      "['guardian', 'corbynista kryptonite']\n",
      "772 [\"guardian's soy boy\", 'socialist', 'corbynista kryptonite']\n",
      "[]\n",
      "773 ['stalin']\n",
      "['adm', 'venezuela']\n",
      "774 ['adm. faller', 'venezuela']\n",
      "[]\n",
      "775 []\n",
      "[]\n",
      "776 []\n",
      "[]\n",
      "777 []\n",
      "[]\n",
      "778 []\n",
      "[]\n",
      "779 []\n",
      "[]\n",
      "780 []\n",
      "[]\n",
      "781 []\n",
      "['venezuela', 'latin', 'america', 'bernie sanders', 'united states']\n",
      "782 ['venezuela', 'latin america', 'democrat socialists', 'bernie sanders', 'united states']\n",
      "[]\n",
      "783 []\n",
      "[]\n",
      "784 []\n",
      "[]\n",
      "785 []\n",
      "['venezuelans']\n",
      "786 ['venezuelans']\n",
      "[]\n",
      "787 ['1989 caracazo massacres', 'neoliberal']\n",
      "['gnb in', 'caracas']\n",
      "788 ['gnb', 'caracas']\n",
      "[]\n",
      "789 []\n",
      "[]\n",
      "790 []\n",
      "[]\n",
      "791 ['spanish']\n",
      "['pepe mujica', 'maduro']\n",
      "792 ['pepe mujica', 'left', 'maduro']\n",
      "['venezuela']\n",
      "793 ['venezuela']\n",
      "[]\n",
      "794 []\n",
      "[]\n",
      "795 []\n",
      "['canada']\n",
      "796 ['canada']\n",
      "[]\n",
      "797 []\n",
      "[]\n",
      "798 ['macron']\n",
      "[]\n",
      "799 []\n",
      "['caracas']\n",
      "800 ['caracas']\n",
      "['msnbc']\n",
      "801 ['msnbc']\n",
      "['venezuela', 'maduro']\n",
      "802 ['venezuela', 'maduro']\n",
      "[]\n",
      "803 []\n",
      "[]\n",
      "804 ['puerto ordaz']\n",
      "[]\n",
      "805 []\n",
      "['moa', 'guaido', 'snookered', 'white house']\n",
      "806 ['guaido', 'white house']\n",
      "['venezuela embassy', 'maduro', 'hilton']\n",
      "807 ['venezuela embassy', 'maduro', 'correspondence dinner', 'hilton']\n",
      "[]\n",
      "808 []\n",
      "['guardian', 'corbynista kryptonite']\n",
      "809 [\"guardian's soy boy\", 'corbynista kryptonite']\n",
      "['russians']\n",
      "810 ['russians']\n",
      "['venezuela', 'armed forces']\n",
      "811 ['venezuela', 'armed forces']\n",
      "[]\n",
      "812 []\n",
      "[]\n",
      "813 []\n",
      "[]\n",
      "814 ['socialism']\n",
      "[]\n",
      "815 []\n",
      "['maduro']\n",
      "816 ['maduro']\n",
      "[]\n",
      "817 []\n",
      "[]\n",
      "818 ['macron']\n",
      "['guardian', 'corbynista kryptonite']\n",
      "819 [\"guardian's soy boy\", 'socialist', 'corbynista kryptonite']\n",
      "[]\n",
      "820 ['macron']\n",
      "['u.s', 'u.s', 'venezuela']\n",
      "821 ['federal aviation administration', 'u.s', 'u.s', 'venezuela']\n",
      "['president']\n",
      "822 []\n",
      "[]\n",
      "823 ['los teques']\n",
      "['adm', 'venezuela']\n",
      "824 ['adm. faller', 'venezuela']\n",
      "['guardian', 'corbynista kryptonite']\n",
      "825 [\"guardian's soy boy\", 'socialist', 'corbynista kryptonite']\n",
      "['caracas']\n",
      "826 ['caracas']\n",
      "[]\n",
      "827 []\n",
      "[]\n",
      "828 ['macron']\n",
      "['u.s. secretary of state']\n",
      "829 ['u.s. secretary of state', 'russian foreign minister']\n",
      "['guaido']\n",
      "830 ['guaido']\n",
      "[]\n",
      "831 []\n",
      "[]\n",
      "832 ['macron']\n",
      "['msnbc']\n",
      "833 ['msnbc']\n",
      "[]\n",
      "834 ['democratic socialism']\n",
      "[]\n",
      "835 []\n",
      "[]\n",
      "836 []\n",
      "['trump administration']\n",
      "837 ['trump administration', 'globalism']\n",
      "['pres of venezuela', 'venezuela']\n",
      "838 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "839 []\n",
      "['gnb']\n",
      "840 ['gnb']\n",
      "[]\n",
      "841 []\n",
      "['ar', 'washington']\n",
      "842 ['ar-57', 'kent', 'washington', 'p90']\n",
      "[]\n",
      "843 []\n",
      "['juan guaidó', 'maduro']\n",
      "844 ['juan guaidó', 'maduro']\n",
      "['gnb in', 'caracas']\n",
      "845 ['gnb', 'caracas']\n",
      "['gnb in', 'caracas']\n",
      "846 ['gnb', 'caracas']\n",
      "[]\n",
      "847 []\n",
      "[]\n",
      "848 []\n",
      "['dow', 'sp', 'powell', 'fred imbert']\n",
      "849 ['dow', 'sp 500', 'powell', 'fred imbert']\n",
      "[]\n",
      "850 []\n",
      "[]\n",
      "851 ['macron']\n",
      "['pres of venezuela', 'venezuela']\n",
      "852 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "853 []\n",
      "['trump administration']\n",
      "854 ['trump administration', 'globalism']\n",
      "[]\n",
      "855 ['socialism']\n",
      "['caracas']\n",
      "856 ['caracas']\n",
      "['lavrov']\n",
      "857 ['lavrov']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "858 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "859 ['venezuela', 'maduro', 'venezuela']\n",
      "['aplicacion del']\n",
      "860 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "861 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "862 []\n",
      "[]\n",
      "863 ['socialism']\n",
      "['trump administration']\n",
      "864 ['trump administration', 'globalism']\n",
      "['armed forces', 'miraflores', 'maduro']\n",
      "865 ['armed forces', 'miraflores', 'nicholas maduro']\n",
      "[]\n",
      "866 []\n",
      "[]\n",
      "867 ['macron']\n",
      "[]\n",
      "868 ['los teques']\n",
      "[]\n",
      "869 []\n",
      "[]\n",
      "870 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "871 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "872 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "873 ['pres of venezuela', 'venezuela']\n",
      "['caracas', 'catalonia']\n",
      "874 ['spanish', 'caracas', 'catalonia']\n",
      "[]\n",
      "875 []\n",
      "[]\n",
      "876 []\n",
      "[]\n",
      "877 []\n",
      "[]\n",
      "878 []\n",
      "['trump administration']\n",
      "879 ['trump administration', 'globalism']\n",
      "[]\n",
      "880 []\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "881 ['venezuela', 'maduro', 'venezuela']\n",
      "[]\n",
      "882 []\n",
      "['president', 'united states']\n",
      "883 ['president of the united states']\n",
      "[]\n",
      "884 []\n",
      "[]\n",
      "885 []\n",
      "[]\n",
      "886 []\n",
      "[]\n",
      "887 ['macron']\n",
      "[]\n",
      "888 []\n",
      "['lavrov', 'u.s']\n",
      "889 ['lavrov', 'russian', 'u.s']\n",
      "[]\n",
      "890 []\n",
      "['gnb in', 'caracas']\n",
      "891 ['gnb', 'caracas']\n",
      "[]\n",
      "892 ['macron']\n",
      "[]\n",
      "893 []\n",
      "[]\n",
      "894 ['assad']\n",
      "[]\n",
      "895 ['opposition', 'maracaibo']\n",
      "['rory carroll', 'guardian', 'caracas', 'bolivarian revolution']\n",
      "896 ['rory carroll', 'guardian', 'caracas', 'bolivarian revolution']\n",
      "[]\n",
      "897 []\n",
      "['la carlota']\n",
      "898 ['la carlota']\n",
      "[]\n",
      "899 ['remain', 'eu', 'france']\n",
      "[]\n",
      "900 []\n",
      "[]\n",
      "901 ['diplomacy', 'us government']\n",
      "['maduro']\n",
      "902 ['venezuelan army', 'maduro']\n",
      "['gnb in caracas']\n",
      "903 ['gnb', 'caracas']\n",
      "['pres of venezuela', 'venezuela']\n",
      "904 ['pres of venezuela', 'venezuela']\n",
      "['east coast']\n",
      "905 ['east coast', 'west coast']\n",
      "[]\n",
      "906 []\n",
      "[]\n",
      "907 ['socialism']\n",
      "[]\n",
      "908 []\n",
      "[]\n",
      "909 []\n",
      "['president', 'juan guaido']\n",
      "910 ['juan guaido']\n",
      "[]\n",
      "911 []\n",
      "['el julio perotti diario']\n",
      "912 ['el julio perotti dario']\n",
      "[]\n",
      "913 ['macron']\n",
      "['venezuela']\n",
      "914 ['venezuela']\n",
      "['canada', 'u.s', 'u.s']\n",
      "915 ['u.s', 'u.s']\n",
      "[]\n",
      "916 ['socialism']\n",
      "[]\n",
      "917 []\n",
      "['armed forces', 'fn fal', 'juan guadió', 'american']\n",
      "918 ['national bolivarian armed forces', 'ak-103', 'fn fal', 'fn fnc', 'us', 'juan guadió', 'north-american assault rifle m-16']\n",
      "['usmc']\n",
      "919 ['usmc']\n",
      "[]\n",
      "920 []\n",
      "[]\n",
      "921 ['socialist']\n",
      "[]\n",
      "922 ['national guard']\n",
      "['u.s']\n",
      "923 ['u.s']\n",
      "[]\n",
      "924 []\n",
      "['venezuela']\n",
      "925 ['paramilitaries', 'guerrillas', 'military', 'drug trafficiking', 'venezuela']\n",
      "[]\n",
      "926 []\n",
      "['russiagaters']\n",
      "927 ['russiagaters', 'militant zionists', 'qanon cultists']\n",
      "['east coast']\n",
      "928 ['east coast', 'west coast']\n",
      "[]\n",
      "929 []\n",
      "[]\n",
      "930 ['macron']\n",
      "[]\n",
      "931 []\n",
      "[]\n",
      "932 []\n",
      "[]\n",
      "933 []\n",
      "[]\n",
      "934 []\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "935 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['pres of venezuela', 'venezuela']\n",
      "936 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "937 []\n",
      "['adm', 'venezuela']\n",
      "938 ['adm. faller', 'venezuela']\n",
      "['pepe mujica']\n",
      "939 ['pepe mujica']\n",
      "['president', 'united states']\n",
      "940 ['president of the united states']\n",
      "[]\n",
      "941 ['osint', 'eisenhower', 'roosevelt']\n",
      "['la marea naranja']\n",
      "942 ['la marea naranja']\n",
      "['president']\n",
      "943 []\n",
      "[]\n",
      "944 []\n",
      "['venezuela']\n",
      "945 []\n",
      "[]\n",
      "946 ['national guard']\n",
      "[]\n",
      "947 []\n",
      "['brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "948 ['ret. brigadier general', 'u.s', 'u.s', 'juan guaido']\n",
      "['president', 'guaidó']\n",
      "949 ['president guaidó']\n",
      "['trump administration']\n",
      "950 ['trump administration', 'globalism']\n",
      "[]\n",
      "951 []\n",
      "['trump administration']\n",
      "952 ['trump administration', 'globalism']\n",
      "[]\n",
      "953 ['isp cantv']\n",
      "['altamira']\n",
      "954 ['altamira']\n",
      "['vzla']\n",
      "955 ['vzla']\n",
      "['trump administration']\n",
      "956 ['trump administration', 'globalism']\n",
      "[]\n",
      "957 []\n",
      "['venezuelans', 'saudi arabia']\n",
      "958 ['venezuelans', 'saudi arabia']\n",
      "[]\n",
      "959 ['macron']\n",
      "['US']\n",
      "960 ['us', 'isis', 'us']\n",
      "['pres of venezuela', 'venezuela']\n",
      "961 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "962 []\n",
      "['venezuela']\n",
      "963 []\n",
      "[]\n",
      "964 []\n",
      "[]\n",
      "965 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "966 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "967 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "968 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "969 []\n",
      "['maduro', 'maduro']\n",
      "970 ['trudeau', 'maduro', 'maduro']\n",
      "['trump administration']\n",
      "971 ['trump administration', 'globalism']\n",
      "['vietnams', 'iraq']\n",
      "972 ['vietnams', 'iraq']\n",
      "['venezuela', 'maduro']\n",
      "973 ['venezuela', 'maduro']\n",
      "['hospital maria auxiliadora']\n",
      "974 ['hospital maria auxiliadora', 'minister of health zulema tomás']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "975 ['trish regan primetime', 'u.s']\n",
      "[]\n",
      "976 []\n",
      "['adm', 'iran', 'cuba']\n",
      "977 ['adm. faller', 'iran', 'tehran', 'cuba']\n",
      "['president']\n",
      "978 []\n",
      "[]\n",
      "979 []\n",
      "[]\n",
      "980 ['macron']\n",
      "[]\n",
      "981 ['macron']\n",
      "['adm', 'iran', 'cuba']\n",
      "982 ['adm. faller', 'iran', 'tehran', 'cuba']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "983 ['trish regan primetime', 'u.s']\n",
      "['altamira']\n",
      "984 ['altamira']\n",
      "[]\n",
      "985 []\n",
      "[]\n",
      "986 []\n",
      "['usmc']\n",
      "987 ['usmc']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "988 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['caracas', 'punta cana']\n",
      "989 ['caracas', 'punta cana']\n",
      "['maduro', 'american', 'maduro', 'america']\n",
      "990 ['maduro', 'american', 'maduro', 'america']\n",
      "['pres of venezuela', 'venezuela']\n",
      "991 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "992 []\n",
      "['u.s']\n",
      "993 ['u.s']\n",
      "[]\n",
      "994 []\n",
      "[]\n",
      "995 []\n",
      "[]\n",
      "996 []\n",
      "[]\n",
      "997 []\n",
      "[]\n",
      "998 ['venezuelan']\n",
      "[]\n",
      "999 ['socialism']\n",
      "[]\n",
      "1000 []\n",
      "[]\n",
      "1001 []\n",
      "['saudi arabia']\n",
      "1002 ['russia', 'saudi arabia', 'shia']\n",
      "['gnb in', 'caracas']\n",
      "1003 ['gnb', 'caracas']\n",
      "[]\n",
      "1004 []\n",
      "[]\n",
      "1005 ['democratic socialism']\n",
      "[]\n",
      "1006 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "1007 ['pres of venezuela', 'venezuela']\n",
      "['venezuela']\n",
      "1008 ['venezuela']\n",
      "[]\n",
      "1009 []\n",
      "['russians']\n",
      "1010 []\n",
      "[]\n",
      "1011 ['macron']\n",
      "['trump administration']\n",
      "1012 ['trump administration', 'globalism']\n",
      "['venezuela', 'maduro', 'venezuela']\n",
      "1013 ['venezuela', 'maduro', 'venezuela']\n",
      "['u.s', 'venezuela']\n",
      "1014 ['u.s', 'venezuela']\n",
      "[]\n",
      "1015 []\n",
      "['venezuelans']\n",
      "1016 ['venezuelans']\n",
      "['pres of venezuela', 'venezuela']\n",
      "1017 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "1018 []\n",
      "[]\n",
      "1019 []\n",
      "['gnb in', 'caracas']\n",
      "1020 ['gnb', 'caracas']\n",
      "[]\n",
      "1021 []\n",
      "[]\n",
      "1022 []\n",
      "['venezuela']\n",
      "1023 ['venezuela', 'socialist dictator']\n",
      "[]\n",
      "1024 ['opposition']\n",
      "['pres of venezuela', 'venezuela']\n",
      "1025 ['pres of venezuela', 'venezuela']\n",
      "['president', 'nicolas maduro', 'maduro', 'cuba', 'narco']\n",
      "1026 ['nicolas maduro', 'maduro', 'cuba', 'narco terrorist regime']\n",
      "['venezuelans']\n",
      "1027 ['venezuelans']\n",
      "['altamira', 'altamira', 'guaido']\n",
      "1028 ['altamira', 'altamira', 'vz', 'guaido']\n",
      "['ar', 'washington']\n",
      "1029 ['ar-57', 'kent', 'washington', 'p90']\n",
      "[]\n",
      "1030 ['macron']\n",
      "[]\n",
      "1031 []\n",
      "['youtube', 'bing', 'guaido', 'caracas']\n",
      "1032 ['youtube', 'bing', 'google', 'guaido', 'caracas']\n",
      "['trish regan primetime', 'u.s', 'president']\n",
      "1033 ['trish regan primetime', 'u.s']\n",
      "['trump administration']\n",
      "1034 ['trump administration', 'globalism']\n",
      "[]\n",
      "1035 []\n",
      "['cia']\n",
      "1036 ['irish', 'cia']\n",
      "[]\n",
      "1037 []\n",
      "[]\n",
      "1038 []\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "1039 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['vietnams', 'iraq']\n",
      "1040 ['vietnams', 'iraq']\n",
      "[]\n",
      "1041 []\n",
      "[]\n",
      "1042 ['macron']\n",
      "['pepe mujica']\n",
      "1043 ['pepe mujica']\n",
      "['cyclone kenneth', 'mozambique', 'lebo diseko', 'pemba']\n",
      "1044 ['cyclone kenneth', 'mozambique', 'lebo diseko', 'pemba']\n",
      "[]\n",
      "1045 []\n",
      "[]\n",
      "1046 []\n",
      "['russians']\n",
      "1047 ['russians']\n",
      "[]\n",
      "1048 []\n",
      "[]\n",
      "1049 ['macron']\n",
      "['pepe mujica', 'maduro']\n",
      "1050 ['pepe mujica', 'left', 'maduro']\n",
      "[]\n",
      "1051 []\n",
      "[]\n",
      "1052 []\n",
      "[]\n",
      "1053 []\n",
      "[]\n",
      "1054 ['day now', 'faller']\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "1055 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "['gnb in', 'caracas']\n",
      "1056 ['gnb', 'caracas']\n",
      "[]\n",
      "1057 []\n",
      "[]\n",
      "1058 []\n",
      "[]\n",
      "1059 []\n",
      "['venezuela']\n",
      "1060 ['venezuela', 'socialist']\n",
      "[]\n",
      "1061 ['macron']\n",
      "['venezuela', 'venezuela', 'venezuelans']\n",
      "1062 ['venezuela', 'venezuela', 'venezuelans']\n",
      "['united states']\n",
      "1063 ['united states']\n",
      "[]\n",
      "1064 []\n",
      "['adm']\n",
      "1065 ['adm. faller']\n",
      "['usmc']\n",
      "1066 ['usmc']\n",
      "[]\n",
      "1067 []\n",
      "[]\n",
      "1068 ['socialism']\n",
      "[]\n",
      "1069 []\n",
      "['pres of venezuela', 'venezuela']\n",
      "1070 ['pres of venezuela', 'venezuela']\n",
      "['venezuela', 'maduro']\n",
      "1071 ['venezuela', 'maduro']\n",
      "[]\n",
      "1072 []\n",
      "['january', 'maduro', 'president']\n",
      "1073 ['maduro']\n",
      "['gnb in', 'caracas']\n",
      "1074 ['gnb', 'caracas']\n",
      "[]\n",
      "1075 []\n",
      "['american', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "1076 ['american', 'us', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine']\n",
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077 []\n",
      "[]\n",
      "1078 []\n",
      "['russians']\n",
      "1079 ['russians']\n",
      "[]\n",
      "1080 ['starbucks commies']\n",
      "['msnbc']\n",
      "1081 ['msnbc']\n",
      "['u.s']\n",
      "1082 ['u.s']\n",
      "[]\n",
      "1083 ['spanish']\n",
      "[]\n",
      "1084 []\n",
      "[]\n",
      "1085 []\n",
      "[]\n",
      "1086 []\n",
      "[]\n",
      "1087 []\n",
      "[]\n",
      "1088 ['ves', 'usd', 'ves', 'usd']\n",
      "[]\n",
      "1089 ['biz coach news']\n",
      "[]\n",
      "1090 []\n",
      "[]\n",
      "1091 ['macron']\n",
      "['president']\n",
      "1092 []\n",
      "[]\n",
      "1093 []\n",
      "['united states']\n",
      "1094 ['united states']\n",
      "[]\n",
      "1095 []\n",
      "[]\n",
      "1096 []\n",
      "['utc']\n",
      "1097 ['utc', 'vet']\n",
      "['president', 'nicolas maduro', 'maduro', 'cuba', 'narco']\n",
      "1098 ['nicolas maduro', 'maduro', 'cuba', 'narco terrorist regime']\n",
      "['uk', 'US']\n",
      "1099 ['uk', 'us', 'venezuelan', 'us']\n",
      "['pres of venezuela', 'venezuela']\n",
      "1100 ['pres of venezuela', 'venezuela']\n",
      "[]\n",
      "1101 []\n",
      "[]\n",
      "1102 []\n",
      "['adm', 'china', 'venezuela']\n",
      "1103 ['adm. faller', 'china', 'venezuela', 'venezuelan', 'venezuelan']\n",
      "[]\n",
      "1104 []\n",
      "[]\n",
      "1105 []\n",
      "[]\n",
      "1365 1064\n",
      "23 1041 1342\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "\n",
    "all_annotations=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted.iterrows():\n",
    "    output_mentions_list+=flatten(row.output_mentions.tolist(),[])\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    \n",
    "#     output_mentions_list=flatten(complete_tweet_dataframe_grouped_df_sorted[complete_tweet_dataframe_grouped_df_sorted.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     all_annotations+=annotated_mention_list\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner=len(output_mentions_list)\n",
    "#     total_tagged+=len(output_mentions_list)\n",
    "#     total_annotations+=len(annotated_mention_list)\n",
    "    \n",
    "#     while(annotated_mention_list):\n",
    "#         if(len(output_mentions_list)):\n",
    "#             annotated_candidate= annotated_mention_list.pop()\n",
    "#             if(annotated_candidate in output_mentions_list):\n",
    "#                 output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "#                 tp_counter_inner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "#     fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count+=tp_counter_inner\n",
    "#     false_positive_count+=fp_counter_inner\n",
    "#     false_negative_count+=fn_counter_inner\n",
    "\n",
    "# print(total_annotations,total_tagged)\n",
    "# print(true_positive_count,false_positive_count,false_negative_count)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021616541353383457 0.01684981684981685 0.01893783449979415\n",
      "100.66425275802612\n",
      "104.03408288955688\n"
     ]
    }
   ],
   "source": [
    "precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "f_measure=2*(precision*recall)/(precision+recall)\n",
    "print(precision,recall,f_measure)\n",
    "\n",
    "print(time2-time1)\n",
    "print(time3-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-a150bcecdd2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-a150bcecdd2b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2425 487 905\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#3K\n",
    "#SVM\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#RF\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#LR\n",
    "2415 460 915\n",
    "0.84 0.7252252252252253 0.7784045124899275\n",
    "\n",
    "1K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ritter- TwitterNLP in Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "ritter_annotator=pd.read_csv(\"/Users/satadisha/Documents/GitHub/my-baseline-setup/ritter_tweets_3k_annotated_output.csv\",sep =',',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3067\n",
      "['ID', 'HashTags', 'TweetText', 'Output', 'mentions_other', 'User', 'ritter_candidates', 'calai_candidates', 'stanford_candidates']\n"
     ]
    }
   ],
   "source": [
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "print(len(ritter_annotator))\n",
    "print(ritter_annotator.columns.tolist())\n",
    "CTrie_ritter=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n"
     ]
    }
   ],
   "source": [
    "ritter_annotated_candidates=ritter_annotator['Output'].tolist()\n",
    "for candidate in ritter_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_ritter.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesinRitterTrie=CTrie_ritter.displayTrie(\"\",[])\n",
    "print(len(candidatesinRitterTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 645\n",
      "-0.23599405851453567\n",
      "For entities:  (511, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "For entities:  (511, 6)\n",
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4566 incomplete tweets:  157\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_Ritter = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_Ritter, converted_candidates_w_Ritter, complete_tweet_dataframe_grouped_df_sorted_w_Ritter= Phase2_w_Ritter.executor(max_batch_value,tweet_sentence_df,CTrie_ritter,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[RussiaGate]</td>\n",
       "      <td>[JanKimbrough]</td>\n",
       "      <td>[REPORT: FBI Obtained FISA Warrant For Trump A...</td>\n",
       "      <td>[fbi::*3*||fisa warrant::*5*6*||trump aide::*8...</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(fbi, g), (fisa, g), (trump, g), (the russia...</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[[the russian government]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[zi_cam]</td>\n",
       "      <td>[BUSTED....]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[sicsemp4ever, sicsemp4ever]</td>\n",
       "      <td>[Carter Page is trending but 3 weeks ago there...</td>\n",
       "      <td>[carter page::*1*2*||team trump::*15*16*||, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(carter page, g), (fisa, g), (trump, g)], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[raponikoff]</td>\n",
       "      <td>[Looks like it might be time for some more tom...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[MsEntropy]</td>\n",
       "      <td>[Sean Spicer on Carter Page:]</td>\n",
       "      <td>[sean spicer on carter page::*1*2*3*4*5*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(sean spicer, g), (carter page, g)]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       index entry_batch  sentID      hashtags  \\\n",
       "0        0       [nan]         [0]     [0]  [RussiaGate]   \n",
       "1        1       [nan]         [0]     [0]            []   \n",
       "2        2  [nan, nan]      [0, 0]  [0, 1]          [, ]   \n",
       "3        3       [nan]         [0]     [0]            []   \n",
       "4        4       [nan]         [0]     [0]            []   \n",
       "\n",
       "                           user  \\\n",
       "0                [JanKimbrough]   \n",
       "1                      [zi_cam]   \n",
       "2  [sicsemp4ever, sicsemp4ever]   \n",
       "3                  [raponikoff]   \n",
       "4                   [MsEntropy]   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [REPORT: FBI Obtained FISA Warrant For Trump A...   \n",
       "1                                       [BUSTED....]   \n",
       "2  [Carter Page is trending but 3 weeks ago there...   \n",
       "3  [Looks like it might be time for some more tom...   \n",
       "4                      [Sean Spicer on Carter Page:]   \n",
       "\n",
       "                                    phase1Candidates annotation  \\\n",
       "0  [fbi::*3*||fisa warrant::*5*6*||trump aide::*8...       [[]]   \n",
       "1                                                 []       [[]]   \n",
       "2      [carter page::*1*2*||team trump::*15*16*||, ]   [[], []]   \n",
       "3                                                 []       [[]]   \n",
       "4        [sean spicer on carter page::*1*2*3*4*5*||]       [[]]   \n",
       "\n",
       "  stanford_candidates                   output_mentions  completeness  \\\n",
       "0                [[]]              [[fbi, fisa, trump]]       [False]   \n",
       "1                [[]]                              [[]]        [True]   \n",
       "2            [[], []]  [[carter page, fisa, trump], []]  [True, True]   \n",
       "3                [[]]                              [[]]        [True]   \n",
       "4                [[]]      [[sean spicer, carter page]]        [True]   \n",
       "\n",
       "  current_minus_entry                              candidates_with_label  \\\n",
       "0               [0.0]  [[(fbi, g), (fisa, g), (trump, g), (the russia...   \n",
       "1               [0.0]                                               [[]]   \n",
       "2          [0.0, 0.0]    [[(carter page, g), (fisa, g), (trump, g)], []]   \n",
       "3               [0.0]                                               [[]]   \n",
       "4               [0.0]             [[(sean spicer, g), (carter page, g)]]   \n",
       "\n",
       "               only_good_candidates        ambiguous_candidates  \n",
       "0              [[fbi, fisa, trump]]  [[the russian government]]  \n",
       "1                              [[]]                        [[]]  \n",
       "2  [[carter page, fisa, trump], []]                    [[], []]  \n",
       "3                              [[]]                        [[]]  \n",
       "4      [[sean spicer, carter page]]                        [[]]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted_w_Ritter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2770\n",
      "2385 385 437\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_ritter=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Ritter[complete_tweet_dataframe_grouped_df_sorted_w_Ritter.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610108303249098 0.8451452870304749 0.8530042918454936\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM\n",
    "0.8605577689243028 0.8428520752039731 0.8516129032258064\n",
    "\n",
    "#RF\n",
    "0.8614435981138919 0.8433948863636364 0.8523237035707877\n",
    "\n",
    "#LR\n",
    "0.8610108303249098 0.8451452870304749 0.8530042918454936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaguilar as Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n",
    "\n",
    "\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/tweebo-parser/tweets_3k_annotated.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTrie_gaguilar=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795\n",
      "2724\n",
      "771\n"
     ]
    }
   ],
   "source": [
    "file_text=f.read()\n",
    "\n",
    "output_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n'))) #conll\n",
    "print(len(output_sentences))\n",
    "gaguilar_annotated_candidates=[]\n",
    "for line in output_sentences:\n",
    "    if(line):\n",
    "        tabs=line.split('\\t')\n",
    "        if(tabs):\n",
    "            for candidate in tabs:\n",
    "                gaguilar_annotated_candidates.append(candidate)\n",
    "print(len(gaguilar_annotated_candidates))\n",
    "for candidate in gaguilar_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_gaguilar.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesingaguilarTrie=CTrie_gaguilar.displayTrie(\"\",[])\n",
    "print(len(candidatesingaguilarTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 731\n",
      "-0.23514899240820253\n",
      "For entities:  (590, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "For entities:  (590, 6)\n",
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4593 incomplete tweets:  130\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_gaguilar = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_gaguilar, converted_candidates_w_gaguilar, complete_tweet_dataframe_grouped_df_sorted_w_gaguilar= Phase2_w_gaguilar.executor(max_batch_value,tweet_sentence_df,CTrie_gaguilar,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2986\n",
      "2484 502 471\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_gaguilar=0\n",
    "false_positive_count_gaguilar=0\n",
    "false_negative_count_gaguilar=0\n",
    "\n",
    "total_annotations_gaguilar=0\n",
    "total_tagged_gaguilar=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_gaguilar=[]\n",
    "    tp_counter_inner_gaguilar=0\n",
    "    fp_counter_inner_gaguilar=0\n",
    "    fn_counter_inner_gaguilar=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_gaguilar=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_gaguilar.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_gaguilar=flatten(complete_tweet_dataframe_grouped_df_sorted_w_gaguilar[complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_gaguilar=len(output_mentions_list_gaguilar)\n",
    "    total_tagged_gaguilar+=len(output_mentions_list_gaguilar)\n",
    "    total_annotations_gaguilar+=len(annotated_mention_list_gaguilar)\n",
    "    \n",
    "    while(annotated_mention_list_gaguilar):\n",
    "        if(len(output_mentions_list_gaguilar)):\n",
    "            annotated_candidate= annotated_mention_list_gaguilar.pop()\n",
    "            if(annotated_candidate in output_mentions_list_gaguilar):\n",
    "                output_mentions_list_gaguilar.pop(output_mentions_list_gaguilar.index(annotated_candidate))\n",
    "                tp_counter_inner_gaguilar+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_gaguilar.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_gaguilar.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_gaguilar=len(unrecovered_annotated_mention_list_gaguilar)\n",
    "    fp_counter_inner_gaguilar=all_postitive_counter_inner_gaguilar- tp_counter_inner_gaguilar\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_gaguilar+=tp_counter_inner_gaguilar\n",
    "    false_positive_count_gaguilar+=fp_counter_inner_gaguilar\n",
    "    false_negative_count_gaguilar+=fn_counter_inner_gaguilar\n",
    "\n",
    "print(total_annotations_gaguilar,total_tagged_gaguilar)\n",
    "print(true_positive_count_gaguilar,false_positive_count_gaguilar,false_negative_count_gaguilar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8318821165438715 0.8406091370558376 0.8362228581046962\n"
     ]
    }
   ],
   "source": [
    "precision_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_positive_count_gaguilar)\n",
    "recall_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_negative_count_gaguilar)\n",
    "f_measure_gaguilar=2*(precision_gaguilar*recall_gaguilar)/(precision_gaguilar+recall_gaguilar)\n",
    "print(precision_gaguilar,recall_gaguilar,f_measure_gaguilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LR\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#RF\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#SVM\n",
    "0.8318821165438715 0.8406091370558376 0.8362228581046962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "3374 3849\n",
      "2501 1348 673\n"
     ]
    }
   ],
   "source": [
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# neuroner_file = open('mentions_output_tweets_3K.txt', 'r') \n",
    "# neuroner_lines = neuroner_file.readlines()\n",
    "# print(len(neuroner_lines))\n",
    "# line_count=0\n",
    "# neuroner_annotated_candidates=[]\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "#     neuroner_output=neuroner_lines[line_count]\n",
    "#     output_mentions_list_neuroner=[candidate.lower().strip(string.punctuation).strip() for candidate in neuroner_output.split(',') if(candidate.strip(string.punctuation).strip())]\n",
    "#     neuroner_annotated_candidates.extend(output_mentions_list_neuroner)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner - tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "    \n",
    "#     line_count+=1\n",
    "    \n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6497791634190699 0.7879647132955262 0.7122312402107361\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroNER as Phase I Entity Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "[]\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "# tweet_sentence_df_2nd_copy=tweet_sentence_df.copy(deep=True)\n",
    "# CTrie_neuroner=trie.Trie(\"ROOT\")\n",
    "# print(len(neuroner_annotated_candidates))\n",
    "# print(phase2stopwordList)\n",
    "\n",
    "# for candidateText in neuroner_annotated_candidates:\n",
    "# #     print(candidateText)\n",
    "#     if(candidateText not in all_stopwords):\n",
    "#         CTrie_neuroner.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "# candidatesinNeuronerTrie=CTrie_neuroner.displayTrie(\"\",[])\n",
    "# print(len(candidatesinNeuronerTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4721 4721 1057\n",
      "-0.2661675732774245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4525 incomplete tweets:  196\n",
      "16\n",
      "16\n",
      "final tally:  4721 4721\n",
      "524:  524    [[world news, fbi, fisa, trump, washington post]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "# Phase2_w_Neuroner = phase2.EntityResolver()\n",
    "# candidate_base_post_Phase2_w_Neuroner, converted_candidates_w_Neuroner, complete_tweet_dataframe_grouped_df_sorted_w_Neuroner= Phase2_w_Neuroner.executor(max_batch_value,tweet_sentence_df_2nd_copy,CTrie_neuroner,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_2nd_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted_w_Neuroner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 3129\n",
      "2507 622 549\n"
     ]
    }
   ],
   "source": [
    "# # from ast import literal_eval\n",
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     output_mentions_list_neuroner=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Neuroner[complete_tweet_dataframe_grouped_df_sorted_w_Neuroner.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "# #     print(row['TweetText'])\n",
    "# #     print(tweet_ID, annotated_mention_list)\n",
    "# #     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner- tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "\n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8012144455097475 0.8203534031413613 0.8106709781729993\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2095\n",
      "1571 524 875\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    output_mentions_list_ritter=[]\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "    candidate_list_ritter=flatten(ritter_annotator[ritter_annotator.ID==tweet_ID].Output.tolist(),[])\n",
    "    for ritter_candidate in candidate_list_ritter:\n",
    "        output_mentions_list_ritter+= [remAmpersand(elem).strip(string.punctuation).strip() for elem in ritter_candidate.lower().split(',') if(elem)]\n",
    "        \n",
    "#     output_mentions_list_ritter= [remAmpersand(elem).strip(string.punctuation).strip().lower() for elem in candidate_list_ritter]\n",
    "    \n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list_ritter)\n",
    "#     print(output_mentions_list_ritter)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498806682577566 0.6422730989370401 0.6919180797181238\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with different annotators:\n",
    "\n",
    "##With CS+ in phase 1:\n",
    "0.7982625482625483 0.7359833877187778 0.7658589288470442\n",
    "\n",
    "##With Turboparse chunker in phase 1:\n",
    "0.8381118881118881 0.7104327208061648 0.7690086621751684\n",
    "\n",
    "##Just TwitterNLP:\n",
    "0.7460620525059666 0.6335630320226996 0.6852257781674704\n",
    "\n",
    "##With TwitterNLP entity annotator in phase 1:\n",
    "0.856 0.8288732394366197 0.8422182468694097\n",
    "\n",
    "##Just NeuroNER:\n",
    "0.6497791634190699 0.7879647132955262 0.7122312402107361\n",
    "\n",
    "##With NeuroNER entity annotator in phase 1:\n",
    "0.8012144455097475 0.8203534031413613 0.8106709781729993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
