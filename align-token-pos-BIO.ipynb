{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n"
     ]
    }
   ],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "# import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rreplace(s, old, new, occurrence):\n",
    "    if s.endswith(old):\n",
    "        li = s.rsplit(old, occurrence)\n",
    "        return new.join(li)\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "        elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    strip_op= rreplace(rreplace(rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "#     print(tweet_word_list)\n",
    "#     print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "#     print(encoded_tag_sequence)\n",
    "    untagged=[(tweet_word_list[i],'O') for i in range(len(tweet_word_list))]\n",
    "    \n",
    "    return [(tweet_word_list[i],encoded_tag_sequence[i]) for i in range(len(tweet_word_list))], untagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(word_tag_tuples):\n",
    "    \n",
    "    mentions=[]\n",
    "    candidateMention=''\n",
    "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
    "    for tup in word_tag_tuples:\n",
    "        candidate=tup[0]\n",
    "        tag=tup[1]\n",
    "        if(tag=='O'):\n",
    "            if(candidateMention):\n",
    "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "                    if mention_to_add.endswith(\"'s\"):\n",
    "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    elif mention_to_add.endswith(\"’s\"):\n",
    "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    else:\n",
    "                        mention_to_add=mention_to_add\n",
    "                    if(mention_to_add!=''):\n",
    "                        mentions.append(mention_to_add)\n",
    "            candidateMention=''\n",
    "        else:\n",
    "            if (tag=='B'):\n",
    "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "                    if mention_to_add.endswith(\"'s\"):\n",
    "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    elif mention_to_add.endswith(\"’s\"):\n",
    "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    else:\n",
    "                        mention_to_add=mention_to_add\n",
    "                    if(mention_to_add!=''):\n",
    "                        mentions.append(mention_to_add)\n",
    "                candidateMention=candidate\n",
    "            else:\n",
    "                candidateMention+=\" \"+candidate\n",
    "        # if (tag=='B'):\n",
    "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
    "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "        #         if(mention_to_add):\n",
    "        #             mentions.append(mention_to_add)\n",
    "        #     candidateMention=candidate\n",
    "        # else:\n",
    "        #     candidateMention+=\" \"+candidate\n",
    "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
    "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "            if(mention_to_add!=''):\n",
    "                mentions.append(mention_to_add)\n",
    "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
    "    # print('extracted mentions:', mentions)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(annotated_mention_list,output_mentions_list):\n",
    "\n",
    "    # print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "    print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "    \n",
    "    precision=(tp_counter_inner)/(tp_counter_inner+fp_counter_inner)\n",
    "    recall=(tp_counter_inner)/(tp_counter_inner+fn_counter_inner)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007\n",
      "for tally: 2620\n",
      "2619\n",
      "2619 2619\n",
      "2619 2619 2619\n",
      "2004\n",
      "for tally: 4979\n",
      "4978\n",
      "4978 4978\n",
      "4978 4978 4978\n",
      "1009\n",
      "for tally: 2527\n",
      "2526\n",
      "2526 2526\n",
      "2526 2526 2526\n",
      "1004\n",
      "for tally: 3085\n",
      "3084\n",
      "3084 3084\n",
      "3084 3084 3084\n",
      "1054\n",
      "for tally: 3755\n",
      "3754\n",
      "3754 3754\n",
      "3754 3754 3754\n"
     ]
    }
   ],
   "source": [
    "filenames=['ripcity','pikapika','roevwade','billnye','billdeblasio']\n",
    "fileMentionLists=[]\n",
    "fileSentenceLists=[]\n",
    "filePOSTaggedSentenceLists=[]\n",
    "fileBioSentencesLists=[]\n",
    "fileUntaggedSentencesLists=[]\n",
    "fileSentenceLevelMentionsLists=[]\n",
    "\n",
    "for filename in filenames:\n",
    "\n",
    "    tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/\"+filename+\".csv\",sep =',',keep_default_na=False)\n",
    "    print(len(tweets_unpartitoned))\n",
    "\n",
    "    df_holder=[]\n",
    "    batch_number=0\n",
    "    # tweetList=[]\n",
    "    # sentenceList=[]\n",
    "    sentID=0\n",
    "    # sentID_to_tweet_ID={}\n",
    "    tweet_ID_to_sentID={}\n",
    "    mentionList=[]\n",
    "\n",
    "    for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "        index=row.Index\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        hashtags=str(row.HashTags)\n",
    "\n",
    "        user=str(row.User)\n",
    "        tweetText=str(row.TweetText)\n",
    "        annot_raw=\"\"\n",
    "        stanford_candidates=\"\"\n",
    "        ritter_candidates = \"\"\n",
    "        calai_candidates=\"\"\n",
    "\n",
    "        ne_List_final=[]\n",
    "        userMention_List_final=[]\n",
    "        tweetSentenceList=splitSentence(tweetText)\n",
    "    #     sentenceList.extend(tweetSentenceList)\n",
    "\n",
    "        sentIDList=[]\n",
    "\n",
    "        for sentence in tweetSentenceList:\n",
    "            sentIDList.append(sentID)\n",
    "            sentID+=1\n",
    "    #         sentID_to_tweet_ID[sentID]=int(index)\n",
    "    #         sentID+=1\n",
    "\n",
    "        tweet_ID_to_sentID[index]=sentIDList\n",
    "\n",
    "        mentions=[]\n",
    "        for sentence_level in str(row.mentions_other).split(';'):\n",
    "            if(sentence_level):\n",
    "                for mention in sentence_level.split(','):\n",
    "                    if(mention):\n",
    "                        mentions.append(mention.strip())\n",
    "        mentionList.append(mentions)\n",
    "    \n",
    "    fileMentionLists.append(mentionList)\n",
    "\n",
    "# # do sentence wise parsing here\n",
    "# try:\n",
    "#     result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# except:\n",
    "#     print(f'{e}\\n{e.message}')\n",
    "# print('parse done!')\n",
    "\n",
    "# sentences=[]\n",
    "# for sentId, sentence in enumerate(sentenceList):\n",
    "#     result=result_conll[sentId]\n",
    "#     result_lines=result.split('\\n')\n",
    "#     sent_tokens=[]\n",
    "#     for line in result_lines:\n",
    "#         tabs=line.split('\\t')\n",
    "#         sent_tokens.append(tabs[0])\n",
    "#     sentences.append(sent_tokens)\n",
    "\n",
    "\n",
    "# for filename in filenames:\n",
    "    #or read from parsed files and tally len of sentences\n",
    "    f = open(\"/Users/satadisha/Documents/GitHub/BIO_annotations/twokenized_BIO_gaguilar/\"+filename+\"_BIOannotated_twokenized_POSTAG.txt\", \"r\")\n",
    "    file_text=f.read()\n",
    "    print('for tally:',len(file_text.split('\\n\\n')))\n",
    "    sentenceList=[]\n",
    "    pos_tagged_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n'))) #conll\n",
    "    print(len(pos_tagged_sentences))\n",
    "    for pos_tagged_sentence in pos_tagged_sentences:\n",
    "        sentence_tokens=[]\n",
    "        annotation=[]\n",
    "        lines=pos_tagged_sentence.split('\\n')\n",
    "        for line in lines:\n",
    "            if(line):\n",
    "                tabs=line.split('\\t')\n",
    "    # \t        if not((tabs[0]=='')&(tabs[1]=='')):\n",
    "                sentence_tokens.append(tabs[0])\n",
    "        sentenceList.append(sentence_tokens)\n",
    "    fileSentenceLists.append(sentenceList)\n",
    "    filePOSTaggedSentenceLists.append(pos_tagged_sentences)\n",
    "    \n",
    "    print(len(pos_tagged_sentences),len(sentenceList))\n",
    "\n",
    "    # extract sentences for each tweet\n",
    "    sentID=0\n",
    "    write_text=''\n",
    "    write_text_untagged=''\n",
    "    sentence_level_mentions=[]\n",
    "\n",
    "    for tweetID in tweet_ID_to_sentID.keys():\n",
    "        sentIDs=tweet_ID_to_sentID[tweetID]\n",
    "        mentions=mentionList[tweetID]\n",
    "        all_tweet_tokens=[]\n",
    "        sentence_limits=[]\n",
    "#         print(tweetID)\n",
    "        start=0\n",
    "        for sentID in sentIDs:\n",
    "            all_tweet_tokens+=sentenceList[sentID]\n",
    "            sentence_limits.append((start,start+len(sentenceList[sentID])))\n",
    "            start+=len(sentenceList[sentID])\n",
    "        tweet_word_encoding_list, untagged= get_encoding_seq(all_tweet_tokens, mentions)\n",
    "    #     print(tweet_word_encoding_list)\n",
    "        for tup in sentence_limits:\n",
    "            sentence_word_tag_tuples=[]\n",
    "            for index in range(tup[0],tup[1]):\n",
    "                write_text+=tweet_word_encoding_list[index][0]+'\\t'+tweet_word_encoding_list[index][1]+'\\n'\n",
    "                sentence_word_tag_tuples.append((tweet_word_encoding_list[index][0],tweet_word_encoding_list[index][1]))\n",
    "                write_text_untagged+=untagged[index][0]+'\\t'+untagged[index][1]+'\\n'\n",
    "            write_text+='\\n'\n",
    "            write_text_untagged+='\\n'\n",
    "\n",
    "            sentence_level_mentions.append(get_entities(sentence_word_tag_tuples))\n",
    "\n",
    "    bio_sentences=list(filter (lambda elem: elem!='', write_text.split('\\n\\n')))\n",
    "    untagged_sentences=list(filter (lambda elem: elem!='', write_text_untagged.split('\\n\\n')))\n",
    "    print(len(bio_sentences),len(untagged_sentences),len(sentence_level_mentions))\n",
    "    \n",
    "    fileBioSentencesLists.append(bio_sentences)\n",
    "    fileUntaggedSentencesLists.append(untagged_sentences)\n",
    "    fileSentenceLevelMentionsLists.append(sentence_level_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# print(bio_sentences)\n",
    "print(len(fileBioSentencesLists))\n",
    "print(len(fileSentenceLevelMentionsLists))\n",
    "print(len(fileUntaggedSentencesLists))\n",
    "print(len(filePOSTaggedSentenceLists))\n",
    "# different merge iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 13207\n",
      "3754 3754\n"
     ]
    }
   ],
   "source": [
    "split1_train=[0,1,2,3]\n",
    "split1_test=4\n",
    "\n",
    "train_total=0\n",
    "\n",
    "# write_train_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split1.train.pos\", \"w\")\n",
    "# write_train_bio = open(\"/Users/satadisha/Documents/GitHub/3A/split1.train.bio\", \"w\")\n",
    "\n",
    "# write_test_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split1.test.pos\", \"w\")\n",
    "# write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/3A/split1.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "for index in split1_train:\n",
    "    pos_tagged_sentences=filePOSTaggedSentenceLists[index]\n",
    "    train_total+=len(pos_tagged_sentences)\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_tagged_sentences=fileBioSentencesLists[index]\n",
    "    for sentence in bio_tagged_sentences:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "# write_train_pos.write(train_text_pos)\n",
    "# write_train_pos.close()\n",
    "\n",
    "# write_train_bio.write(train_text_BIO)\n",
    "# write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=filePOSTaggedSentenceLists[split1_test]\n",
    "test_untagged_fold=fileUntaggedSentencesLists[split1_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "# write_test_pos.write(test_text_pos)\n",
    "# write_test_pos.close()\n",
    "\n",
    "# write_test_untagged.write(test_text_untagged)\n",
    "# write_test_untagged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tagged mentions: 1193\n",
      "all output mentions: 1025\n",
      "933 92 260\n",
      "precision:  0.9102439024390244\n",
      "recall:  0.7820620284995808\n",
      "f_measure:  0.8412984670874661\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "mention_fold=fileSentenceLevelMentionsLists[split1_test]\n",
    "all_mentions=[]\n",
    "for mentions in mention_fold:\n",
    "    all_mentions+=mentions\n",
    "print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "f = open('/Users/satadisha/Documents/GitHub/3A/split1_output.txt')\n",
    "output_text=f.read()\n",
    "all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "print('all output mentions:',len(all_outputs))\n",
    "\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 13877\n",
      "3084 3084\n"
     ]
    }
   ],
   "source": [
    "split2_train=[0,1,2,4]\n",
    "split2_test=3\n",
    "\n",
    "train_total=0\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split2.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/3A/split2.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split2.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/3A/split2.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "for index in split2_train:\n",
    "    pos_tagged_sentences=filePOSTaggedSentenceLists[index]\n",
    "    train_total+=len(pos_tagged_sentences)\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_tagged_sentences=fileBioSentencesLists[index]\n",
    "    for sentence in bio_tagged_sentences:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=filePOSTaggedSentenceLists[split2_test]\n",
    "test_untagged_fold=fileUntaggedSentencesLists[split2_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tagged mentions: 794\n",
      "all output mentions: 588\n",
      "431 157 363\n",
      "precision:  0.7329931972789115\n",
      "recall:  0.5428211586901763\n",
      "f_measure:  0.6237337192474675\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "mention_fold=fileSentenceLevelMentionsLists[split2_test]\n",
    "all_mentions=[]\n",
    "for mentions in mention_fold:\n",
    "    all_mentions+=mentions\n",
    "print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "f = open('/Users/satadisha/Documents/GitHub/3A/split2_output.txt')\n",
    "output_text=f.read()\n",
    "all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "print('all output mentions:',len(all_outputs))\n",
    "\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 14435\n",
      "2526 2526\n"
     ]
    }
   ],
   "source": [
    "split3_train=[0,1,3,4]\n",
    "split3_test=2\n",
    "\n",
    "train_total=0\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split3.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/3A/split3.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split3.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/3A/split3.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "for index in split3_train:\n",
    "    pos_tagged_sentences=filePOSTaggedSentenceLists[index]\n",
    "    train_total+=len(pos_tagged_sentences)\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_tagged_sentences=fileBioSentencesLists[index]\n",
    "    for sentence in bio_tagged_sentences:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=filePOSTaggedSentenceLists[split3_test]\n",
    "test_untagged_fold=fileUntaggedSentencesLists[split3_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tagged mentions: 1823\n",
      "all output mentions: 1363\n",
      "1301 62 522\n",
      "precision:  0.954512105649303\n",
      "recall:  0.7136588041689522\n",
      "f_measure:  0.8166980539861894\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "mention_fold=fileSentenceLevelMentionsLists[split3_test]\n",
    "all_mentions=[]\n",
    "for mentions in mention_fold:\n",
    "    all_mentions+=mentions\n",
    "print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "f = open('/Users/satadisha/Documents/GitHub/3A/split3_output.txt')\n",
    "output_text=f.read()\n",
    "all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "print('all output mentions:',len(all_outputs))\n",
    "\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 11983\n",
      "4978 4978\n"
     ]
    }
   ],
   "source": [
    "split4_train=[0,2,3,4]\n",
    "split4_test=1\n",
    "\n",
    "train_total=0\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split4.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/3A/split4.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split4.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/3A/split4.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "for index in split4_train:\n",
    "    pos_tagged_sentences=filePOSTaggedSentenceLists[index]\n",
    "    train_total+=len(pos_tagged_sentences)\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_tagged_sentences=fileBioSentencesLists[index]\n",
    "    for sentence in bio_tagged_sentences:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=filePOSTaggedSentenceLists[split4_test]\n",
    "test_untagged_fold=fileUntaggedSentencesLists[split4_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tagged mentions: 1148\n",
      "all output mentions: 874\n",
      "770 104 378\n",
      "precision:  0.8810068649885584\n",
      "recall:  0.6707317073170732\n",
      "f_measure:  0.76162215628091\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "mention_fold=fileSentenceLevelMentionsLists[split4_test]\n",
    "all_mentions=[]\n",
    "for mentions in mention_fold:\n",
    "    all_mentions+=mentions\n",
    "print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "f = open('/Users/satadisha/Documents/GitHub/3A/split4_output.txt')\n",
    "output_text=f.read()\n",
    "all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "print('all output mentions:',len(all_outputs))\n",
    "\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 14342\n",
      "2619 2619\n"
     ]
    }
   ],
   "source": [
    "split5_train=[1,2,3,4]\n",
    "split5_test=0\n",
    "\n",
    "train_total=0\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split5.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/3A/split5.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/3A/split5.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/3A/split5.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "for index in split5_train:\n",
    "    pos_tagged_sentences=filePOSTaggedSentenceLists[index]\n",
    "    train_total+=len(pos_tagged_sentences)\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_tagged_sentences=fileBioSentencesLists[index]\n",
    "    for sentence in bio_tagged_sentences:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=filePOSTaggedSentenceLists[split5_test]\n",
    "test_untagged_fold=fileUntaggedSentencesLists[split5_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tagged mentions: 1105\n",
      "all output mentions: 765\n",
      "693 72 412\n",
      "precision:  0.9058823529411765\n",
      "recall:  0.6271493212669683\n",
      "f_measure:  0.7411764705882353\n"
     ]
    }
   ],
   "source": [
    "## evaluation\n",
    "mention_fold=fileSentenceLevelMentionsLists[split5_test]\n",
    "all_mentions=[]\n",
    "for mentions in mention_fold:\n",
    "    all_mentions+=mentions\n",
    "print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "f = open('/Users/satadisha/Documents/GitHub/3A/split5_output.txt')\n",
    "output_text=f.read()\n",
    "all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "print('all output mentions:',len(all_outputs))\n",
    "\n",
    "get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3B setup beyond this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5 5\n"
     ]
    }
   ],
   "source": [
    "length = int(len(bio_sentences)/5) #length of each fold\n",
    "pos_tagged_folds = []\n",
    "bio_tagged_folds= []\n",
    "untagged_folds= []\n",
    "mention_folds= []\n",
    "\n",
    "for i in range(4):\n",
    "    pos_tagged_folds += [pos_tagged_sentences[i*length:(i+1)*length]]\n",
    "    bio_tagged_folds += [bio_sentences[i*length:(i+1)*length]]\n",
    "    untagged_folds += [untagged_sentences[i*length:(i+1)*length]]\n",
    "    mention_folds += [sentence_level_mentions[i*length:(i+1)*length]]\n",
    "    \n",
    "pos_tagged_folds += [pos_tagged_sentences[4*length:len(pos_tagged_sentences)]]\n",
    "bio_tagged_folds += [bio_sentences[4*length:len(bio_sentences)]]\n",
    "untagged_folds += [untagged_sentences[4*length:len(untagged_sentences)]]\n",
    "mention_folds += [sentence_level_mentions[4*length:len(sentence_level_mentions)]]\n",
    "\n",
    "print(len(pos_tagged_folds),len(bio_tagged_folds),len(untagged_folds),len(mention_folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], ['de blasio'], [], [], [], [], [], [], [], [], ['de blasio'], [], ['nyc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new york'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['bill'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['second amendment'], ['democrats'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['second amendment'], ['democrats'], [], [], [], [], ['de blasio'], ['ocasio'], ['de blasio'], [], [], [], [], [], [], ['de blasio'], ['deblasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['bill de blasio', 'new york'], ['david dinkins', 'abraham beame'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['heckles', 'new yorkers', 'de blasio'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], ['bill de blasio'], ['david dinkins', 'abraham beame'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc mayor bill de blasio', 'u.s. president'], ['roman emperor caligula'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['second amendment'], ['democrats'], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['democratic', 'republicans'], ['jeff probst', 'anderson cooper'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc'], ['ny'], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], ['nyc', 'ca'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['bill de blasio'], ['nyc'], ['ny', 'u.s'], [], [], [], ['de blasio'], ['donald trump', 'trump'], ['nyc mayor'], ['ny attorney general'], [], [], [], [], [], [], [], [], [], ['bill de blasio'], ['nyc'], ['ny', 'u.s'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], ['de blasio'], ['trump'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['groundhog'], [], [], [], ['groundhog'], [], [], ['dnc'], [], [], [], ['de blasio'], [], ['groundhog'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['groundhog'], [], [], ['new york city'], ['sandy'], ['nycacc'], [], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], ['president trump'], ['nyc'], ['america'], [], [], [], [], ['de blasio'], ['new york city'], [], ['nyc'], [], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['beyoncé'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new york'], [], [], [], ['de blasio'], ['democratic party', 'chuck', 'groundhog'], ['chuck'], ['groundhog'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['groundhog'], [], [], [], ['groundhog'], [], [], ['new york city', 'nyc'], [], ['christmas'], [], ['new york city', 'nyc'], [], ['christmas'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['groundhog'], [], [], [], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], ['de blasio', 'mayor of new york'], [], [], [], ['de blasio'], [], [], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['dem'], ['dem'], [], ['dem'], ['dem'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['democrats'], [], [], ['de blasio', 'maxine waters'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['groundhog'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc mayor de blasio', 'sentinel colorado'], [], [], ['gma'], ['nyc'], ['new yorkers'], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], ['groundhog'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], ['bill de blasio', 'president trump', 'country house', 'maximum security'], [], [], [], ['groundhog'], [], [], [], [], ['ben'], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['america'], [], ['ben'], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], ['nyc', 'america'], ['nyc', 'america'], ['ben'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], ['trump'], ['new york city'], [], [], [], ['de blasio'], ['new york', 'trump'], ['napoleon', 'france', 'europe', 'waterloo'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['new york', 'trump'], ['napoleon', 'france', 'europe', 'waterloo'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new york', 'trump'], ['napoleon', 'france', 'europe', 'waterloo'], [], ['america'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['new york', 'trump'], ['napoleon', 'france', 'europe', 'waterloo'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['bolsonaro'], [], [], [], [], [], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], ['nyc', 'trump', 'de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['u.s'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['democrats'], [], [], [], [], [], ['de blasio'], [], ['mr'], ['president'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['trump'], [], [], [], [], ['de blasio'], ['bill'], ['bill'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new york mayor'], ['bill de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], ['bill de blasio', 'ny', 'china'], [], [], [], ['de blasio'], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], [], ['us'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], ['n.y', 'trump'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['ben'], [], [], ['new york'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], ['de blasio'], [], ['mayor of new york'], [], [], [], [], [], [], [], [], [], [], ['empire state building'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], ['new yorkers'], [], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], [], [], [], ['de blasio'], [], ['de blasio'], [], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['america'], ['democrats'], [], [], [], [], ['de blasio'], [], ['americans', 'democrat'], [], [], [], [], ['de blasio'], ['new york'], [], [], ['empire state building'], [], [], [], ['de blasio'], [], [], ['empire state building'], [], [], [], [], [], [], [], ['de blasio'], ['bill de blasio', 'bill de blasio'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['american'], [], ['americans', 'democrat'], [], [], ['empire state building'], [], [], [], ['de blasio'], ['us'], [], [], ['americans', 'democrat'], [], ['new yorkers'], [], [], [], ['joe biden'], ['socialism', 'new york'], [], ['americans', 'democrat'], [], ['sandy'], ['nycacc'], [], ['nyc mayor'], ['warren wilhelm'], [], ['marxist'], [], [], [], ['nyc', 'bill de blasio'], [], [], [], ['de blasio'], ['democrats'], ['nyc'], [], [], [], ['de blasio'], ['tucker carlson'], [], [], [], [], [], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['americans', 'democrat'], [], [], [], [], ['de blasio'], ['american'], [], [], [], ['de blasio'], [], [], ['americans', 'democrat'], [], [], [], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], ['callie'], ['u.s. senator', 'sat'], [], [], [], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], ['american'], [], [], ['us'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['us'], [], [], [], [], [], [], [], ['empire state building'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['nyc', 'bill de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['warren wilhelm'], [], ['warren de blasio-wilhelm', 'bill de blasio'], [], [], ['bill despicable'], [], [], [], [], ['de blasio'], [], [], ['empire state building'], [], [], [], [], ['new yorkers'], [], ['sandy'], ['nycacc'], [], ['nyc'], ['bill de blasio', 'laguardia', 'world airport awards'], [], [], [], [], [], ['de blasio'], [], [], ['americans', 'democrat'], [], [], [], [], [], [], ['de blasio'], [], [], ['americans', 'democrat'], [], [], [], [], [], ['americans', 'democrat'], [], [], [], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], ['democrats', 'democratic'], [], [], [], [], [], ['democrat'], [], ['mayor', 'new york city'], [], [], ['bill de blasio', 'laguardia', 'world airport awards'], [], [], ['democrats', 'democrat'], [], [], [], [], ['bill de blasio', 'laguardia', 'world airport awards'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc mayor de blasio', 'democrat', 'presidential candidates'], [], [], ['bill de blasio'], [], [], [], [], [], [], [], ['de blasio'], ['new yorkers', 'liberals'], [], ['bill de blasio'], [], ['usa'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['bill de blasio', 'laguardia', 'world airport awards'], [], [], [], ['bill despicable'], [], ['ny'], ['nyc', 'subways'], [], [], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['de blasio'], ['new york'], [], ['judas', 'jezebel', 'new york city'], [], [], [], [], [], ['de blasio'], ['judas', 'jezebel', 'new york city'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['black america'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], ['judas', 'jezebel', 'new york city'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], ['usa'], [], [], ['whoopi goldberg'], [], ['bill despicable'], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], ['democrats', 'bill de blasio'], [], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['bill despicable'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], ['empire state building'], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], ['bill de blasio', 'laguardia', 'world airport awards'], [], [], [], [], [], ['de blasio'], [], [], ['empire state building'], [], [], [], [], [], [], [], [], [], ['mayor ny'], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['governor', 'de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['potus'], [], [], ['de blasio', 'ortez', 'ny'], [], [], [], ['empire state building'], [], [], [], [], ['judas', 'jezebel', 'new york city'], [], [], [], [], [], [], ['de blasio'], ['tucker carlson', 'fox news'], [], [], [], ['de blasio'], ['ny'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nycacc', 'new york city'], ['wild west'], ['nycacc', 'new york city'], ['wild west'], [], [], [], [], [], [], ['nyc'], [], [], [], [], [], ['de blasio'], ['new york city'], ['wild west'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['usa'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['twitter'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['bill de blasio', 'nyc'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['amazon'], [], ['new yorkers'], ['groundhog'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], ['amazon'], [], ['new yorkers'], ['groundhog'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['united democrats', 'republicans'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['america'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new york city', 'bill de blasio'], [], ['new york city', 'bill de blasio'], [], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['president trump'], [], ['new york city'], [], [], [], ['bolsheviks', 'mensheviks'], [], [], ['bernie'], [], [], [], ['mayor bill de blasio', 'new yorkers', 'usa'], [], [], [], [], [], ['de blasio'], [], [], [], [], ['united democrats', 'republicans'], [], [], [], [], ['de blasio'], ['united democrats', 'republicans'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['gracie mansion'], ['nypd', 'liberty island'], ['nyc'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['united democrats', 'republicans'], [], [], [], [], ['de blasio'], ['2020 presidential race'], [], ['alabama'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['new yorkers'], ['united democrats', 'republicans'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['groundhog'], [], ['big bird'], [], ['americans'], ['judas', 'jezebel', 'new york city'], [], [], [], [], [], [], [], [], [], ['de blasio'], [], ['de blasio'], ['democrats', 'new york state', 'president trump'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nypd', 'black lives matter'], [], [], [], [], ['de blasio'], ['new york'], [], [], [], [], [], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['whoopie'], ['republican', 'hollywood', 'newyork'], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['democrat primary'], ['democrats'], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['nyc'], [], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], ['ny'], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['baby donnie'], ['donnie'], ['white house'], [], [], [], [], ['de blasio'], [], ['the view'], [], ['bill de blasio', 'the view'], ['sandy'], ['nycacc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['groundhog'], [], ['the view'], [], ['bill de blasio', 'the view'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], [], [], [], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['new york'], [], [], [], [], [], [], [], ['de blasio'], [], ['bill'], [], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['new yorkers'], [], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['nyc mayor de blasio', 'sentinel colorado'], [], [], [], ['de blasio'], [], [], [], [], [], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['de blasio'], [], ['doh', 'aspca'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['potus'], ['bill'], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], ['dem'], ['dems'], [], [], [], [], [], ['de blasio'], ['white house'], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], ['the view'], [], ['bill de blasio', 'the view'], ['ed berg daily'], [], [], [], [], ['de blasio'], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], [], ['the view'], [], ['bill de blasio', 'the view'], ['nyc mayor de blasio', 'democrat'], [], [], ['nyc', 'bernie sanders'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['nyc'], ['dem', 'trump'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], ['de blasio'], [], ['de blasio'], [], [], [], ['de blasio'], ['bill'], ['black lives matter'], [], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['sandy'], ['nycacc'], [], [], ['nycacc'], [], [], [], ['potus'], [], [], ['usa'], [], [], [], ['de blasio'], ['sandy'], ['nycacc'], [], ['bill de blasio'], [], [], [], [], ['bill de blasio', 'bill de blasio', 'scaramucci'], [], ['nycacc'], [], [], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['ben'], [], [], [], [], [], ['de blasio'], ['mets'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], ['potus'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], ['the view'], [], ['bill de blasio', 'the view'], ['de blasio', 'mayor of new york', 'nycacc', 'southern hemisphere'], [], [], [], ['bill de blasio', 'mayor of new york city'], ['groundhog'], [], [], [], [], [], [], [], ['de blasio'], [], ['new york', 'bill de blasio', 'trump'], [], [], [], ['de blasio'], ['bill de blasio'], ['nyc'], ['ny', 'u.s'], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], [], [], ['trump'], [], [], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], ['ben'], [], [], [], ['bill', 'hillary'], [], [], ['new york city'], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], [], ['bill', 'hillary'], [], [], [], [], [], ['de blasio'], [], ['bill', 'hillary'], [], [], ['bill de blasio'], ['nyc'], ['ny', 'u.s'], [], ['bill', 'hillary'], [], [], ['new york city'], [], ['bill', 'hillary'], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], ['new york city mayor bill de blasio'], [], [], [], [], [], [], [], ['de blasio'], ['hillary clinton'], ['obama'], ['potus'], [], ['mayor of nyc'], [], [], [], [], [], ['de blasio'], ['america'], [], ['nyc'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], ['us'], [], ['hillary clinton'], ['obama'], ['potus'], [], ['mayor of nyc'], [], [], [], [], [], ['de blasio'], ['hillary clinton'], ['obama'], ['potus'], [], ['mayor of nyc'], [], [], [], ['de blasio'], [], [], ['hillary clinton'], ['obama'], ['potus'], [], ['mayor of nyc'], [], [], [], ['de blasio'], ['hillary clinton'], ['obama'], ['potus'], [], ['mayor of nyc'], [], [], [], ['de blasio'], [], [], [], [], ['bill de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], ['trump'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['wh', 'america'], [], [], [], [], [], ['de blasio'], ['new york', 'bill de blasio', 'trump'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['warren wilhelm'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], ['trump'], ['bill'], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], [], ['the view'], [], ['bill de blasio', 'the view'], [], [], [], ['de blasio'], ['de blasio', 'warren wilhelm'], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio'], [], [], [], ['de blasio']]\n"
     ]
    }
   ],
   "source": [
    "# print(sentence_level_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_train=[0,1,2,3]\n",
    "split1_test=4\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/split1.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/split1.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/split1.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/split1.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "train_total=0\n",
    "for index in split1_train:\n",
    "    pos_fold=pos_tagged_folds[index]\n",
    "    train_total+=len(pos_fold)\n",
    "    for sentence in pos_fold:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_fold=bio_tagged_folds[index]\n",
    "    for sentence in bio_fold:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=pos_tagged_folds[split1_test]\n",
    "test_untagged_fold=untagged_folds[split1_test]\n",
    "\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()\n",
    "\n",
    "\n",
    "# ## evaluation of this fold\n",
    "# mention_fold=mention_folds[split1_test]\n",
    "# all_mentions=[]\n",
    "# for mentions in mention_fold:\n",
    "#     all_mentions+=mentions\n",
    "# print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "# f = open('/Users/satadisha/Documents/GitHub/split1_output_selfTrain.txt')\n",
    "# output_text=f.read()\n",
    "# all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "# print('all output mentions:',len(all_outputs))\n",
    "\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 3004\n",
      "750 750\n",
      "all tagged mentions: 249\n",
      "all output mentions: 241\n",
      "228 13 21\n",
      "precision:  0.946058091286307\n",
      "recall:  0.9156626506024096\n",
      "f_measure:  0.9306122448979591\n"
     ]
    }
   ],
   "source": [
    "split2_train=[0,1,2,4]\n",
    "split2_test=3\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/split2.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/split2.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/split2.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/split2.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "train_total=0\n",
    "for index in split2_train:\n",
    "    pos_fold=pos_tagged_folds[index]\n",
    "    train_total+=len(pos_fold)\n",
    "    for sentence in pos_fold:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_fold=bio_tagged_folds[index]\n",
    "    for sentence in bio_fold:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=pos_tagged_folds[split2_test]\n",
    "test_untagged_fold=untagged_folds[split2_test]\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()\n",
    "\n",
    "# ## evaluation of this fold\n",
    "# mention_fold=mention_folds[split2_test]\n",
    "# all_mentions=[]\n",
    "# for mentions in mention_fold:\n",
    "#     all_mentions+=mentions\n",
    "# print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "# f = open('/Users/satadisha/Documents/GitHub/split2_output_selfTrain.txt')\n",
    "# output_text=f.read()\n",
    "# all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "# print('all output mentions:',len(all_outputs))\n",
    "\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 3004\n",
      "750 750\n",
      "all tagged mentions: 248\n",
      "all output mentions: 233\n",
      "212 21 36\n",
      "precision:  0.9098712446351931\n",
      "recall:  0.8548387096774194\n",
      "f_measure:  0.8814968814968815\n"
     ]
    }
   ],
   "source": [
    "split3_train=[0,1,3,4]\n",
    "split3_test=2\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/split3.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/split3.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/split3.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/split3.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "train_total=0\n",
    "for index in split3_train:\n",
    "    pos_fold=pos_tagged_folds[index]\n",
    "    train_total+=len(pos_fold)\n",
    "    for sentence in pos_fold:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_fold=bio_tagged_folds[index]\n",
    "    for sentence in bio_fold:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=pos_tagged_folds[split3_test]\n",
    "test_untagged_fold=untagged_folds[split3_test]\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()\n",
    "\n",
    "# ## evaluation of this fold\n",
    "# mention_fold=mention_folds[split3_test]\n",
    "# all_mentions=[]\n",
    "# for mentions in mention_fold:\n",
    "#     all_mentions+=mentions\n",
    "# print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "# f = open('/Users/satadisha/Documents/GitHub/split3_output_selfTrain.txt')\n",
    "# output_text=f.read()\n",
    "# all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "# print('all output mentions:',len(all_outputs))\n",
    "\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 3004\n",
      "750 750\n",
      "all tagged mentions: 245\n",
      "all output mentions: 243\n",
      "230 13 15\n",
      "precision:  0.9465020576131687\n",
      "recall:  0.9387755102040817\n",
      "f_measure:  0.9426229508196722\n"
     ]
    }
   ],
   "source": [
    "split4_train=[0,2,3,4]\n",
    "split4_test=1\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/split4.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/split4.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/split4.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/split4.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "train_total=0\n",
    "for index in split4_train:\n",
    "    pos_fold=pos_tagged_folds[index]\n",
    "    train_total+=len(pos_fold)\n",
    "    for sentence in pos_fold:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_fold=bio_tagged_folds[index]\n",
    "    for sentence in bio_fold:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=pos_tagged_folds[split4_test]\n",
    "test_untagged_fold=untagged_folds[split4_test]\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()\n",
    "\n",
    "# ## evaluation of this fold\n",
    "# mention_fold=mention_folds[split4_test]\n",
    "# all_mentions=[]\n",
    "# for mentions in mention_fold:\n",
    "#     all_mentions+=mentions\n",
    "# print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "# f = open('/Users/satadisha/Documents/GitHub/split4_output_selfTrain.txt')\n",
    "# output_text=f.read()\n",
    "# all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "# print('all output mentions:',len(all_outputs))\n",
    "\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold: 3004\n",
      "750 750\n",
      "all tagged mentions: 221\n",
      "all output mentions: 219\n",
      "211 8 10\n",
      "precision:  0.9634703196347032\n",
      "recall:  0.9547511312217195\n",
      "f_measure:  0.959090909090909\n"
     ]
    }
   ],
   "source": [
    "split5_train=[1,2,3,4]\n",
    "split5_test=0\n",
    "\n",
    "write_train_pos = open(\"/Users/satadisha/Documents/GitHub/split5.train.pos\", \"w\")\n",
    "write_train_bio = open(\"/Users/satadisha/Documents/GitHub/split5.train.bio\", \"w\")\n",
    "\n",
    "write_test_pos = open(\"/Users/satadisha/Documents/GitHub/split5.test.pos\", \"w\")\n",
    "write_test_untagged = open(\"/Users/satadisha/Documents/GitHub/split5.test.untagged\", \"w\")\n",
    "\n",
    "train_text_pos=''\n",
    "train_text_BIO=''\n",
    "\n",
    "train_total=0\n",
    "for index in split5_train:\n",
    "    pos_fold=pos_tagged_folds[index]\n",
    "    train_total+=len(pos_fold)\n",
    "    for sentence in pos_fold:\n",
    "        train_text_pos+=sentence+'\\n\\n'\n",
    "    \n",
    "    bio_fold=bio_tagged_folds[index]\n",
    "    for sentence in bio_fold:\n",
    "        train_text_BIO+=sentence+'\\n\\n'\n",
    "\n",
    "print('train fold:',train_total)\n",
    "\n",
    "write_train_pos.write(train_text_pos)\n",
    "write_train_pos.close()\n",
    "\n",
    "write_train_bio.write(train_text_BIO)\n",
    "write_train_bio.close()\n",
    "\n",
    "test_text_pos=''\n",
    "test_text_untagged=''\n",
    "\n",
    "test_pos_fold=pos_tagged_folds[split5_test]\n",
    "test_untagged_fold=untagged_folds[split5_test]\n",
    "print(len(test_pos_fold),len(test_untagged_fold))\n",
    "\n",
    "for sentence in test_pos_fold:\n",
    "    test_text_pos+=sentence+'\\n\\n'\n",
    "\n",
    "\n",
    "for sentence in test_untagged_fold:\n",
    "    test_text_untagged+=sentence+'\\n\\n'\n",
    "    \n",
    "write_test_pos.write(test_text_pos)\n",
    "write_test_pos.close()\n",
    "\n",
    "write_test_untagged.write(test_text_untagged)\n",
    "write_test_untagged.close()\n",
    "\n",
    "# ## evaluation of this fold\n",
    "# mention_fold=mention_folds[split5_test]\n",
    "# all_mentions=[]\n",
    "# for mentions in mention_fold:\n",
    "# #     print(mentions)\n",
    "#     all_mentions+=mentions\n",
    "# print('all tagged mentions:',len(all_mentions))\n",
    "\n",
    "# f = open('/Users/satadisha/Documents/GitHub/split5_output_selfTrain.txt')\n",
    "# output_text=f.read()\n",
    "# all_outputs=list(filter (lambda elem: elem!='', output_text.split('\\n')))\n",
    "# print('all output mentions:',len(all_outputs))\n",
    "\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
