{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n"
     ]
    }
   ],
   "source": [
    "from tweebo_parser import API, ServerError\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import emoji\n",
    "import trie\n",
    "import datetime\n",
    "\n",
    "import NE_candidate_module as ne\n",
    "import Mention\n",
    "\n",
    "\n",
    "# import twokenize\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from collections import Iterable, OrderedDict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy import stats\n",
    "\n",
    "# import phase2_Trie_baseline_reintroduction_effectiveness as phase2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=word_tokenize(\"Very well explained take on Carter/ Russia/ FISA/ Trump's sitch.\")\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Existing Lists--------------------\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "tempList=[\"i\",\"and\",\"or\",\"other\",\"another\",\"across\",\"unlike\",\"anytime\",\"were\",\"you\",\"then\",\"still\",\"till\",\"nor\",\"perhaps\",\"otherwise\",\"until\",\"sometimes\",\"sometime\",\"seem\",\"cannot\",\"seems\",\"because\",\"can\",\"like\",\"into\",\"able\",\"unable\",\"either\",\"neither\",\"if\",\"we\",\"it\",\"else\",\"elsewhere\",\"how\",\"not\",\"what\",\"who\",\"when\",\"where\",\"who's\",\"who’s\",\"let\",\"today\",\"tomorrow\",\"tonight\",\"let's\",\"let’s\",\"lets\",\"know\",\"make\",\"oh\",\"via\",\"i\",\"yet\",\"must\",\"mustnt\",\"mustn't\",\"mustn’t\",\"i'll\",\"i’ll\",\"you'll\",\"you’ll\",\"we'll\",\"we’ll\",\"done\",\"doesnt\",\"doesn't\",\"doesn’t\",\"dont\",\"don't\",\"don’t\",\"did\",\"didnt\",\"didn't\",\"didn’t\",\"much\",\"without\",\"could\",\"couldn't\",\"couldn’t\",\"would\",\"wouldn't\",\"wouldn’t\",\"should\",\"shouldn't\",\"souldn’t\",\"shall\",\"isn't\",\"isn’t\",\"hasn't\",\"hasn’t\",\"wasn't\",\"wasn’t\",\"also\",\"let's\",\"let’s\",\"let\",\"well\",\"just\",\"everyone\",\"anyone\",\"noone\",\"none\",\"someone\",\"theres\",\"there's\",\"there’s\",\"everybody\",\"nobody\",\"somebody\",\"anything\",\"else\",\"elsewhere\",\"something\",\"nothing\",\"everything\",\"i'd\",\"i’d\",\"i’m\",\"won't\",\"won’t\",\"i’ve\",\"i've\",\"they're\",\"they’re\",\"we’re\",\"we're\",\"we'll\",\"we’ll\",\"we’ve\",\"we've\",\"they’ve\",\"they've\",\"they’d\",\"they'd\",\"they’ll\",\"they'll\",\"again\",\"you're\",\"you’re\",\"you've\",\"you’ve\",\"thats\",\"that's\",'that’s','here’s',\"here's\",\"what's\",\"what’s\",\"i’m\",\"i'm\",\"a\",\"so\",\"except\",\"arn't\",\"aren't\",\"arent\",\"this\",\"when\",\"it\",\"it’s\",\"it's\",\"he's\",\"she's\",\"she'd\",\"he'd\",\"he'll\",\"she'll\",\"she’ll\",\"many\",\"can't\",\"cant\",\"can’t\",\"even\",\"yes\",\"no\",\"these\",\"here\",\"there\",\"to\",\"maybe\",\"<hashtag>\",\"<hashtag>.\",\"ever\",\"every\",\"never\",\"there's\",\"there’s\",\"whenever\",\"wherever\",\"however\",\"whatever\",\"always\",\"although\"]\n",
    "for item in tempList:\n",
    "    if item not in cachedStopWords:\n",
    "        cachedStopWords.append(item)\n",
    "cachedStopWords.remove(\"don\")\n",
    "cachedStopWords.remove(\"your\")\n",
    "cachedStopWords.remove(\"up\")\n",
    "cachedTitles = [\"mr.\",\"mr\",\"mrs.\",\"mrs\",\"miss\",\"ms\",\"sen.\",\"dr\",\"dr.\",\"prof.\",\"president\",\"congressman\"]\n",
    "prep_list=[\"in\",\"at\",\"of\",\"on\",\"v.\"] #includes common conjunction as well\n",
    "article_list=[\"a\",\"an\",\"the\"]\n",
    "conjoiner=[\"de\"]\n",
    "day_list=[\"sunday\",\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"mon\",\"tues\",\"wed\",\"thurs\",\"fri\",\"sat\",\"sun\"]\n",
    "month_list=[\"january\",\"february\",\"march\",\"april\",\"may\",\"june\",\"july\",\"august\",\"september\",\"october\",\"november\",\"december\",\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
    "chat_word_list=[\"nope\",\"gee\",\"hmm\",\"bye\",\"vs\",\"ouch\",\"omw\",\"qt\",\"dj\",\"dm\",\"congrat\",\"haueheuaeh\",\"ahushaush\",\"jr\",\"please\",\"retweet\",\"2mrw\",\"2moro\",\"4get\",\"ooh\",\"reppin\",\"idk\",\"oops\",\"yup\",\"stfu\",\"uhh\",\"2b\",\"dear\",\"yay\",\"btw\",\"ahhh\",\"b4\",\"ugh\",\"ty\",\"cuz\",\"coz\",\"sorry\",\"yea\",\"asap\",\"ur\",\"bs\",\"rt\",\"lmfao\",\"lfmao\",\"slfmao\",\"u\",\"r\",\"nah\",\"umm\",\"ummm\",\"thank\",\"thanks\",\"congrats\",\"whoa\",\"rofl\",\"ha\",\"ok\",\"okay\",\"hey\",\"hi\",\"huh\",\"ya\",\"yep\",\"yeah\",\"fyi\",\"duh\",\"damn\",\"lol\",\"omg\",\"congratulations\",\"fucking\",\"fuck\",\"f*ck\",\"wtf\",\"wth\",\"aka\",\"wtaf\",\"xoxo\",\"rofl\",\"imo\",\"wow\",\"fck\",\"haha\",\"hehe\",\"hoho\"]\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "#string.punctuation.extend('“','’','”')\n",
    "#---------------------Existing Lists--------------------\n",
    "\n",
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(gutenberg_text)\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('ret.')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('rep.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords=cachedStopWords+cachedTitles+prep_list+article_list+conjoiner+day_list+month_list+chat_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes server is running locally at 0.0.0.0:8000\n",
    "tweebo_api = API()\n",
    "proper_noun_tag='^'\n",
    "common_noun_tag='N'\n",
    "prep_tag='P'\n",
    "\n",
    "\n",
    "def flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if mylist !=[]:\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                flatten(item, outlist)\n",
    "            else:\n",
    "#                 if isinstance(item,ne.NE_candidate):\n",
    "#                     item.phraseText=item.phraseText.strip(' \\t\\n\\r')\n",
    "#                     item.reset_length()\n",
    "#                 else:\n",
    "                if type(item)!= int:\n",
    "                    item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "    \n",
    "def splitSentence(tweetText):\n",
    "#     print(tweetText)\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, tweetText.split('\\n')))\n",
    "    # tweetSentenceList_inter=self.flatten(list(map(lambda sentText: sent_tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList_inter= flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        temp=[]\n",
    "\n",
    "        if \"(\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"(\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: '('+elem, temp))\n",
    "        elif \")\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\")\")))\n",
    "            if(temp):\n",
    "                temp=list(map(lambda elem: elem+')', temp))\n",
    "            # temp.append(temp1[-1])\n",
    "        elif ((\"-\" in word)&(not word.endswith(\"-\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"-\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'-', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"?\" in word)&(not word.endswith(\"?\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"?\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'?', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\":\" in word)&(not word.endswith(\":\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\":\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+':', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\",\" in word)&(not word.endswith(\",\"))):\n",
    "            #temp=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\",\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+',', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif ((\"/\" in word)&(not word.endswith(\"/\"))):\n",
    "            temp1=list(filter(lambda elem: elem!='',word.split(\"/\")))\n",
    "            if(temp1):\n",
    "                temp=list(map(lambda elem: elem+'/', temp1[:-1]))\n",
    "            temp.append(temp1[-1])\n",
    "        elif (list(p_dots.finditer(word))):\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "        elif \"…\" in word:\n",
    "            temp=list(filter(lambda elem: elem!='',word.split(\"…\")))\n",
    "            if(temp):\n",
    "                if(word.endswith(\"…\")):\n",
    "                    temp=list(map(lambda elem: elem+'…', temp))\n",
    "                else:\n",
    "                    temp=list(map(lambda elem: elem+'…', temp[:-1]))+[temp[-1]]\n",
    "        else:\n",
    "            #if word not in string.punctuation:\n",
    "            temp=[word]\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordsII(sentence):\n",
    "    tempList=[]\n",
    "    tempWordList=sentence.split()\n",
    "    p_dots= re.compile(r'[.]{2,}')\n",
    "    #print(tempWordList)\n",
    "    for word in tempWordList:\n",
    "        if (list(p_dots.finditer(word))):\n",
    "#             print('==>',word)\n",
    "            matched_spans= list(p_dots.finditer(word)) \n",
    "            temp=[]\n",
    "            next_string_start=0\n",
    "            for matched_span in matched_spans:\n",
    "                matched_start=matched_span.span()[0]\n",
    "                this_excerpt=word[next_string_start:matched_start]\n",
    "                if(this_excerpt):\n",
    "                    temp.append(this_excerpt)\n",
    "                next_string_start=matched_span.span()[1]\n",
    "            if(next_string_start<len(word)):\n",
    "                last_excerpt=word[next_string_start:]\n",
    "                if(last_excerpt):\n",
    "                    temp.append(last_excerpt)\n",
    "#             print(temp)\n",
    "        elif((word.count('.')==1)&(word.endswith('.'))):\n",
    "            words=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",word)))\n",
    "            temp=[]\n",
    "            for token in words:\n",
    "                if(token!='.'):\n",
    "                    temp+=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@#’0-9\\'])',token)))\n",
    "                else:\n",
    "                    temp.append('.')\n",
    "        else:\n",
    "            temp=list(filter(lambda elem: elem!='',re.split('([^a-zA-Záéíó@.#’\\'0-9])',word)))\n",
    "        if(temp):\n",
    "            tempList.append(temp)\n",
    "    tweetWordList=flatten(tempList,[])\n",
    "    return tweetWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(strip_op):\n",
    "#     strip_op=word\n",
    "    strip_op=(((strip_op.lstrip(string.punctuation)).rstrip(string.punctuation)).strip()).lower()\n",
    "    strip_op=(strip_op.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "    #strip_op= self.rreplace(self.rreplace(self.rreplace(strip_op,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#     if strip_op.endswith(\"'s\"):\n",
    "#         li = strip_op.rsplit(\"'s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     elif strip_op.endswith(\"’s\"):\n",
    "#         li = strip_op.rsplit(\"’s\", 1)\n",
    "#         return ''.join(li)\n",
    "#     else:\n",
    "#         return strip_op\n",
    "    return strip_op\n",
    "\n",
    "def split_apostrophe(strip_op):\n",
    "    if strip_op.endswith(\"'s\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’s\"):\n",
    "        li = strip_op.rfind(\"’s\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"'S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"'S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    elif strip_op.endswith(\"’S\"):\n",
    "#         print('==>',strip_op)\n",
    "        li = strip_op.rfind(\"’S\")\n",
    "        return [strip_op[:li],strip_op[li:]]\n",
    "    else:\n",
    "        return [strip_op]\n",
    "#     return strip_op\n",
    "    \n",
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/venezuela.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "#tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/roevwade.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/wnut17test.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "# print(len(tweets_unpartitoned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105\n"
     ]
    }
   ],
   "source": [
    "# tweets_1=pd.read_csv(\"/Users/satadisha/Documents/GitHub/billnye.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_2=pd.read_csv(\"/Users/satadisha/Documents/GitHub/pikapika.csv\",sep =',',keep_default_na=False)\n",
    "# tweets_3=pd.read_csv(\"/Users/satadisha/Documents/GitHub/ripcity.csv\",sep =',',keep_default_na=False)\n",
    "\n",
    "# tweets_unpartitoned= pd.concat([tweets_1,tweets_2,tweets_3])\n",
    "print(len(tweets_unpartitoned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with TurboParser NP Chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time1=time.time()\n",
    "df_holder=[]\n",
    "batch_number=0\n",
    "# tweetList=[]\n",
    "sentenceList=[]\n",
    "sentID=0\n",
    "sentID_to_tweet_ID={}\n",
    "mentionList=[]\n",
    "\n",
    "for row in tweets_unpartitoned.itertuples():\n",
    "\n",
    "    index=row.Index\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    hashtags=str(row.HashTags)\n",
    "\n",
    "    user=str(row.User)\n",
    "    tweetText=str(row.TweetText)\n",
    "    annot_raw=\"\"\n",
    "    stanford_candidates=\"\"\n",
    "    ritter_candidates = \"\"\n",
    "    calai_candidates=\"\"\n",
    "\n",
    "    ne_List_final=[]\n",
    "    userMention_List_final=[]\n",
    "    tweetSentenceList=splitSentence(tweetText)\n",
    "    sentenceList.extend(tweetSentenceList)\n",
    "    \n",
    "    for sentence in tweetSentenceList:\n",
    "        sentID_to_tweet_ID[sentID]=int(index)\n",
    "        sentID+=1\n",
    "    \n",
    "    mentions=[]\n",
    "    \n",
    "#     if(len(tweetSentenceList)!=len(str(row.mentions_other_BIO).split(';'))):\n",
    "#         print('index: ',index)\n",
    "    \n",
    "    for sentence_level in str(row.mentions_other).split(';'):\n",
    "        if(sentence_level):\n",
    "            for mention in sentence_level.split(','):\n",
    "                if(mention):\n",
    "                    mentions.append(mention.strip())\n",
    "    \n",
    "#     if(len(tweetSentenceList)!= len(mentions)):\n",
    "#         print('tally: ',len(tweetSentenceList), len(mentions))\n",
    "#         print(tweetSentenceList)\n",
    "#         print(row.mentions_other)\n",
    "#     print(mentions)\n",
    "\n",
    "    mentionList.append(mentions)\n",
    "#     tweetList.append(tweetText)\n",
    "    \n",
    "    for sen_index in range(len(tweetSentenceList)):\n",
    "        sentence=tweetSentenceList[sen_index]\n",
    "        annotation=[]\n",
    "        tweetWordList=getWords(sentence)\n",
    "        enumerated_tweetWordList=[(token,idx) for idx,token in enumerate(tweetWordList)]\n",
    "#         phase1Candidates\n",
    "        dict1 = {'tweetID':str(index), 'sentID':str(sen_index), 'hashtags':hashtags, 'user':user, 'TweetSentence':sentence, 'tweetwordList': enumerated_tweetWordList, 'start_time':now,'entry_batch':batch_number,'annotation':annotation,'stanford_candidates':stanford_candidates,'ritter_candidates':ritter_candidates,'calai_candidates':calai_candidates}\n",
    "        df_holder.append(dict1)\n",
    "\n",
    "#     for candidate in ne_List_final:\n",
    "#         #self.insert_dict (candidate,self.NE_container,candidateBase,index,candidate.sen_index,batch_number)\n",
    "#         candidateText=(((candidate.phraseText.lstrip(string.punctuation)).rstrip(string.punctuation)).strip(' \\t\\n\\r')).lower()\n",
    "#         candidateText=(candidateText.lstrip('“‘’”')).rstrip('“‘’”')\n",
    "#         candidateText= self.rreplace(self.rreplace(self.rreplace(candidateText,\"'s\",\"\",1),\"’s\",\"\",1),\"’s\",\"\",1)\n",
    "#         # if(index==9423):\n",
    "#         #     print(candidateText)\n",
    "#         combined=[]+cachedStopWords+cachedTitles+prep_list+chat_word_list+article_list+day_list\n",
    "#         if not ((candidateText in combined)|(candidateText.isdigit())|(self.is_float(candidateText))):\n",
    "#             self.CTrie.__setitem__(candidateText.split(),len(candidateText.split()),candidate.features,batch_number)\n",
    "\n",
    "#     NE_list_phase1+=ne_List_final\n",
    "\n",
    "#     UserMention_list+=userMention_List_final\n",
    "\n",
    "tweet_sentence_df= pd.DataFrame(df_holder,columns=('tweetID', 'sentID', 'hashtags', 'user', 'TweetSentence','tweetwordList', 'start_time','entry_batch','annotation','stanford_candidates','ritter_candidates','calai_candidates'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_data = ['Guangdong Public University of Foreign Studies is located in Guangzhou.',\n",
    "#              'Lucy is in Kolkata with diamonds.','Bernie Sanders says his fight is for the working class.','elizabeth warren chaired the CBFC',\n",
    "#              'coronavirus is scary!','U.S. is struggling'\n",
    "#             ]\n",
    "\n",
    "# print(len(mentionList),len(tweetList),len(sentID_to_tweet_ID.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_conll=[]\n",
    "try:\n",
    "# #     result_stanford = tweebo_api.parse_stanford(text_data)\n",
    "# #     result_conll = tweebo_api.parse_conll(text_data)\n",
    "    result_conll = tweebo_api.parse_conll(sentenceList)\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[:1000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[1000:2000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[2000:3000])\n",
    "# #     result_conll += tweebo_api.parse_conll(tweetList[3000:])\n",
    "except ServerError as e:\n",
    "    print(f'{e}\\n{e.message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse done!\n",
      "2548\n"
     ]
    }
   ],
   "source": [
    "print('parse done!')\n",
    "print(len(sentenceList))\n",
    "\n",
    "# print(len(tweetList))\n",
    "# print(len(result_conll))\n",
    "\n",
    "# print(sentID_to_tweet_ID[15])\n",
    "# print(result_conll[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just printing the twokenized sentences\n",
    "# sentId=0\n",
    "# df_holder=[]\n",
    "# df_columns=['tweet_id','sentence_id','word']\n",
    "# for sentence in sentenceList:\n",
    "# #     print(sentence)\n",
    "#     sentence_tokens= flatten([split_apostrophe(elem) for elem in getWordsII(sentence)],[])\n",
    "# #     print(sentence_tokens)\n",
    "# #     result=result_conll[sentId]\n",
    "#     for token in sentence_tokens:\n",
    "# #     for result_line in result.split('\\n'):\n",
    "# #         tabs = result_line.split('\\t')\n",
    "#         df_dict={'tweet_id':str(sentID_to_tweet_ID[sentId]),'sentence_id':str(sentId), 'word':token}\n",
    "#         df_holder.append(df_dict)\n",
    "#     sentId+=1\n",
    "\n",
    "# df_out = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print('pre-encoding dataframe: ', len(df_out))\n",
    "\n",
    "# #align mentions with tweets and generate BIO encoding:\n",
    "# encoded_df_columns=['Sentence #','Word','Tag']\n",
    "# encoded_df_holder=[]\n",
    "\n",
    "# # file_text=''\n",
    "# for index, mentions in enumerate(mentionList):\n",
    "#     tweet_sentID_list= df_out[df_out['tweet_id']==str(index)].sentence_id.tolist()\n",
    "#     tweet_word_list= df_out[df_out['tweet_id']==str(index)].word.tolist()\n",
    "#     print('tweet ID:', index,mentions)\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, mentions)\n",
    "    \n",
    "# #     print('tallying list lengths: ',len(tweet_sentID_list),len(tweet_word_list),len(tweet_encoding_list))\n",
    "    \n",
    "#     for encoded_list_index, sentID in enumerate(tweet_sentID_list):\n",
    "#         encoded_df_dict={'Sentence #':tweet_sentID_list[encoded_list_index], 'Word':tweet_word_list[encoded_list_index], 'Tag':tweet_encoding_list[encoded_list_index]}\n",
    "#         encoded_df_holder.append(encoded_df_dict)\n",
    "# #         file_text+=tweet_word_list[encoded_list_index]+'\\t'+tweet_encoding_list[encoded_list_index]+'\\n'\n",
    "# #     file_text+='\\n'\n",
    "\n",
    "# encoded_df_out=pd.DataFrame(encoded_df_holder,columns=encoded_df_columns)\n",
    "# print('post-encoding dataframe: ', len(encoded_df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/tweets_3k_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/venezuela_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billnye_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/pikapika_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/ripcity_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/roevwade_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "# encoded_df_out.to_csv(\"/Users/satadisha/Documents/GitHub/billdeblasio_BIOannotated_twokenized.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "\n",
    "# print(encoded_df_out.tail(40))\n",
    "\n",
    "# import re\n",
    "# mystr='Macron.'\n",
    "# print(split_apostrophe(mystr))\n",
    "# print(mystr.split('-'))\n",
    "# re.split(\"(-)\",mystr)\n",
    "# if((mystr.count('.')==1)&(mystr.endswith('.'))):\n",
    "#     temp=list(filter(lambda elem: elem!='',re.split(\"(\\.)\",mystr)))\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "# conll_nounPhrase_chunking(conll_results)\n",
    "\n",
    "def getConnectedComponents(visited, adjList):\n",
    "    cc=[]\n",
    "    cc_positions=[]\n",
    "    nodeList=list(visited.keys())\n",
    "#     print('**',visited,adjList)\n",
    "    for ind in range(len(nodeList)):\n",
    "        node=nodeList[ind]\n",
    "#         print('==>',node)\n",
    "        if not(visited[node][0]):\n",
    "            if(ind>0):\n",
    "                last.sort(key = int)\n",
    "                if('^' in posStr):\n",
    "#                     print('::',last)\n",
    "                    candidateStringInner=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "                    cc.append(candidateStringInner)\n",
    "                    cc_positions.append(last)\n",
    "            last=[]\n",
    "            posStr=''\n",
    "            bfs=[node]\n",
    "            while(bfs):\n",
    "                curr=bfs.pop(0)\n",
    "                visited[curr][0]=True\n",
    "                last.append(curr)\n",
    "                posStr+=visited[curr][2]\n",
    "                for neighbour in adjList[curr]:\n",
    "                    if(not visited[neighbour][0]):\n",
    "                        bfs.append(neighbour)\n",
    "        ind+=1\n",
    "    last.sort(key = int)\n",
    "    if('^' in posStr):\n",
    "        candidateString=(' '.join([visited[elem][1] for elem in last])).strip()\n",
    "        cc.append(candidateString)\n",
    "        cc_positions.append(last)\n",
    "#     print('connected components:')\n",
    "#     print(cc)\n",
    "    return cc, cc_positions\n",
    "\n",
    "def conll_nounPhrase_chunking(tabbed_entries):\n",
    "    spans=[]\n",
    "    span=[]\n",
    "    for tabbed_entry in tabbed_entries:\n",
    "        entry=[]\n",
    "        if((tabbed_entry[3]==proper_noun_tag)|(tabbed_entry[3]==common_noun_tag)):\n",
    "            entry=tabbed_entry\n",
    "        if(tabbed_entry[3]==prep_tag):\n",
    "            head=int(tabbed_entry[6])-1\n",
    "            if(head>0):\n",
    "                head_entry=tabbed_entries[head]\n",
    "                if((head_entry[3]==proper_noun_tag)|(head_entry[3]==common_noun_tag)):\n",
    "                    entry=tabbed_entry\n",
    "        if(entry):\n",
    "            if(int(entry[0])>1):\n",
    "                if(span):\n",
    "                    if((int(entry[0])-int(span[-1][0]))>1):\n",
    "                        spans.append(span)\n",
    "                        span=[entry]\n",
    "                    else:\n",
    "                        span.append(entry)\n",
    "                else:\n",
    "                    span=[entry]\n",
    "            else:\n",
    "                span=[entry]\n",
    "    if(spans):\n",
    "        if(spans[-1][0]!=span[0]):\n",
    "            spans.append(span)\n",
    "    else:\n",
    "        if(span):\n",
    "            spans.append(span)\n",
    "    \n",
    "    final_spans=[]\n",
    "    final_spans_positions=[]\n",
    "    for span in spans:\n",
    "        minIndex=int(span[0][0])\n",
    "        maxIndex=int(span[-1][0])\n",
    "        visited={}\n",
    "        adjList={}\n",
    "        for entry in span:\n",
    "            visited[entry[0]]=[False,entry[1],entry[3]]\n",
    "            if(entry[0] not in adjList.keys()):\n",
    "                adjList[entry[0]]=[]\n",
    "            dependency=entry[6]\n",
    "            if((int(dependency)>=minIndex)&(int(dependency)<=maxIndex)):\n",
    "                adjList[entry[0]].append(dependency)\n",
    "                if(dependency not in adjList.keys()):\n",
    "                    adjList[dependency]=[]\n",
    "                adjList[dependency].append(entry[0])\n",
    "        retTup=getConnectedComponents(visited,adjList)\n",
    "        final_spans.extend(retTup[0])\n",
    "        final_spans_positions.extend(retTup[1])\n",
    "    return final_spans,final_spans_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_F1(annotated_mention_list,output_mentions_list):\n",
    "\n",
    "    # print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    all_postitive_counter_inner=len(output_mentions_list)\n",
    "    while(annotated_mention_list):\n",
    "        if(len(output_mentions_list)):\n",
    "            annotated_candidate= annotated_mention_list.pop()\n",
    "            if(annotated_candidate in output_mentions_list):\n",
    "                output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                tp_counter_inner+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "            break\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "    fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "    print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "    \n",
    "    precision=(tp_counter_inner)/(tp_counter_inner+fp_counter_inner)\n",
    "    recall=(tp_counter_inner)/(tp_counter_inner+fn_counter_inner)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized.txt\", \"w\")\n",
    "# file_text1=''\n",
    "\n",
    "# f = open(\"/Users/satadisha/Documents/GitHub/train1_twokenized_POSTAG.txt\", \"w\")\n",
    "# file_text=''\n",
    "\n",
    "# candidates=[]\n",
    "# # sentId=0\n",
    "# tweetId=0\n",
    "# CTrie=trie.Trie(\"ROOT\")\n",
    "# # for sentence in sentenceList:\n",
    "# for tweet in tweetList:\n",
    "# #     result=result_conll[sentId]\n",
    "#     result=result_conll[tweetId]\n",
    "#     tweet_word_list=[]\n",
    "#     conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "#     for result_line in conll_results:\n",
    "#         file_text+=result_line[1]+'\\t'+result_line[3]+'\\n'\n",
    "#         tweet_word_list+=result_line[1]\n",
    "# #         file_text1+=result_line[1]+'\\tO'+'\\n'\n",
    "# #         print('result_line: ', result_line)\n",
    "#     file_text+='\\n'\n",
    "#     tweetId+=1\n",
    "#     tweetMentions=mentionList[tweetId]\n",
    "#     tweet_encoding_list= get_encoding_seq(tweet_word_list, tweetMentions)\n",
    "#     print(len(tweet_word_list),len(tweet_encoding_list))\n",
    "#     for ind, word in enumerate(tweet_word_list):\n",
    "#         file_text1+=word+'\\t'+tweet_encoding_list[ind]+'\\n'\n",
    "#     file_text1+='\\n'\n",
    "# #     sentId+=1\n",
    "#     tweetId+=1\n",
    "\n",
    "# f.write(file_text)\n",
    "# f.close()\n",
    "\n",
    "# f1.write(file_text1)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2548 1105 2548\n"
     ]
    }
   ],
   "source": [
    "candidates=[]\n",
    "sentId=0\n",
    "CTrie=trie.Trie(\"ROOT\")\n",
    "phase1outputs=[]\n",
    "for sentence in sentenceList:\n",
    "    result=result_conll[sentId]\n",
    "    conll_results=[result_line.split('\\t') for result_line in result.split('\\n')]\n",
    "    sentence_candidates,sentence_candidates_positions=conll_nounPhrase_chunking(conll_results)\n",
    "#     print(sentence)\n",
    "#     print(conll_results)\n",
    "#     print(sentence_candidates)\n",
    "\n",
    "    candidate_ind=0\n",
    "    phase1Out=''\n",
    "#     for candidate in ne_List_allCheck:\n",
    "#         position = '*'+'*'.join(str(v) for v in candidate.position)\n",
    "#         position=position+'*'\n",
    "#         candidate.set_sen_index(sen_index)\n",
    "#         phase1Out+=(((candidate.phraseText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "\n",
    "    for candidateText in sentence_candidates:\n",
    "#         print(candidateText)\n",
    "        candidateText=candidateText.lower()\n",
    "        phase1outputs.append((((candidateText).lstrip(string.punctuation)).strip()))\n",
    "        position = '*'+'*'.join(str(v) for v in sentence_candidates_positions[candidate_ind])\n",
    "        position=position+'*'\n",
    "#         print(candidateText,sentence_candidates_positions[candidate_ind])\n",
    "        CTrie.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "        candidate_ind+=1\n",
    "        phase1Out+=(((candidateText).lstrip(string.punctuation)).strip())+ '::'+str(position)+\"||\" \n",
    "        \n",
    "#     candidates.append(sentence_candidates)\n",
    "    candidates.append(phase1Out)\n",
    "\n",
    "    sentId+=1\n",
    "#     print('===========')\n",
    "print(len(sentenceList),len(tweets_unpartitoned),len(candidates))\n",
    "\n",
    "tweet_sentence_df['phase1Candidates']=candidates\n",
    "time2=time.time()\n",
    "# print(tweet_sentence_df['phase1Candidates'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "candidates=CTrie.displayTrie(\"\",[])\n",
    "print(len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  2548 2548 272\n",
      "-0.3769889624645107\n",
      "For entities:  (158, 6)\n",
      "For non-entities:  (112, 6)\n",
      "For ambiguous:  (2, 6)\n",
      "For entities:  (158, 6)\n",
      "For non-entities:  (112, 6)\n",
      "For ambiguous:  (2, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  2520 incomplete tweets:  28\n",
      "16\n",
      "16\n",
      "final tally:  2548 2548\n",
      "524:  524    [[]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2 = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2, converted_candidates, complete_tweet_dataframe_grouped_df_sorted= Phase2.executor(max_batch_value,tweet_sentence_df,CTrie,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df)\n",
    "time3=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_unpartitoned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>[0, 1, 2]</td>\n",
       "      <td>[Venezuela, Venezuela, Venezuela]</td>\n",
       "      <td>[RheallN, RheallN, RheallN]</td>\n",
       "      <td>[This is a good thread on the backstory that h...</td>\n",
       "      <td>[point in #venezuela::*15*16*17*||, trump admi...</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "      <td>[[], [trump administration], []]</td>\n",
       "      <td>[True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(us, b)], [(trump administration, g)], [(pea...</td>\n",
       "      <td>[[], [trump administration], []]</td>\n",
       "      <td>[[], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[France, MayDay, France, MayDay, France, MayDa...</td>\n",
       "      <td>[Abreus, Abreus, Abreus, Abreus, Abreus]</td>\n",
       "      <td>[Wow., Absolute chaos today in #France on #May...</td>\n",
       "      <td>[, chaos today in #france on #mayday::*2*3*4*5...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(police, b)], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[France, MayDay, France, MayDay, France, MayDa...</td>\n",
       "      <td>[Clare_Scotland, Clare_Scotland, Clare_Scotlan...</td>\n",
       "      <td>[Wow., Absolute chaos today in #France on #May...</td>\n",
       "      <td>[, chaos today in #france on #mayday::*2*3*4*5...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(police, b)], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan, nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5]</td>\n",
       "      <td>[Venezuela, TrishRegan, Venezuela, TrishRegan,...</td>\n",
       "      <td>[MAGALOGAN2379, MAGALOGAN2379, MAGALOGAN2379, ...</td>\n",
       "      <td>[TONIGHT - hear directly from ⁦@realDonaldTrum...</td>\n",
       "      <td>[⁦::*6*||venezuela::*10*||, , tonigh::*3*||pre...</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "      <td>[[], [], [pres of venezuela], [venezuela], [],...</td>\n",
       "      <td>[True, True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[], [], [(pres of venezuela, g)], [(venezuela...</td>\n",
       "      <td>[[], [], [pres of venezuela], [venezuela], [],...</td>\n",
       "      <td>[[], [], [], [], [], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan, nan, nan, nan, nan]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "      <td>[Venezuela, Venezuela, Venezuela, Venezuela, V...</td>\n",
       "      <td>[andreaskapp, andreaskapp, andreaskapp, andrea...</td>\n",
       "      <td>[Watch @anyaparampil's great interview demolis...</td>\n",
       "      <td>[trump::*7*||push for regime change in #venezu...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "      <td>[[trump], [trump, neocon nutjobs pompeo, bolto...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>[[(trump, g)], [(trump, g), (neocon nutjobs po...</td>\n",
       "      <td>[[trump], [trump, neocon nutjobs pompeo, bolto...</td>\n",
       "      <td>[[], [], [], [], []]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID                           index         entry_batch  \\\n",
       "0        0                 [nan, nan, nan]           [0, 0, 0]   \n",
       "1        1       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "2        2       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "3        3  [nan, nan, nan, nan, nan, nan]  [0, 0, 0, 0, 0, 0]   \n",
       "4        4       [nan, nan, nan, nan, nan]     [0, 0, 0, 0, 0]   \n",
       "\n",
       "               sentID                                           hashtags  \\\n",
       "0           [0, 1, 2]                  [Venezuela, Venezuela, Venezuela]   \n",
       "1     [0, 1, 2, 3, 4]  [France, MayDay, France, MayDay, France, MayDa...   \n",
       "2     [0, 1, 2, 3, 4]  [France, MayDay, France, MayDay, France, MayDa...   \n",
       "3  [0, 1, 2, 3, 4, 5]  [Venezuela, TrishRegan, Venezuela, TrishRegan,...   \n",
       "4     [0, 1, 2, 3, 4]  [Venezuela, Venezuela, Venezuela, Venezuela, V...   \n",
       "\n",
       "                                                user  \\\n",
       "0                        [RheallN, RheallN, RheallN]   \n",
       "1           [Abreus, Abreus, Abreus, Abreus, Abreus]   \n",
       "2  [Clare_Scotland, Clare_Scotland, Clare_Scotlan...   \n",
       "3  [MAGALOGAN2379, MAGALOGAN2379, MAGALOGAN2379, ...   \n",
       "4  [andreaskapp, andreaskapp, andreaskapp, andrea...   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [This is a good thread on the backstory that h...   \n",
       "1  [Wow., Absolute chaos today in #France on #May...   \n",
       "2  [Wow., Absolute chaos today in #France on #May...   \n",
       "3  [TONIGHT - hear directly from ⁦@realDonaldTrum...   \n",
       "4  [Watch @anyaparampil's great interview demolis...   \n",
       "\n",
       "                                    phase1Candidates  \\\n",
       "0  [point in #venezuela::*15*16*17*||, trump admi...   \n",
       "1  [, chaos today in #france on #mayday::*2*3*4*5...   \n",
       "2  [, chaos today in #france on #mayday::*2*3*4*5...   \n",
       "3  [⁦::*6*||venezuela::*10*||, , tonigh::*3*||pre...   \n",
       "4  [trump::*7*||push for regime change in #venezu...   \n",
       "\n",
       "                 annotation       stanford_candidates  \\\n",
       "0              [[], [], []]              [[], [], []]   \n",
       "1      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "2      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "3  [[], [], [], [], [], []]  [[], [], [], [], [], []]   \n",
       "4      [[], [], [], [], []]      [[], [], [], [], []]   \n",
       "\n",
       "                                     output_mentions  \\\n",
       "0                   [[], [trump administration], []]   \n",
       "1                               [[], [], [], [], []]   \n",
       "2                               [[], [], [], [], []]   \n",
       "3  [[], [], [pres of venezuela], [venezuela], [],...   \n",
       "4  [[trump], [trump, neocon nutjobs pompeo, bolto...   \n",
       "\n",
       "                           completeness             current_minus_entry  \\\n",
       "0                    [True, True, True]                 [0.0, 0.0, 0.0]   \n",
       "1        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "2        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "3  [True, True, True, True, True, True]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "4        [True, True, True, True, True]       [0.0, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                               candidates_with_label  \\\n",
       "0  [[(us, b)], [(trump administration, g)], [(pea...   \n",
       "1                    [[], [], [(police, b)], [], []]   \n",
       "2                    [[], [], [(police, b)], [], []]   \n",
       "3  [[], [], [(pres of venezuela, g)], [(venezuela...   \n",
       "4  [[(trump, g)], [(trump, g), (neocon nutjobs po...   \n",
       "\n",
       "                                only_good_candidates      ambiguous_candidates  \n",
       "0                   [[], [trump administration], []]              [[], [], []]  \n",
       "1                               [[], [], [], [], []]      [[], [], [], [], []]  \n",
       "2                               [[], [], [], [], []]      [[], [], [], [], []]  \n",
       "3  [[], [], [pres of venezuela], [venezuela], [],...  [[], [], [], [], [], []]  \n",
       "4  [[trump], [trump, neocon nutjobs pompeo, bolto...      [[], [], [], [], []]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'chaos today in #france on #mayday', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'trump', 'push for regime change in #venezuela', 'trump', 'neocon nutjobs pompeo', 'bolton', 'abrams', 'trump', 'maduro', 'venezuela', 'president of #cuba', 'dictator raul castro', 'message as #venezuela minister of defense', 'castro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'los teques', 'cities of #venezuela', 'maduro', 'venezuela', 'trump', 'cuba', 'canada', 'maduro', 'bolton', 'abrams', 'erdogan 📞 putin', 'maduro', 'media crackdown in venezuela', 'maduro', 'venezuela', 'secretary of state mike pompeo', 'united states', 'venezuela', 'venezuela', 'nicolás maduro', 'maduro', 'caracas', 'venezuela like', 'guayana marchers', 'venezuela', 'capital #caracas on', 'day of #operacionlibertad', 'caracas', 'venezuela like', 'guayana marchers', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'vpn services', 'venezuela', 'scenes in venezuela as', 'maduro', 'people of #venezuela', 'u.s. secretary of state', 'call with russian foreign minister', 'venezuela', 'maduro', 'us', 'venezuela', 'people of #venezuela', 'maduro regime', 'maduro', 'venezuela', 'pl #lainminencia fi #venezuela', 'venezuela', 'venezuela', 'maduro', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'venezuela', 'maduro', 'venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'russia', 'u.s. of consequences', 'venezuela', 'novel from author juan guaido about', 'venezuela', 'venezuela', 'narco murderers', 'venezuela', 'usmc', 'russia', 'u.s. of consequences', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'america', 'usa #maga', 'maduro', 'venezuela', 'el diario de dmfusion', 'venezuela', 'maduro', 'venezuela', 'gnb in', 'city of caracas', 'altamira', 'caracas', 'gnb', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'venezuela’s case', 'maduro', 'venezuela', 'cuba', 'lima group', 'venezuelans', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'venezuela’s case', 'maduro', 'venezuela', 'maduro', 'venezuela', 'scenes in venezuela as', 'maduro', 'msnbc', 'altamira', 'caracas', 'venezuela', 'venezuela', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'usmc', 'maduro', 'venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'maduro', 'venezuela', 'venezuela', 'guaido', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'venezuela', 'jewel of latin america', 'democrat socialists like bernie sanders', 'venezuela', 'united states', 'maduro', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'president hillary', 'gnb in caracas', 'adm', 'faller', 'hasc on #venezuela crisis', 'iran', 'flights from tehran', 'cuba', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'russia', 'u.s. of consequences', 'in venezuela', 'venezuela', 'govt troops', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'maduro', 'venezuela', 'capital #caracas on', 'day of #operacionlibertad', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'points of #caracas', 'may', 'operacionlibertad', 'maduro', 'venezuela', 'pl #lainminencia fi #venezuela', 'camera assistant rubén brito', 'altamira interchange', 'area of la carlota', \"spanish government's position on\", 'venezuela people', 'catalonia', 'caracas', 'catalonia', 'gnb in', 'city of caracas', 'gnb', 'city of valencia', 'carabobo', 'usmc', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'venezuela', 'maduro', 'venezuela', 'gnb in', 'city of caracas', 'maduro', 'venezuela', 'usmc', 'maduro', 'venezuela', 'crabcon 🔵', 'situation in #venezuela', 'la carlota', 'air base in caracas', 'adm', 'faller', 'hasc on #venezuela crisis', 'china', 'maduro', 'venezuela', 'venezuela', 'venezuelans', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'maduro', 'american', 'maduro', 'america', 'move for democracy in venezuela', 'venezuela', 'venezuelans', 'people of #venezuela', 'chaos today in #france on #mayday', 'venezuela', 'thugocracy', 'gru', 'chavismo', 'venezuela', \"venezuela's\", 'juan guaidó', 'us', 'maduro', 'pepe mujica', 'maduro’s', 'venezuela', 'corbynista kryptonite', 'maduro', 'venezuela', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'ron paul', 'guaido', 'venezuelan coup-creators', 'zero hedge #bringtroopshome #stopww3 #endlesswars #maga #trump', 'maduro', 'death squads', 'faes', 'caracas #venezuela', 'may1', 'fire/harshly', 'attn #canada', 'maduro', 'venezuela', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'btrs', 'bolivar state', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'scenes in #venezuela', 'of #cuba', 'maduro', 'venezuela', 'move for democracy in venezuela', 'venezuela', 'venezuelans', 'currency for members in #california', 'xrp', 'brazil', 'colorado/ florida', 'u.s.', 'maduro', 'venezuela', 'children in #yemen to', 'mike pompeo', 'people of #venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'shot in #venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'maduro', 'venezuela', 'juanguido supporters', 'venezuela', 'maduro', 'venezuela', 'maduro', 'chaos today in #france on #mayday', 'venezuela', 'trish regan', 'venezuela crisis', 'venezuela', 'maduroregime', 'altamira', 'maduro', 'venezuela', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'capital #caracas on', 'day of #operacionlibertad', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'transit in #venezuela', '\\u2066', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'venezuela', 'maduro’s policies', 'us', 'uk headline news', 'us', 'us', 'uk media', 'maduro', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'comment from pepe mujica about demonstrators in #venezuela', 'maduro', 'maduro', 'us', 'maduro', 'venezuela', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama', 'hillary coup', 'chaos today in #france on #mayday', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'socialism', 'policestate in #america', 'guaido protestors', 'gnb vehicle near la carlota airbase', 'maduro', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'corbynista kryptonite', 'russia', 'u.s. of consequences', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'fort russ news', 'maduro', 'maduro', 'venezuela', 'example of #venezuela', 'utopia', 'u.s. secretary of state', 'call with russian foreign minister', 'venezuela', 'ron paul', 'guaido', 'venezuelan coup-creators', 'zero hedge #bringtroopshome #stopww3 #endlesswars #maga #trump', 'events in #venezuela', 'trump administration responses to', 'u.s.', 'juanguido supporters', 'venezuela', 'venezuelans against', 'venezuelans', 'maduro', 'venezuela', 'maduro', 'venezuela', 'gun🇺🇸🇺🇸🇺🇸', 'venezuela’s case', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'adm', 'faller', 'hasc on #venezuelan crisis', 'people of #venezuela', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'maduro', 'venezuela', 'maduro', 'venezuela', 'anti-government protests in france', 'twitter', 'msm', 'bbc', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'maduro', 'maduro', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'faller', 'united states', 'government of #venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'venezuelan gov’t', 'bolivar', 'exchange rate in #venezuela for', 'ves/usd', 'maduro', 'ves/usd', 'venezuelans', 'us interventions in', 'rise of isis', 'libya', 'us intervention in #venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'msnbc', 'venezuela', 'power #controls everything', 'venezuela', 'venezuela', 'maduro', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'maduro', 'venezuela', 'usa foriegn policy', 'libya', 'yemen', 'gaza', 'syria', 'honduras', 'iran', 'somalia', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuela', 'us', 'bernie sanders', 'comrades in congress', 'venezuela', 'jewel of latin america', 'democrat socialists like bernie sanders', 'venezuela', 'united states', 'venezuela', 'venezuela', 'legitamite president of venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'u.s. admiral', \"message for #venezuela's military\", 'maduro govt', 'chavez', 'mestizos', 'demonstrators against chavez', 'guaido', 'coup against maduro #venezuela 🇻🇪', 'us politicians', 'lot of #fakenews about', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'venezuelans', 'streets of #venezuela', 'support of #operaci ónlibertad', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', \"venezuela's inflation rate\", 'gnb in', 'city of caracas', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'msnbc', 'venezuela', 'power #controls everything', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama', 'hillary coup', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'venezuela', 'haiti', 'doc duvalier', 'scenes in venezuela as', 'maduro', 'venezuelans', 'maduro', 'venezuela', 'venezuelans', 'maduro', 'venezuela', 'in #venezuela', 'lavrov to #pompeo', 'venezuela', 'movement in caracas', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'la carlota air base', 'caracas', 'venezuela', 'russians', 'maduro', 'venezuela', 'cuba', 'usa', 'maduro', 'venezuela', 'moa', 'guaidó', 'snookered', 'white house', 'united states', 'trishregan', '8pme #foxbusiness', 'maduro', 'venezuela', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'beatriz becerra on twitter', 'venezuela', 'sb', 'deportation of venezuelans', 'maduro dictatorship', 'venezuelans', 'in florida', 'mossos', 'u.s.', 'guaido', 'maduro', 'u.s.', 'venezuela', 'freeland', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'south american country', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'russia', 'maduro', 'venezuela', 'cuba regime', 'destruction of #venezuela', 'venezuelans', 'united states', 'venezuela', 'cuba', 'pompeo', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'nicolas maduro', 'chaos today in #france on #mayday', 'venezuela', 'usmc', 'maduro', 'venezuela', 'russia', 'u.s. of consequences', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'govt', 'young voices', 'daniel dimartino for', 'in venezuela', 'on fox news', 'chaos today in #france on #mayday', 'venezuela', \"maduro's side\", 'scenes in #venezuela', 'smith', 'caracas capitol of #venezuela', 'supporters of president juan guaido', '👏👏👏👏', 'starbucks commies that', 'venezuela', 'lavrov', 'venezuela', 'camera assistant rubén brito', 'altamira interchange', 'area of la carlota', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'in #barquisimeto today', 'venezuela', 'nicolás maduro', 'maduro', 'maduro', 'venezuela', 'maduro', 'venezuela', 'in #barquisimeto today', 'maduro', 'venezuela', 'venezuela', 'paris', 'adm', 'faller', 'hasc on #venezuela crisis', 'iran', 'flights from tehran', 'cuba', 'maduro', 'venezuela', 'russia', 'reuters', 'pentagon', 'venezuela disinfo', 'venezuelans', 'united states', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'people of #venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'florida', 'usmc', 'venezuela', 'airdrop #venezuela', 'narco murderers', 'venezuela', 'maduro', 'venezuela', 'movement in caracas', 'gods speed to', 'people of #venezuela', 'venezuela', 'caracas capitol of #venezuela', 'supporters of president juan guaido', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'admirer of maduro', 'us', 'chavez', 'president of #venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'maduro', 'venezuela', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'guaido', 'chaos today in #france on #mayday', 'venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'yemen', 'saudi arabia', 'us -uk', 'concocting lies about #venezuela to', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'maduroregime', 'scenes in venezuela as', 'chaos today in #france on #mayday', 'venezuela', 'anon #venezuela account', 'friends like donald trump', 'mike pence', 'john bolton', 'elliott abrams', 'ted cruz', 'marco rubio', 'juan guaido', 'freedom for venezuelans', 'american', 'usmc', 'thousands of children from #venezuela', 'colombia', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', '\\u2066', 'on #venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'maduro', 'venezuela', 'children in #yemen to', 'mike pompeo', 'people of #venezuela', 'ar-57 uppers', 'company in kent', 'washington', 'p90 magazine on', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'scott', 'broward', 'dws election', 'maduro people', 'police vehicles', 'trump', 'venezuela', 'colombia', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama', 'hillary coup', 'maduro', 'venezuela', 'comment from pepe mujica about', 'demonstrators in #venezuela', 'maduro', 'venezuela', 'pepe mujica', 'maduro’s', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'corbynista kryptonite', 'caracas', 'venezuela like', 'guayana marchers', 'maduro death squads', 'faes', 'caracas #venezuela', 'may1', 'fire/harshly', 'attn #canada', 'uk', 'us', 'us sanctions', 'maduro', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'maduro', 'venezuelan national assembly', 'nicolas maduro', 'maduro', 'venezuela', 'maduro speech', 'utc', 'maduro', 'venezuela', 'syria', 'people of #venezuela against', 'maduro', 'venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'gods speed to', 'people of #venezuela', 'venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'history of #unitedstates', 'movement of #leopoldolopez from', 'streets of #venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'maduro', 'venezuela', 'united states', 'venezuela', 'venezuela', 'guardian', 'sisters in washington', '1may', 'maduro regime', 'capital #caracas', 'faller', 'aplicacion del', 'maduro', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'trudeau liberals', 'federal ndp', 'opposition protests in altamira', 'couple of blocks from altamira', 'march in vz history', 'guaido', 'adm', 'faller', 'hasc on #venezuelan crisis', 'people of #venezuela', 'in #barquisimeto today', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'corbynista kryptonite', 'ar', 'kent', 'washington', 'p90 magazine on', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'united states', 'venezuela', 'shot in #venezuela', 'maduro', 'venezuela', 'ottawa #canada', 'latin', 'cuba', 'venezuela', 'cia', 'boeing', 'n881yv', 'bogota', '21air', 'shipment of arms to #venezuela', 'february', 'venezuela', 'guaido', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'mayday', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'maduro', 'venezuela', 'venezuelans', 'venezuela', 'maduro', 'venezuela', 'san diego', 'us interventions in', 'rise of isis', 'libya', 'us intervention in #venezuela', 'msm', 'bbc', 'venezuela', 'maduro', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'united states', 'trishregan', '8pme #foxbusiness', 'maduro', 'venezuela', 'maduro', 'venezuela', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'gnb in', 'city of caracas', 'venezuela', 'usmc', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'gnb vehicle near la carlota airbase', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'gnb in', 'city of caracas', 'maduro', 'venezuela', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama-hillary coup', 'maduro', 'venezuela', 'comment from pepe mujica about', 'demonstrators in #venezuela', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'maduro', 'venezuela', 'capital #caracas on', 'day of #operacionlibertad', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'guaido', 'btr-s', 'bolivar state', 'venezuelans', 'streets of #venezuela', 'support of #operaci ónlibertad', 'maduro', 'venezuela', 'maduro', 'venezuela', 'pat robertson', 'us military', 'president nicolas maduro', 'christian', 'maduro', 'venezuela', 'statistic on #venezuela', 'maduro', 'venezuela', 'president of #cuba', 'dictator raul castro', 'message as #venezuela minister of defense', 'castro', 'venezuela', 'anon #venezuela account', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'trump', 'embargo against #cuba', 'venezuela', 'peace', 'maduro', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'maduro death squads', 'faes', 'caracas #venezuela', 'may1', 'fire/harshly', 'attn #canada', 'maduro', 'venezuela', 'maduro', 'venezuela', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'maduro', 'venezuela', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'maduro', 'venezuela', 'maduro', 'venezuela', 'venezuelans', 'maduro', 'venezuela', 'starbucks commies that', 'venezuela', 'maduro', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'narco murderers', 'venezuela', 'u.s. war', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'blackwater', 'iraq', 'venezuela', 'us soil', 'maduro', 'venezuela', 'gnb in', 'city of caracas', 'us government', 'altamira', 'caracas', 'maduro', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'venezuela’s case', 'maduro', 'venezuela', 'venezuela government', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'maduro', 'venezuela', 'venezuela', 'maduro speech', 'utc', 'venezuela', 'juanguaido', 'aplicacion del', 'maduro', 'venezuela', 'maduro', 'venezuela', 'mayday', 'situation in #venezuela', 'chaos today in #france on #mayday', 'venezuela', 'admirer of maduro', 'us', 'chavez', 'president of #venezuela', 'maduro', 'venezuela', 'u.s.', 'moves in #venezuela', 'reuters', 'venezuela', 'usmc', 'venezuela', 'peace', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'capital #caracas on', 'day of #operacionlibertad', 'venezuela’s case', 'maduro', 'venezuela', 'russia', 'u.s. of consequences', 'venezuela', 'venezuela', 'events in venezuela', 'classic us regime change coup', 'venezuela', 'cia', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'venezuela’s case', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'healthcare professionals from #venezuela', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'scenes in venezuela as', 'pro-maduro', 'faller', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'paramilitaries', 'venezuela', 'in #venezuela', 'lavrov to #pompeo', 'narco murderers', 'venezuela', 'venezuela’s case', 'maduro', 'venezuela', 'venezuela', 'rory carroll', 'man in caracas', 'bolivarian revolution', 'venezuela', 'los teques', 'cities of #venezuela', 'los teques', 'cities of #venezuela', 'maduro', 'venezuela', 'caracas', 'venezuela', 'maduro', 'venezuela', 'youtube', 'bing', 'google services', 'venezuela', 'guaido', 'caracas', 'big time', 'kinder', 'maduro', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'nicolás maduro', 'maduro', 'venezuela’s case', 'maduro', 'venezuela', 'uss theodore roosevelt', 'maduro', 'venezuela', 'maduro', 'venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'scenes in #venezuela', 'of #cuba', 'order in #venezuela', 'people of venezuela', 'venezuela', 'los teques', 'cities of #venezuela', 'france', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'santafe #caracas', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'no-show fiasco in #venezuela', 'bay of pigs', 'allen dulles', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'venezuela', 'nicolás maduro', 'maduro', 'pompeo', 'gnb in', 'city of caracas', 'maduro', 'situation in #venezuela', 'venezuelans', 'streets of #venezuela', 'support of #operaci ónlibertad', 'people of #venezuela', 'yellowvests', 'france', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuela’s case', 'maduro', 'venezuela', 'venezuela', 'gnb in', 'city of caracas', 'us', 'venezuela via', 'venezuelans', 'shot in #venezuela', 'venezuela', 'venezuela', 'defense secretary shanahan', 'trip to europe', 'venezuela', 'bolton', 'nsc l', 'weds', 'u.s.', 'russia', 'cuba', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'anti-government protests in france', 'twitter', 'cia stooge', 'crimes against #venezuela', 'venezuela', 'venezuela', 'venezuela’s case', 'maduro', 'venezuela', 'streets of #venezuela', 'us', 'venezuela', 'maduro', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'starbucks commies that', 'venezuela', 'venezuela', 'venezuela', 'narco murderers', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'los teques', 'cities of #venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', '❗ comment by', 'department on developments in #venezuela 🇻🇪', 'maduro', 'venezuela', 'u.s. war', 'venezuela', 'gnb in', 'city of caracas', 'people of #venezuela', 'capital #caracas on', 'day of #operacionlibertad', 'cuba', 'lima group', 'venezuelans', 'chaos today in #france on #mayday', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'maduro', 'american', 'maduro', 'america', 'armed forces', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'adm', 'faller', 'hasc on #venezuelan crisis', 'people of #venezuela', 'scenes in venezuela as', 'pro-maduro', 'caracas', 'venezuela like', 'guayana marchers', 'venezuela', 'venezuela', 'starbucks commies that', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'chaos today in #france on #mayday', 'venezuela', 'ron paul', 'guaido', 'venezuelan coup-creators', 'zero hedge #keeptroopshome #endlesswars', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuelans', 'people of #venezuela than', 'people of #yemen', 'saudi arabia', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'scenes in venezuela as', 'pro-maduro', 'scenes in #venezuela', 'maduro', 'venezuela', 'cubans with', 'maduro', 'venezuela', 'venezuela', 'gnb', 'city of caracas', 'chaos today in #france on #mayday', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'ted', 'barr', 'chaos today in #france on #mayday', 'venezuela', 'example of #venezuela', 'utopia', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'maduro', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'maduro', 'venezuela', 'united states', 'venezuela', 'in #barquisimeto today', 'chaos today in #france on #mayday', 'venezuela', 'smith', 'caracas', 'venezuela like', 'guayana marchers', 'maduro', 'venezuela', '✈️ tc-tsr', 'russia', 'caracas', 'punta cana', 'chaos today in #france on #mayday', 'venezuela', 'venezuela', 'maduroregime', 'people as #nicolasmaduro attempts to', \"military's represión\", 'us', 'venezuela', 'maduro', 'venezuela', 'la guardia', 'venezuela’s case', 'maduro', 'venezuela', 'twitter account of', 'correo del orinoco', 'va', 'censorship by twitter', 'starbucks commies that', 'venezuela', 'ron paul', 'venezuelan coup-creators', 'maduro', 'venezuela', 'east coast', 'west coast part', 'shot in #venezuela', 'venezuela', 'paris', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama-hillary coup', 'coup in #venezuela', 'riots in #honduras', 'us puppet', 'obama-hillary coup', 'gnb in', 'city of caracas', 'venezuela', 'president of #venezuela', 'oas', 'cuba', 'scenes in venezuela as', 'pro-maduro', 'los teques', 'cities of #venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'los teques', 'cities of #venezuela', 'paramilitaries', 'venezuela', 'police', 'mayday2019 crowds', 'paris', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'clashes in #venezuela', 'trump dives into', 'muslim brotherhood', 'isis chief', 'scenes in venezuela as', 'pro-maduro', 'maduro', 'venezuela', 'usmc', 'narco murderers', 'venezuela', 'people of #venezuela', 'bolton', 'trump', 'blackwater', 'mercenaries for #venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuela', 'caracas', 'venezuela like', 'guayana marchers', 'venezuela’s case', 'maduro', 'venezuela', 'maduro', 'venezuela', 'corbynista kryptonite', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'maduro supporters', 'stream from #venezuelalibre #mayday', 'maduro', 'venezuela', 'us', 'venezuela #standoff', 'maduro', 'venezuela', 'venezuela', 'lo siento', 'caracas', 'venezuela like', 'guayana marchers', 'pide', 'venezuela', 'jewel of latin america', 'democrat socialists like bernie sanders', 'venezuela', 'united states', 'maduro', 'venezuela', 'ict4d2019 conference', 'venezuela', 'zimbabwe', 'venezuelans', 'streets of #venezuela', 'support of #operaci ónlibertad', 'poor of #venezuela', 'gnb in', 'city of caracas', 'prayers', 'venezuela', 'hybrid war on #venezuela', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'pepe mujica', 'maduro’s', 'venezuela', 'venezuela', 'venezuela', 'canada', 'people of #venezuela in', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'movement in caracas', 'msnbc', 'venezuela', 'power #controls everything', 'scenes in venezuela as', 'pro-maduro', 'maduro', 'venezuela', 'opposition march in puerto ordaz', 'maduro', 'venezuela', 'moa', 'guaido', 'snookered', 'white house', 'venezuela embassy', 'maduro coup', 'warcrimes', 'hilton', 'corbynista kryptonite', 'in venezuela', 'armed forces', 'maduro', 'venezuela', 'maduro', 'venezuela', 'scenes in #venezuela', 'maduro', 'venezuela', 'mass protests in #venezuela', 'maduro', 'russia', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'corbynista kryptonite', 'chaos today in #france on #mayday', 'venezuela', 'u.s. air operators', \"venezuela's airspace\", 'u.s. air operators in venezuela', 'scenes in #venezuela', 'of #cuba', 'los teques', 'cities of #venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'corbynista kryptonite', 'movement in caracas', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'u.s. secretary of state', 'call with russian foreign minister', 'venezuela', 'clashes in #venezuela between', 'govt troops', 'guaido supporters', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'msnbc', 'venezuela', 'power #controls everything', 'keen', 'venezuela showdown', 'maduro', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'gnb troops', 'grandma in #venezuela', 'ar-57 uppers', 'company in kent', 'washington', 'p90 magazine on', 'maduro', 'venezuela', 'juan guaidó', 'venezuela', 'strongman maduro', 'gnb', 'city of caracas', 'gnb in', 'city of caracas', 'capital #caracas on', 'day of #operacionlibertad', 'caracas', 'venezuela like', 'guayana marchers', 'dow', 'sp', 'powell', 'fred imbert', 'aapl $amd $clx $mgm $wynn #tradetalks #ipowatch $uber $lyft', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'anon #venezuela account', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'opposition protests in caracas', 'lavrov', 'russia', 'cuba', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'venezuela’s case', 'maduro', 'venezuela', 'aplicacion del', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', '🔴', 'caracas', 'miraflores', 'palace of dictator nicholas maduro', 'chaos today in #france on #mayday', 'venezuela', 'los teques', 'cities of #venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', \"spanish government's position on\", 'venezuela', 'catalonia', 'caracas', 'catalonia', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'caracas', 'venezuela like', 'guayana marchers', 'venezuela’s case', 'maduro', 'venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'opposition protests in barquisimeto', 'maduro', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'lavrov', 'venezuela risks', 'venezuela', 'gnb', 'city of caracas', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'assad playbook', 'opposition protests in maracaibo', 'venezuela', 'rory carroll', 'man in caracas', 'bolivarian revolution', 'maduro', 'venezuela', 'people at la carlota', 'eu attitude', 'venezuela', 'giletsjaunes in france', 'maduro', 'venezuela', 'us government', 'maduro', 'gnb in caracas', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'east coast', 'west coast part', 'maduro', 'venezuela', 'maduro', 'venezuela', 'president juan guaido', 'maduro', 'venezuela', 'el julio perotti diario', 'chaos today in #france on #mayday', 'venezuela', 'healthcare professionals from #venezuela', 'venezuela', 'canada', 'u.s', 'people of #apure state', 'downfall of #maduro', 'operacionlibertad', 'bolivarian armed forces of #venezuela', 'ak-103', 'fn fal', 'us puppet', 'juan guadió', 'north-american assault rifle', 'usmc', 'people of #venezuela in', 'maduroregime', 'shot in #venezuela', 'u.s. military', 'venezuela', 'paramilitaries', 'venezuela', 'venezuelacoup', 'russiagaters', 'qanon cultists', 'east coast', 'west coast part', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'venezuela', 'comment from pepe mujica about', 'demonstrators in #venezuela', 'united states', 'trishregan', '8pme #foxbusiness', 'eisenhower/roosevelt #venezuela story', 'la marea naranja', 'president', 'maduro', 'venezuela', 'venezuela', 'shot in #venezuela', 'maduro', 'venezuela', 'brigadier general', 'u.s.', 'involvement in #venezuela', 'u.s.', 'juan', 'streets of #caracas', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'venezuela', 'at state isp cantv', 'venezuela', 'class suburb of altamira', 'vzla', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'coup in #venezuela', 'venezuelans', 'people of #venezuela than', 'people of #yemen', 'saudi arabia', 'chaos today in #france on #mayday', 'venezuela', 'us interventions in', 'rise of isis', 'libya', 'us intervention in #venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'capital #caracas on', 'day of #operacionlibertad', 'trudeau', 'in #cuba', 'maduro sympathizer', 'venezuela /whether', 'cuba', 'canada', 'maduro', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'vietnams', 'iraq', 'mountains jungles', 'scenes in venezuela as', 'pro-maduro', 'hospital maria auxiliadora', 'minister of health zulema tomás of #peru', 'refugees from #venezuela', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'maduro', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'iran', 'flights from tehran', 'cuba', 'officers in #venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'iran', 'flights from tehran', 'cuba', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'venezuela', 'class suburb of altamira', 'in #venezuela', 'lavrov to #pompeo', 'maduro', 'venezuela', 'usmc', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', '✈️ tc-tsr', 'russia', 'caracas', 'punta cana', 'venezuela', 'maduro', 'american', 'maduro', 'america', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuela', 'u.s.', 'venezuela', 'people of #venezuela', 'maduro', 'venezuela', 'maduro', 'transit in #venezuela', '\\u2066', 'venezuela', 'embargo on #cuba', 'venezuela', 'maduro', 'venezuela', 'russia', 'syria', 'saudi arabia', 'shia minority', 'gnb in', 'city of caracas', 'maduro', 'venezuela', 'in #barquisimeto today', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'venezuela’s case', 'maduro', 'venezuela', 'u.s.', 'on venezuela scenarios', 'shanahan via', 'maduro', 'venezuela', 'venezuelans', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'gnb in', 'city of caracas', 'venezuela', 'crisis in #venezuela by', 'venezuela', 'govt', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'president', 'nicolas maduro’s', 'venezuela', 'maduro', 'narco', 'venezuelans', 'opposition protests in altamira', 'couple of blocks from altamira', 'march in vz history', 'guaido', 'ar-57 uppers', 'company in kent', 'washington', 'p90 magazine on', 'chaos today in #france on #mayday', 'venezuela', 'us_peak_oil supplies', 'us provider of #heatingoil #diesel #kerosene', 'jsf35s', '737max8', 'youtube', 'bing', 'google services', 'venezuela', 'guaido', 'caracas', 'trish regan primetime', 'u.s.', 'president of #venezuela', '8pm et on', 'point in #venezuela', 'trump administration', 'maga', 'road to world peace', 'in #venezuela', 'lavrov to #pompeo', 'coup in #venezuela', 'cia propaganda', 'maduro', 'venezuela', 'people of #venezuela', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'vietnams', 'iraq', 'mountains jungles', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'comment from pepe mujica about', 'demonstrators in #venezuela', 'cyclone kenneth', 'mozambique', 'lebo diseko', 'pemba', 'maduro', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'pepe mujica', 'maduro’s', 'venezuela', 'maduro', 'venezuela', 'faller', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'gnb', 'city of caracas', 'maduro', 'venezuela', 'maduro', 'venezuela', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'move for democracy in venezuela', 'venezuela', 'venezuelans', 'united states', 'government of #venezuela', 'maduro', 'venezuela', 'adm', 'faller', 'hasc on #venezuelan crisis', 'people of #venezuela', 'usmc', 'maduro', 'venezuela', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'scenes in venezuela as', 'pro-maduro', 'maduro', 'venezuela', 'coup in #venezuela', 'january', 'maduro', 'gnb in', 'city of caracas', 'american', 'us government', 'venezuela', 'iran', 'libya', 'iraq', 'syria', 'haiti', 'honduras', 'afghanistan', 'yemen', 'palestine', 'starbucks commies that', 'venezuela', 'msnbc', 'venezuela', 'power #controls everything', 'edition of #presstvdebate', 'u.s. #venezuela intervention', 'apr-2019', 'spanish occupation', 'independence referendum voters in #catalonia', 'oct-2917', 'maduro', 'venezuela', 'venezuela', 'venezuela', 'lo siento', 'maduro', 'venezuela', 'bolivar', 'exchange rate in #venezuela for', 'ves/usd', 'maduro', 'ves/usd', 'maduro', 'venezuela', 'chaos today in #france on #mayday', 'venezuela', 'president', 'maduro', 'venezuela', 'united states', 'maduro', 'venezuela', 'venezuela', 'venezuela', 'maduro speech at', 'utc', 'president', 'nicolas maduro’s', 'venezuela', 'maduro', 'narco', 'uk', 'us', 'us sanctions', '\\u2066', 'venezuela', 'tonigh', 'pres of venezuela', '\\u2069', 'venezuela', 'maduro', 'venezuela', 'maduro', 'venezuela', 'adm', 'faller', 'hasc on #venezuela crisis', 'china', 'maduro', 'venezuela', 'in #barquisimeto today']\n",
      "798 1836 567\n",
      "precision:  0.30296127562642367\n",
      "recall:  0.5846153846153846\n",
      "f_measure:  0.39909977494373594\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "\n",
    "all_mentions=[]\n",
    "all_outputs=[]\n",
    "\n",
    "true_positive_count=0\n",
    "false_positive_count=0\n",
    "false_negative_count=0\n",
    "\n",
    "total_annotations=0\n",
    "total_tagged=0\n",
    "for index, row in complete_tweet_dataframe_grouped_df_sorted.iterrows():\n",
    "    output=flatten(list(row.output_mentions),[])\n",
    "#     print(output)\n",
    "    all_outputs+=output\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list=[]\n",
    "    tp_counter_inner=0\n",
    "    fp_counter_inner=0\n",
    "    fn_counter_inner=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    \n",
    "#     output_mentions_list=flatten(complete_tweet_dataframe_grouped_df_sorted[complete_tweet_dataframe_grouped_df_sorted.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "    all_mentions+=annotated_mention_list\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner=len(output_mentions_list)\n",
    "#     total_tagged+=len(output_mentions_list)\n",
    "#     total_annotations+=len(annotated_mention_list)\n",
    "    \n",
    "#     while(annotated_mention_list):\n",
    "#         if(len(output_mentions_list)):\n",
    "#             annotated_candidate= annotated_mention_list.pop()\n",
    "#             if(annotated_candidate in output_mentions_list):\n",
    "#                 output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "#                 tp_counter_inner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "#     fp_counter_inner=all_postitive_counter_inner- tp_counter_inner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count+=tp_counter_inner\n",
    "#     false_positive_count+=fp_counter_inner\n",
    "#     false_negative_count+=fn_counter_inner\n",
    "\n",
    "# print(total_annotations,total_tagged)\n",
    "# print(true_positive_count,false_positive_count,false_negative_count)\n",
    "print(phase1outputs)\n",
    "get_F1(all_mentions,phase1outputs)\n",
    "# get_F1(all_mentions,all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-5d1f4d336703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfalse_positive_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positive_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfalse_negative_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf_measure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "f_measure=2*(precision*recall)/(precision+recall)\n",
    "print(precision,recall,f_measure)\n",
    "\n",
    "print(time2-time1)\n",
    "print(time3-time1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-99-a150bcecdd2b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-99-a150bcecdd2b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    2425 487 905\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#3K\n",
    "#SVM\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#RF\n",
    "2425 487 905\n",
    "0.832760989010989 0.7282282282282282 0.7769945530278757\n",
    "\n",
    "#LR\n",
    "2415 460 915\n",
    "0.84 0.7252252252252253 0.7784045124899275\n",
    "\n",
    "#1K\n",
    "#SVM\n",
    "# precision:  0.8075117370892019\n",
    "# recall:  0.63003663003663\n",
    "# f_measure:  0.7078189300411523"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ritter- TwitterNLP in Phase 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3k annotated tweets\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "ritter_annotator=pd.read_csv(\"/Users/satadisha/Documents/GitHub/my-baseline-setup/ritter_tweets_3k_annotated_output.csv\",sep =',',keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3067\n",
      "['ID', 'HashTags', 'TweetText', 'Output', 'mentions_other', 'User', 'ritter_candidates', 'calai_candidates', 'stanford_candidates']\n"
     ]
    }
   ],
   "source": [
    "# tweets=tweets_unpartitoned['TweetText'].tolist()\n",
    "print(len(ritter_annotator))\n",
    "print(ritter_annotator.columns.tolist())\n",
    "CTrie_ritter=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n"
     ]
    }
   ],
   "source": [
    "ritter_annotated_candidates=ritter_annotator['Output'].tolist()\n",
    "for candidate in ritter_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_ritter.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesinRitterTrie=CTrie_ritter.displayTrie(\"\",[])\n",
    "print(len(candidatesinRitterTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 645\n",
      "-0.23599405851453567\n",
      "For entities:  (511, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "For entities:  (511, 6)\n",
      "For non-entities:  (111, 6)\n",
      "For ambiguous:  (23, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4566 incomplete tweets:  157\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_Ritter = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_Ritter, converted_candidates_w_Ritter, complete_tweet_dataframe_grouped_df_sorted_w_Ritter= Phase2_w_Ritter.executor(max_batch_value,tweet_sentence_df,CTrie_ritter,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>index</th>\n",
       "      <th>entry_batch</th>\n",
       "      <th>sentID</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetSentence</th>\n",
       "      <th>phase1Candidates</th>\n",
       "      <th>annotation</th>\n",
       "      <th>stanford_candidates</th>\n",
       "      <th>output_mentions</th>\n",
       "      <th>completeness</th>\n",
       "      <th>current_minus_entry</th>\n",
       "      <th>candidates_with_label</th>\n",
       "      <th>only_good_candidates</th>\n",
       "      <th>ambiguous_candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[RussiaGate]</td>\n",
       "      <td>[JanKimbrough]</td>\n",
       "      <td>[REPORT: FBI Obtained FISA Warrant For Trump A...</td>\n",
       "      <td>[fbi::*3*||fisa warrant::*5*6*||trump aide::*8...</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[False]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(fbi, g), (fisa, g), (trump, g), (the russia...</td>\n",
       "      <td>[[fbi, fisa, trump]]</td>\n",
       "      <td>[[the russian government]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[zi_cam]</td>\n",
       "      <td>[BUSTED....]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[nan, nan]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[sicsemp4ever, sicsemp4ever]</td>\n",
       "      <td>[Carter Page is trending but 3 weeks ago there...</td>\n",
       "      <td>[carter page::*1*2*||team trump::*15*16*||, ]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[True, True]</td>\n",
       "      <td>[0.0, 0.0]</td>\n",
       "      <td>[[(carter page, g), (fisa, g), (trump, g)], []]</td>\n",
       "      <td>[[carter page, fisa, trump], []]</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[raponikoff]</td>\n",
       "      <td>[Looks like it might be time for some more tom...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[MsEntropy]</td>\n",
       "      <td>[Sean Spicer on Carter Page:]</td>\n",
       "      <td>[sean spicer on carter page::*1*2*3*4*5*||]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[True]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[[(sean spicer, g), (carter page, g)]]</td>\n",
       "      <td>[[sean spicer, carter page]]</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetID       index entry_batch  sentID      hashtags  \\\n",
       "0        0       [nan]         [0]     [0]  [RussiaGate]   \n",
       "1        1       [nan]         [0]     [0]            []   \n",
       "2        2  [nan, nan]      [0, 0]  [0, 1]          [, ]   \n",
       "3        3       [nan]         [0]     [0]            []   \n",
       "4        4       [nan]         [0]     [0]            []   \n",
       "\n",
       "                           user  \\\n",
       "0                [JanKimbrough]   \n",
       "1                      [zi_cam]   \n",
       "2  [sicsemp4ever, sicsemp4ever]   \n",
       "3                  [raponikoff]   \n",
       "4                   [MsEntropy]   \n",
       "\n",
       "                                       TweetSentence  \\\n",
       "0  [REPORT: FBI Obtained FISA Warrant For Trump A...   \n",
       "1                                       [BUSTED....]   \n",
       "2  [Carter Page is trending but 3 weeks ago there...   \n",
       "3  [Looks like it might be time for some more tom...   \n",
       "4                      [Sean Spicer on Carter Page:]   \n",
       "\n",
       "                                    phase1Candidates annotation  \\\n",
       "0  [fbi::*3*||fisa warrant::*5*6*||trump aide::*8...       [[]]   \n",
       "1                                                 []       [[]]   \n",
       "2      [carter page::*1*2*||team trump::*15*16*||, ]   [[], []]   \n",
       "3                                                 []       [[]]   \n",
       "4        [sean spicer on carter page::*1*2*3*4*5*||]       [[]]   \n",
       "\n",
       "  stanford_candidates                   output_mentions  completeness  \\\n",
       "0                [[]]              [[fbi, fisa, trump]]       [False]   \n",
       "1                [[]]                              [[]]        [True]   \n",
       "2            [[], []]  [[carter page, fisa, trump], []]  [True, True]   \n",
       "3                [[]]                              [[]]        [True]   \n",
       "4                [[]]      [[sean spicer, carter page]]        [True]   \n",
       "\n",
       "  current_minus_entry                              candidates_with_label  \\\n",
       "0               [0.0]  [[(fbi, g), (fisa, g), (trump, g), (the russia...   \n",
       "1               [0.0]                                               [[]]   \n",
       "2          [0.0, 0.0]    [[(carter page, g), (fisa, g), (trump, g)], []]   \n",
       "3               [0.0]                                               [[]]   \n",
       "4               [0.0]             [[(sean spicer, g), (carter page, g)]]   \n",
       "\n",
       "               only_good_candidates        ambiguous_candidates  \n",
       "0              [[fbi, fisa, trump]]  [[the russian government]]  \n",
       "1                              [[]]                        [[]]  \n",
       "2  [[carter page, fisa, trump], []]                    [[], []]  \n",
       "3                              [[]]                        [[]]  \n",
       "4      [[sean spicer, carter page]]                        [[]]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_tweet_dataframe_grouped_df_sorted_w_Ritter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2770\n",
      "2385 385 437\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_ritter=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Ritter[complete_tweet_dataframe_grouped_df_sorted_w_Ritter.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8610108303249098 0.8451452870304749 0.8530042918454936\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SVM\n",
    "0.8605577689243028 0.8428520752039731 0.8516129032258064\n",
    "\n",
    "#RF\n",
    "0.8614435981138919 0.8433948863636364 0.8523237035707877\n",
    "\n",
    "#LR\n",
    "0.8610108303249098 0.8451452870304749 0.8530042918454936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaguilar as Phase I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phase2_Trie_baseline_reintroduction_effectiveness as phase2\n",
    "from ast import literal_eval\n",
    "import string\n",
    "\n",
    "z_score=-0.1119\n",
    "max_batch_value=0\n",
    "phase2stopwordList=[]\n",
    "reintroduction_threshold_dummy=2\n",
    "\n",
    "\n",
    "\n",
    "def remAmpersand(candidateStr):\n",
    "    candidateStr=candidateStr.replace('&amp;','')\n",
    "    return candidateStr\n",
    "    \n",
    "string.punctuation=string.punctuation+'…‘’'\n",
    "f = open(\"/Users/satadisha/Documents/GitHub/tweebo-parser/tweets_3k_annotated.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTrie_gaguilar=trie.Trie(\"ROOT\")\n",
    "tweet_sentence_df_copy=tweet_sentence_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1795\n",
      "2724\n",
      "771\n"
     ]
    }
   ],
   "source": [
    "file_text=f.read()\n",
    "\n",
    "output_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n'))) #conll\n",
    "print(len(output_sentences))\n",
    "gaguilar_annotated_candidates=[]\n",
    "for line in output_sentences:\n",
    "    if(line):\n",
    "        tabs=line.split('\\t')\n",
    "        if(tabs):\n",
    "            for candidate in tabs:\n",
    "                gaguilar_annotated_candidates.append(candidate)\n",
    "print(len(gaguilar_annotated_candidates))\n",
    "for candidate in gaguilar_annotated_candidates:\n",
    "    candidateList= [remAmpersand(elem).strip(string.punctuation).strip() for elem in candidate.lower().split(',') if(elem)]\n",
    "#     print(candidateList)\n",
    "    for candidateText in candidateList:\n",
    "        CTrie_gaguilar.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "candidatesingaguilarTrie=CTrie_gaguilar.displayTrie(\"\",[])\n",
    "print(len(candidatesingaguilarTrie))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4723 4723 731\n",
      "-0.23514899240820253\n",
      "For entities:  (590, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "For entities:  (590, 6)\n",
      "For non-entities:  (117, 6)\n",
      "For ambiguous:  (24, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4593 incomplete tweets:  130\n",
      "16\n",
      "16\n",
      "final tally:  4723 4723\n",
      "524:  524    [[fbi, fisa, trump, carter page, washington po...\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "Phase2_w_gaguilar = phase2.EntityResolver()\n",
    "candidate_base_post_Phase2_w_gaguilar, converted_candidates_w_gaguilar, complete_tweet_dataframe_grouped_df_sorted_w_gaguilar= Phase2_w_gaguilar.executor(max_batch_value,tweet_sentence_df,CTrie_gaguilar,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2986\n",
      "2484 502 471\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_gaguilar=0\n",
    "false_positive_count_gaguilar=0\n",
    "false_negative_count_gaguilar=0\n",
    "\n",
    "total_annotations_gaguilar=0\n",
    "total_tagged_gaguilar=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_gaguilar=[]\n",
    "    tp_counter_inner_gaguilar=0\n",
    "    fp_counter_inner_gaguilar=0\n",
    "    fn_counter_inner_gaguilar=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_gaguilar=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_gaguilar.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "    output_mentions_list_gaguilar=flatten(complete_tweet_dataframe_grouped_df_sorted_w_gaguilar[complete_tweet_dataframe_grouped_df_sorted_w_gaguilar.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list)\n",
    "#     print(output_mentions_list)\n",
    "    \n",
    "    all_postitive_counter_inner_gaguilar=len(output_mentions_list_gaguilar)\n",
    "    total_tagged_gaguilar+=len(output_mentions_list_gaguilar)\n",
    "    total_annotations_gaguilar+=len(annotated_mention_list_gaguilar)\n",
    "    \n",
    "    while(annotated_mention_list_gaguilar):\n",
    "        if(len(output_mentions_list_gaguilar)):\n",
    "            annotated_candidate= annotated_mention_list_gaguilar.pop()\n",
    "            if(annotated_candidate in output_mentions_list_gaguilar):\n",
    "                output_mentions_list_gaguilar.pop(output_mentions_list_gaguilar.index(annotated_candidate))\n",
    "                tp_counter_inner_gaguilar+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_gaguilar.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_gaguilar.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_gaguilar=len(unrecovered_annotated_mention_list_gaguilar)\n",
    "    fp_counter_inner_gaguilar=all_postitive_counter_inner_gaguilar- tp_counter_inner_gaguilar\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_gaguilar+=tp_counter_inner_gaguilar\n",
    "    false_positive_count_gaguilar+=fp_counter_inner_gaguilar\n",
    "    false_negative_count_gaguilar+=fn_counter_inner_gaguilar\n",
    "\n",
    "print(total_annotations_gaguilar,total_tagged_gaguilar)\n",
    "print(true_positive_count_gaguilar,false_positive_count_gaguilar,false_negative_count_gaguilar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8318821165438715 0.8406091370558376 0.8362228581046962\n"
     ]
    }
   ],
   "source": [
    "precision_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_positive_count_gaguilar)\n",
    "recall_gaguilar=(true_positive_count_gaguilar)/(true_positive_count_gaguilar+false_negative_count_gaguilar)\n",
    "f_measure_gaguilar=2*(precision_gaguilar*recall_gaguilar)/(precision_gaguilar+recall_gaguilar)\n",
    "print(precision_gaguilar,recall_gaguilar,f_measure_gaguilar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LR\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#RF\n",
    "0.8313738663083641 0.8384146341463414 0.8348794063079776\n",
    "\n",
    "#SVM\n",
    "0.8318821165438715 0.8406091370558376 0.8362228581046962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just NeuroNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3068\n",
      "3374 3849\n",
      "2501 1348 673\n"
     ]
    }
   ],
   "source": [
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# neuroner_file = open('mentions_output_tweets_3K.txt', 'r') \n",
    "# neuroner_lines = neuroner_file.readlines()\n",
    "# print(len(neuroner_lines))\n",
    "# line_count=0\n",
    "# neuroner_annotated_candidates=[]\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "#     neuroner_output=neuroner_lines[line_count]\n",
    "#     output_mentions_list_neuroner=[candidate.lower().strip(string.punctuation).strip() for candidate in neuroner_output.split(',') if(candidate.strip(string.punctuation).strip())]\n",
    "#     neuroner_annotated_candidates.extend(output_mentions_list_neuroner)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner - tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "    \n",
    "#     line_count+=1\n",
    "    \n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6497791634190699 0.7879647132955262 0.7122312402107361\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroNER as Phase I Entity Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3849\n",
      "[]\n",
      "1425\n"
     ]
    }
   ],
   "source": [
    "# tweet_sentence_df_2nd_copy=tweet_sentence_df.copy(deep=True)\n",
    "# CTrie_neuroner=trie.Trie(\"ROOT\")\n",
    "# print(len(neuroner_annotated_candidates))\n",
    "# print(phase2stopwordList)\n",
    "\n",
    "# for candidateText in neuroner_annotated_candidates:\n",
    "# #     print(candidateText)\n",
    "#     if(candidateText not in all_stopwords):\n",
    "#         CTrie_neuroner.__setitem__(candidateText.split(),len(candidateText.split()),[],batch_number)\n",
    "\n",
    "# candidatesinNeuronerTrie=CTrie_neuroner.displayTrie(\"\",[])\n",
    "# print(len(candidatesinNeuronerTrie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambiguous_candidates_in_batch:  0\n",
      "dataframe lengths:  4721 4721 1057\n",
      "-0.2661675732774245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1282: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']>=0.8]='g'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1283: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][(candidate_featureBase_DF['probability'] > 0.4) & (candidate_featureBase_DF['probability'] < 0.8)] = 'a'\n",
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:1284: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  candidate_featureBase_DF['status'][candidate_featureBase_DF['probability']<=0.4]='b'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "For entities:  (797, 6)\n",
      "For non-entities:  (216, 6)\n",
      "For ambiguous:  (44, 6)\n",
      "Empty DataFrame\n",
      "Columns: [candidate, batch, length, cap, substring-cap, s-o-sCap, all-cap, non-cap, non-discriminative, cumulative, Z_ScoreUnweighted, normalized_cap, normalized_capnormalized_substring-cap, normalized_s-o-sCap, normalized_all-cap, normalized_non-cap, normalized_non-discriminative, probability, status]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satadisha/Documents/GitHub/tweebo-parser/phase2_Trie_baseline_reintroduction_effectiveness.py:369: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ambiguous_bad_candidates['max_column'] =ambiguous_bad_candidates[['cap','substring-cap','s-o-sCap','all-cap','non-cap','non-discriminative']].idxmax(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed tweets:  4525 incomplete tweets:  196\n",
      "16\n",
      "16\n",
      "final tally:  4721 4721\n",
      "524:  524    [[world news, fbi, fisa, trump, washington post]]\n",
      "Name: output_mentions, dtype: object\n",
      "['tweetID', 'index', 'entry_batch', 'sentID', 'hashtags', 'user', 'TweetSentence', 'phase1Candidates', 'annotation', 'stanford_candidates', 'output_mentions', 'completeness', 'current_minus_entry', 'candidates_with_label', 'only_good_candidates', 'ambiguous_candidates']\n"
     ]
    }
   ],
   "source": [
    "# Phase2_w_Neuroner = phase2.EntityResolver()\n",
    "# candidate_base_post_Phase2_w_Neuroner, converted_candidates_w_Neuroner, complete_tweet_dataframe_grouped_df_sorted_w_Neuroner= Phase2_w_Neuroner.executor(max_batch_value,tweet_sentence_df_2nd_copy,CTrie_neuroner,phase2stopwordList,z_score,reintroduction_threshold_dummy,tweet_sentence_df_2nd_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_tweet_dataframe_grouped_df_sorted_w_Neuroner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 3129\n",
      "2507 622 549\n"
     ]
    }
   ],
   "source": [
    "# # from ast import literal_eval\n",
    "# true_positive_count_neuroner=0\n",
    "# false_positive_count_neuroner=0\n",
    "# false_negative_count_neuroner=0\n",
    "\n",
    "# total_annotations_neuroner=0\n",
    "# total_tagged_neuroner=0\n",
    "\n",
    "# for index, row in tweets_unpartitoned.iterrows():\n",
    "#     unrecovered_annotated_mention_list_neuroner=[]\n",
    "#     tp_counter_inner_neuroner=0\n",
    "#     fp_counter_inner_neuroner=0\n",
    "#     fn_counter_inner_neuroner=0\n",
    "    \n",
    "#     tweet_ID=row['ID']\n",
    "#     annotated_mention_list_neuroner=[]\n",
    "#     annotated=row['mentions_other'].lower()\n",
    "    \n",
    "#     if(annotated):\n",
    "#         tweet_level=annotated.split(';')\n",
    "#         if(tweet_level):\n",
    "#             tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "#             for elem in tweet_level:\n",
    "#                 sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "#                 if(sentence_level):\n",
    "#                     annotated_mention_list_neuroner.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "#     output_mentions_list_neuroner=flatten(complete_tweet_dataframe_grouped_df_sorted_w_Neuroner[complete_tweet_dataframe_grouped_df_sorted_w_Neuroner.tweetID==tweet_ID].output_mentions.tolist(),[])\n",
    "    \n",
    "# #     print(row['TweetText'])\n",
    "# #     print(tweet_ID, annotated_mention_list)\n",
    "# #     print(output_mentions_list)\n",
    "    \n",
    "#     all_postitive_counter_inner_neuroner=len(output_mentions_list_neuroner)\n",
    "#     total_tagged_neuroner+=len(output_mentions_list_neuroner)\n",
    "#     total_annotations_neuroner+=len(annotated_mention_list_neuroner)\n",
    "    \n",
    "#     while(annotated_mention_list_neuroner):\n",
    "#         if(len(output_mentions_list_neuroner)):\n",
    "#             annotated_candidate= annotated_mention_list_neuroner.pop()\n",
    "#             if(annotated_candidate in output_mentions_list_neuroner):\n",
    "#                 output_mentions_list_neuroner.pop(output_mentions_list_neuroner.index(annotated_candidate))\n",
    "#                 tp_counter_inner_neuroner+=1\n",
    "#             else:\n",
    "#                 unrecovered_annotated_mention_list_neuroner.append(annotated_candidate)\n",
    "#         else:\n",
    "#             unrecovered_annotated_mention_list_neuroner.extend(annotated_mention_list)\n",
    "#             break\n",
    "\n",
    "#     # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "#     fn_counter_inner_neuroner=len(unrecovered_annotated_mention_list_neuroner)\n",
    "#     fp_counter_inner_neuroner=all_postitive_counter_inner_neuroner- tp_counter_inner_neuroner\n",
    "\n",
    "# #     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "#     true_positive_count_neuroner+=tp_counter_inner_neuroner\n",
    "#     false_positive_count_neuroner+=fp_counter_inner_neuroner\n",
    "#     false_negative_count_neuroner+=fn_counter_inner_neuroner\n",
    "\n",
    "# print(total_annotations_neuroner,total_tagged_neuroner)\n",
    "# print(true_positive_count_neuroner,false_positive_count_neuroner,false_negative_count_neuroner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8012144455097475 0.8203534031413613 0.8106709781729993\n"
     ]
    }
   ],
   "source": [
    "# precision_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_positive_count_neuroner)\n",
    "# recall_neuroner=(true_positive_count_neuroner)/(true_positive_count_neuroner+false_negative_count_neuroner)\n",
    "# f_measure_neuroner=2*(precision_neuroner*recall_neuroner)/(precision_neuroner+recall_neuroner)\n",
    "# print(precision_neuroner,recall_neuroner,f_measure_neuroner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Ritter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3330 2095\n",
      "1571 524 875\n"
     ]
    }
   ],
   "source": [
    "# from ast import literal_eval\n",
    "true_positive_count_ritter=0\n",
    "false_positive_count_ritter=0\n",
    "false_negative_count_ritter=0\n",
    "\n",
    "total_annotations_ritter=0\n",
    "total_tagged_ritter=0\n",
    "\n",
    "for index, row in tweets_unpartitoned.iterrows():\n",
    "    unrecovered_annotated_mention_list_ritter=[]\n",
    "    tp_counter_inner_ritter=0\n",
    "    fp_counter_inner_ritter=0\n",
    "    fn_counter_inner_ritter=0\n",
    "    \n",
    "    tweet_ID=row['ID']\n",
    "    annotated_mention_list_ritter=[]\n",
    "    annotated=row['mentions_other'].lower()\n",
    "    output_mentions_list_ritter=[]\n",
    "    \n",
    "    if(annotated):\n",
    "        tweet_level=annotated.split(';')\n",
    "        if(tweet_level):\n",
    "            tweet_level=[tweet_level_elem for tweet_level_elem in tweet_level if(tweet_level_elem)]\n",
    "            for elem in tweet_level:\n",
    "                sentence_level=[sentence_level_elem for sentence_level_elem in elem.split(',') if(sentence_level_elem)]\n",
    "                if(sentence_level):\n",
    "                    annotated_mention_list_ritter.extend([innermost.strip() for innermost in sentence_level if(innermost)])\n",
    "                    \n",
    "    candidate_list_ritter=flatten(ritter_annotator[ritter_annotator.ID==tweet_ID].Output.tolist(),[])\n",
    "    for ritter_candidate in candidate_list_ritter:\n",
    "        output_mentions_list_ritter+= [remAmpersand(elem).strip(string.punctuation).strip() for elem in ritter_candidate.lower().split(',') if(elem)]\n",
    "        \n",
    "#     output_mentions_list_ritter= [remAmpersand(elem).strip(string.punctuation).strip().lower() for elem in candidate_list_ritter]\n",
    "    \n",
    "    \n",
    "#     print(row['TweetText'])\n",
    "#     print(tweet_ID, annotated_mention_list_ritter)\n",
    "#     print(output_mentions_list_ritter)\n",
    "    \n",
    "    all_postitive_counter_inner_ritter=len(output_mentions_list_ritter)\n",
    "    total_tagged_ritter+=len(output_mentions_list_ritter)\n",
    "    total_annotations_ritter+=len(annotated_mention_list_ritter)\n",
    "    \n",
    "    while(annotated_mention_list_ritter):\n",
    "        if(len(output_mentions_list_ritter)):\n",
    "            annotated_candidate= annotated_mention_list_ritter.pop()\n",
    "            if(annotated_candidate in output_mentions_list_ritter):\n",
    "                output_mentions_list_ritter.pop(output_mentions_list_ritter.index(annotated_candidate))\n",
    "                tp_counter_inner_ritter+=1\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list_ritter.append(annotated_candidate)\n",
    "        else:\n",
    "            unrecovered_annotated_mention_list_ritter.extend(annotated_mention_list)\n",
    "            break\n",
    "\n",
    "    # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "    fn_counter_inner_ritter=len(unrecovered_annotated_mention_list_ritter)\n",
    "    fp_counter_inner_ritter=all_postitive_counter_inner_ritter- tp_counter_inner_ritter\n",
    "\n",
    "#     print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "\n",
    "    true_positive_count_ritter+=tp_counter_inner_ritter\n",
    "    false_positive_count_ritter+=fp_counter_inner_ritter\n",
    "    false_negative_count_ritter+=fn_counter_inner_ritter\n",
    "\n",
    "print(total_annotations_ritter,total_tagged_ritter)\n",
    "print(true_positive_count_ritter,false_positive_count_ritter,false_negative_count_ritter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7498806682577566 0.6422730989370401 0.6919180797181238\n"
     ]
    }
   ],
   "source": [
    "precision_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_positive_count_ritter)\n",
    "recall_ritter=(true_positive_count_ritter)/(true_positive_count_ritter+false_negative_count_ritter)\n",
    "f_measure_ritter=2*(precision_ritter*recall_ritter)/(precision_ritter+recall_ritter)\n",
    "print(precision_ritter,recall_ritter,f_measure_ritter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results with different annotators:\n",
    "\n",
    "##With CS+ in phase 1:\n",
    "0.7982625482625483 0.7359833877187778 0.7658589288470442\n",
    "\n",
    "##With Turboparse chunker in phase 1:\n",
    "0.8381118881118881 0.7104327208061648 0.7690086621751684\n",
    "\n",
    "##Just TwitterNLP:\n",
    "0.7460620525059666 0.6335630320226996 0.6852257781674704\n",
    "\n",
    "##With TwitterNLP entity annotator in phase 1:\n",
    "0.856 0.8288732394366197 0.8422182468694097\n",
    "\n",
    "##Just NeuroNER:\n",
    "0.6497791634190699 0.7879647132955262 0.7122312402107361\n",
    "\n",
    "##With NeuroNER entity annotator in phase 1:\n",
    "0.8012144455097475 0.8203534031413613 0.8106709781729993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
